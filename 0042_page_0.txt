                     Distributed Representations of Words and Phrases
                                   andtheir Compositionality
                          TomasMikolov           Ilya Sutskever        KaiChen
                            GoogleInc.            GoogleInc.          GoogleInc.
                          MountainView           MountainView        MountainView
                       mikolov@google.com    ilyasu@google.com     kai@google.com
                                  GregCorrado                  Jeffrey Dean
                                   GoogleInc.                  GoogleInc.
                                  MountainView                MountainView
                             gcorrado@google.com            jeff@google.com
                                                Abstract
                         The recently introduced continuous Skip-gram model is an efﬁcient method for
                         learning high-quality distributed vector representations that capture a large num-
                         ber of precise syntactic and semantic word relationships. In this paper we present
                         several extensions that improve both the quality of the vectors and the training
                         speed. By subsampling of the frequent words we obtain signiﬁcant speedup and
                         also learn more regular word representations. We also describe a simple alterna-
                         tive to the hierarchical softmax called negative sampling.
                         Aninherent limitation of word representations is their indifference to word order
                         and their inability to represent idiomatic phrases. For example, the meanings of
                         “Canada”and“Air”cannotbeeasilycombinedtoobtain“AirCanada”. Motivated
                         bythis example,wepresenta simplemethodforﬁndingphrasesintext,andshow
                         that learning good vector representations for millions of phrases is possible.
                   1 Introduction
                   Distributed representations of words in a vector space help learning algorithms to achieve better
                   performancein naturallanguageprocessingtasks by groupingsimilar words. Oneoftheearliestuse
                   of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea
                   has since been applied to statistical language modeling with considerable success [1]. The follow
                   up work includes applications to automatic speech recognition and machine translation [14, 7], and
                   a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].
                   Recently, Mikolov et al. [8] introduced the Skip-gram model, an efﬁcient method for learning high-
                   quality vector representations of words from large amounts of unstructured text data. Unlike most
                   of the previously used neural network architectures for learning word vectors, training of the Skip-
                   gram model (see Figure 1) does not involve dense matrix multiplications. This makes the training
                   extremelyefﬁcient: an optimizedsingle-machineimplementationcantrain on morethan100billion
                   wordsinoneday.
                   The word representations computed using neural networks are very interesting because the learned
                   vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of
                   these patterns can be represented as linear translations. For example, the result of a vector calcula-
                   tion vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other word
                   vector [9, 8].
                                                    1
