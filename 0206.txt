                       Cassandra-ADecentralizedStructured Storage System
                                                                  Avinash Lakshman                                 Prashant Malik
                                                                          Facebook                                      Facebook
                   ABSTRACT                                                                                 box Search is a feature that enables users to search through
                   Cassandra is a distributed storage system for managing very                              their Facebook Inbox. At Facebook this meant the system
                   large amounts of structured data spread out across many                                  wasrequiredtohandleaveryhighwritethroughput, billions
                   commodity servers, while providing highly available service                              of writes per day, and also scale with the number of users.
                   with no single point of failure. Cassandra aims to run on top                            Since users are served from data centers that are geograph-
                   of an infrastructure of hundreds of nodes (possibly spread                               ically distributed, being able to replicate data across data
                   across diﬀerent data centers). At this scale, small and large                            centers was key to keep search latencies down. Inbox Search
                   components fail continuously.               The way Cassandra man-                       was launched in June of 2008 for around 100 million users
                   ages the persistent state in the face of these failures drives                           and today we are at over 250 million users and Cassandra
                   the reliability and scalability of the software systems rely-                            has kept up the promise so far. Cassandra is now deployed
                   ing on this service. While in many ways Cassandra resem-                                 as the backend storage system for multiple services within
                   bles a database and shares many design and implementation                                Facebook.
                   strategies therewith, Cassandra does not support a full rela-                               This paper is structured as follows. Section 2 talks about
                   tional data model; instead, it provides clients with a simple                            related work, some of which has been very inﬂuential on our
                   data model that supports dynamic control over data lay-                                  design. Section 3 presents the data model in more detail.
                   out and format. Cassandra system was designed to run on                                  Section 4 presents the overview of the client API. Section
                   cheap commodity hardware and handle high write through-                                  5 presents the system design and the distributed algorithms
                   put while not sacriﬁcing read eﬃciency.                                                  that make Cassandra work. Section 6 details the experiences
                                                                                                            of making Cassandra work and reﬁnements to improve per-
                   1.     INTRODUCTION                                                                      formance. In Section 6.1 we describe how one of the appli-
                                                                                                            cations in the Facebook platform uses Cassandra. Finally
                      Facebook runs the largest social networking platform that                             Section 7 concludes with future work on Cassandra.
                   serves hundreds of millions users at peak times using tens of
                   thousands of servers located in many data centers around                                 2.     RELATEDWORK
                   the world.       There are strict operational requirements on
                   Facebook’s platform in terms of performance, reliability and                                Distributing data for performance, availability and dura-
                   eﬃciency, and to support continuous growth the platform                                  bility has been widely studied in the ﬁle system and database
                   needs to be highly scalable. Dealing with failures in an in-                             communities. Compared to P2P storage systems that only
                   frastructure comprised of thousands of components is our                                 support ﬂat namespaces, distributed ﬁle systems typically
                   standard mode of operation; there are always a small but                                 support hierarchical namespaces. Systems like Ficus[14] and
                   signiﬁcant number of server and network components that                                  Coda[16] replicate ﬁles for high availability at the expense
                   are failing at any given time. As such, the software systems                             of consistency. Update conﬂicts are typically managed us-
                   needtobeconstructedinamannerthattreatsfailuresasthe                                      ing specialized conﬂict resolution procedures. Farsite[2] is
                   norm rather than the exception. To meet the reliability and                              a distributed ﬁle system that does not use any centralized
                   scalability needs described above Facebook has developed                                 server. Farsite achieves high availability and scalability us-
                   Cassandra.                                                                               ing replication. The Google File System (GFS)[9] is another
                      Cassandra uses a synthesis of well known techniques to                                distributed ﬁle system built for hosting the state of Google’s
                   achieve scalability and availability. Cassandra was designed                             internal applications. GFS uses a simple design with a sin-
                   to fulﬁll the storage needs of the Inbox Search problem. In-                             gle master server for hosting the entire metadata and where
                                                                                                            the data is split into chunks and stored in chunk servers.
                                                                                                            However the GFS master is now made fault tolerant using
                                                                                                            the Chubby[3] abstraction. Bayou[18] is a distributed rela-
                                                                                                            tional database system that allows disconnected operations
                   Permission to make digital or hard copies of all or part of this work for                and provides eventual data consistency. Among these sys-
                   personal or classroom use is granted without fee provided that copies are                tems, Bayou, Coda and Ficus allow disconnected operations
                   not made or distributed for proﬁt or commercial advantage and that copies                and are resilient to issues such as network partitions and
                   bear this notice and the full citation on the ﬁrst page. To copy otherwise, to           outages. These systems diﬀer on their conﬂict resolution
                   republish, to post on servers or to redistribute to lists, requires prior speciﬁc        procedures. For instance, Coda and Ficus perform system
                   permission and/or a fee.
                   Copyright 200X ACMX-XXXXX-XX-X/XX/XX...$10.00.                                           level conﬂict resolution and Bayou allows application level
              resolution. All of them however, guarantee eventual consis-        columnName can refer to a speciﬁc column within a col-
              tency. Similar to these systems, Dynamo[6] allows read and       umn family, a column family, a super column family, or a
              write operations to continue even during network partitions      column within a super column.
              and resolves update conﬂicts using diﬀerent conﬂict resolu-
              tion mechanisms, some client driven. Traditional replicated      5.   SYSTEMARCHITECTURE
              relational database systems focus on the problem of guar-          The architecture of a storage system that needs to op-
              anteeing strong consistency of replicated data. Although         erate in a production setting is complex.   In addition to
              strong consistency provides the application writer a con-        the actual data persistence component, the system needs to
              venient programming model, these systems are limited in          have the following characteristics; scalable and robust solu-
              scalability and availability [10]. These systems are not ca-     tions for load balancing, membership and failure detection,
              pable of handling network partitions because they typically      failure recovery, replica synchronization, overload handling,
              provide strong consistency guarantees.                           state transfer, concurrency and job scheduling, request mar-
                Dynamo[6] is a storage system that is used by Amazon           shalling, request routing, system monitoring and alarming,
              to store and retrieve user shopping carts. Dynamo’s Gossip       and conﬁguration management. Describing the details of
              based membership algorithm helps every node maintain in-         each of the solutions is beyond the scope of this paper, so
              formation about every other node. Dynamo can be deﬁned           wewillfocusonthecoredistributedsystemstechniquesused
              as a structured overlay with at most one-hop request rout-       in Cassandra: partitioning, replication, membership, failure
              ing. Dynamo detects updated conﬂicts using a vector clock        handling and scaling. All these modules work in synchrony
              scheme, but prefers a client side conﬂict resolution mecha-      to handle read/write requests. Typically a read/write re-
              nism. A write operation in Dynamo also requires a read to        quest for a key gets routed to any node in the Cassandra
              be performed for managing the vector timestamps. This is         cluster. The node then determines the replicas for this par-
              can be very limiting in environments where systems need          ticular key. For writes, the system routes the requests to
              to handle a very high write throughput. Bigtable[4] pro-         the replicas and waits for a quorum of replicas to acknowl-
              vides both structure and data distribution but relies on a       edge the completion of the writes. For reads, based on the
              distributed ﬁle system for its durability.                       consistency guarantees required by the client, the system ei-
              3.   DATAMODEL                                                   ther routes the requests to the closest replica or routes the
                                                                               requests to all replicas and waits for a quorum of responses.
                A table in Cassandra is a distributed multi dimensional        5.1   Partitioning
              mapindexedbyakey. Thevalueisanobjectwhichishighly
              structured. The row key in a table is a string with no size        Oneofthe key design features for Cassandra is the ability
              restrictions, although typically 16 to 36 bytes long. Every      to scale incrementally. This requires, the ability to dynam-
              operation under a single row key is atomic per replica no        ically partition the data over the set of nodes (i.e., storage
              matter how many columns are being read or written into.          hosts) in the cluster. Cassandra partitions data across the
              Columns are grouped together into sets called column fam-        cluster using consistent hashing [11] but uses an order pre-
              ilies very much similar to what happens in the Bigtable[4]       serving hash function to do so. In consistent hashing the
              system. Cassandra exposes two kinds of columns families,         output range of a hash function is treated as a ﬁxed circular
              Simple and Super column families. Super column families          space or “ring” (i.e. the largest hash value wraps around
              can be visualized as a column family within a column family.     to the smallest hash value). Each node in the system is as-
                Furthermore, applications can specify the sort order of        signed a random value within this space which represents its
              columns within a Super Column or Simple Column family.           position on the ring. Each data item identiﬁed by a key is
              The system allows columns to be sorted either by time or         assigned to a node by hashing the data item’s key to yield
              by name. Time sorting of columns is exploited by applica-        its position on the ring, and then walking the ring clockwise
              tion like Inbox Search where the results are always displayed    to ﬁnd the ﬁrst node with a position larger than the item’s
              in time sorted order. Any column within a column family          position. This node is deemed the coordinator for this key.
              is accessed using the convention column family : column          The application speciﬁes this key and the Cassandra uses it
              and any column within a column family that is of type            to route requests. Thus, each node becomes responsible for
              super is accessed using the convention column family :           the region in the ring between it and its predecessor node
              super column : column. A very good example of the su-            on the ring. The principal advantage of consistent hashing
              per column family abstraction power is given in Section 6.1.     is that departure or arrival of a node only aﬀects its im-
              TypicallyapplicationsuseadedicatedCassandraclusterand            mediate neighbors and other nodes remain unaﬀected. The
              manage them as part of their service. Although the system        basic consistent hashing algorithm presents some challenges.
              supports the notion of multiple tables all deployments have      First, the random position assignment of each node on the
              only one table in their schema.                                  ring leads to non-uniform data and load distribution. Sec-
                                                                               ond, the basic algorithm is oblivious to the heterogeneity in
              4.   API                                                         the performance of nodes. Typically there exist two ways to
                                                                               address this issue: One is for nodes to get assigned to multi-
                The Cassandra API consists of the following three simple       ple positions in the circle (like in Dynamo), and the second
              methods.                                                         is to analyze load information on the ring and have lightly
                 • insert(table,key,rowMutation)                               loaded nodes move on the ring to alleviate heavily loaded
                                                                               nodes as described in [17]. Cassandra opts for the latter as
                 • get(table,key,columnName)                                   it makes the design and implementation very tractable and
                                                                               helps to make very deterministic choices about load balanc-
                 • delete(table,key,columnName)                                ing.
              5.2    Replication                                                  failure detection module emits a value which represents a
                 Cassandra uses replication to achieve high availability and      suspicion level for each of monitored nodes. This value is
              durability. Each data item is replicated at N hosts, where N        deﬁned as Φ. The basic idea is to express the value of Φ on
              is the replication factor conﬁgured“per-instance”. Each key,        a scale that is dynamically adjusted to reﬂect network and
              k, is assigned to a coordinator node (described in the previ-       load conditions at the monitored nodes.
              ous section). The coordinator is in charge of the replication         Φ has the following meaning: Given some threshold Φ,
              of the data items that fall within its range.     In addition       and assuming that we decide to suspect a node A when Φ =
              to locally storing each key within its range, the coordinator       1, then the likelihood that we will make a mistake (i.e., the
              replicates these keys at the N-1 nodes in the ring. Cassandra       decision will be contradicted in the future by the reception
              providestheclientwithvariousoptionsforhowdataneedsto                of a late heartbeat) is about 10%. The likelihood is about
              be replicated. Cassandra provides various replication poli-         1%with Φ = 2, 0.1% with Φ = 3, and so on. Every node in
              cies such as“Rack Unaware”,“Rack Aware”(within a data-              the system maintains a sliding window of inter-arrival times
              center) and“Datacenter Aware”. Replicas are chosen based            of gossip messages from other nodes in the cluster. The
              on the replication policy chosen by the application. If cer-        distribution of these inter-arrival times is determined and
              tain application chooses “Rack Unaware” replication strat-          Φ is calculated. Although the original paper suggests that
              egy then the non-coordinator replicas are chosen by picking         the distribution is approximated by the Gaussian distribu-
              N-1 successors of the coordinator on the ring. For “Rack            tion we found the Exponential Distribution to be a better
              Aware”and“Datacenter Aware”strategies the algorithm is              approximation, because of the nature of the gossip channel
              slightly more involved.   Cassandra system elects a leader          and its impact on latency. To our knowledge our implemen-
              amongst its nodes using a system called Zookeeper[13]. All          tation of the Accrual Failure Detection in a Gossip based
              nodes on joining the cluster contact the leader who tells           setting is the ﬁrst of its kind.  Accrual Failure Detectors
              them for what ranges they are replicas for and leader makes         are very good in both their accuracy and their speed and
              a concerted eﬀort to maintain the invariant that no node            they also adjust well to network conditions and server load
              is responsible for more than N-1 ranges in the ring. The            conditions.
              metadata about the ranges a node is responsible is cached           5.4    Bootstrapping
              locally at each node and in a fault-tolerant manner inside
              Zookeeper - this way a node that crashes and comes back up            Whenanodestartsfor the ﬁrst time, it chooses a random
              knows what ranges it was responsible for. We borrow from            token for its position in the ring. For fault tolerance, the
              Dynamo parlance and deem the nodes that are responsible             mapping is persisted to disk locally and also in Zookeeper.
              for a given range the“preference list”for the range.                The token information is then gossiped around the cluster.
                 Asis explained in Section 5.1 every node is aware of every       This is how we know about all nodes and their respective po-
              other node in the system and hence the range they are re-           sitions in the ring. This enables any node to route a request
              sponsible for. Cassandra provides durability guarantees in          for a key to the correct node in the cluster. In the bootstrap
              the presence of node failures and network partitions by re-         case, when a node needs to join a cluster, it reads its conﬁgu-
              laxing the quorum requirements as described in Section5.2.          ration ﬁle which contains a list of a few contact points within
              Data center failures happen due to power outages, cooling           the cluster. We call these initial contact points, seeds of the
              failures, network failures, and natural disasters. Cassandra        cluster. Seeds can also come from a conﬁguration service
              is conﬁgured such that each row is replicated across multiple       like Zookeeper.
              data centers. In essence, the preference list of a key is con-        In Facebook’s environment node outages (due to failures
              structed such that the storage nodes are spread across mul-         and maintenance tasks) are often transient but may last for
              tiple datacenters. These datacenters are connected through          extended intervals. Failures can be of various forms such
              high speed network links. This scheme of replicating across         as disk failures, bad CPU etc. A node outage rarely signi-
              multiple datacenters allows us to handle entire data center         ﬁes a permanent departure and therefore should not result
              failures without any outage.                                        in re-balancing of the partition assignment or repair of the
              5.3    Membership                                                   unreachable replicas. Similarly, manual error could result
                                                                                  in the unintentional startup of new Cassandra nodes. To
                 Cluster membership in Cassandra is based on Scuttle-             that eﬀect every message contains the cluster name of each
              butt[19], a very eﬃcient anti-entropy Gossip based mech-            Cassandra instance. If a manual error in conﬁguration led
              anism. The salient feature of Scuttlebutt is that it has very       to a node trying to join a wrong Cassandra instance it can
              eﬃcient CPU utilization and very eﬃcient utilization of the         thwarted based on the cluster name. For these reasons, it
              gossip channel. Within the Cassandra system Gossip is not           was deemed appropriate to use an explicit mechanism to
              only used for membership but also to disseminate other sys-         initiate the addition and removal of nodes from a Cassan-
              tem related control state.                                          dra instance. An administrator uses a command line tool
                                                                                  or a browser to connect to a Cassandra node and issue a
               5.3.1   Failure Detection                                          membership change to join or leave the cluster.
                 Failure detection is a mechanism by which a node can             5.5    Scaling the Cluster
              locally determine if any other node in the system is up or
              down. InCassandrafailuredetectionisalsousedtoavoidat-                 Whenanewnodeisaddedintothesystem,itgetsassigned
              tempts to communicate with unreachable nodes during var-            a token such that it can alleviate a heavily loaded node.
              ious operations. Cassandra uses a modiﬁed version of the Φ          This results in the new node splitting a range that some
              Accrual Failure Detector[8]. The idea of an Accrual Failure         other node was previously responsible for. The Cassandra
              Detection is that the failure detection module doesn’t emit         bootstrap algorithm is initiated from any other node in the
              a Boolean value stating a node is up or down. Instead the           system by an operator using either a command line utility
              or the Cassandra web dashboard. The node giving up the             request arrives at any node in the cluster the state machine
              data streams the data over to the new node using kernel-           morphs through the following states (i) identify the node(s)
              kernel copy techniques. Operational experience has shown           that own the data for the key (ii) route the requests to the
              that data can be transferred at the rate of 40 MB/sec from         nodes and wait on the responses to arrive (iii) if the replies
              a single node. We are working on improving this by having          do not arrive within a conﬁgured timeout value fail the re-
              multiple replicas take part in the bootstrap transfer thereby      quest and return to the client (iv) ﬁgure out the latest re-
              parallelizing the eﬀort, similar to Bittorrent.                    sponse based on timestamp (v) schedule a repair of the data
                                                                                 at any replica if they do not have the latest piece of data.
              5.6    Local Persistence                                           For sake of exposition we do not talk about failure scenarios
                 The Cassandra system relies on the local ﬁle system for         here. The system can be conﬁgured to perform either syn-
              datapersistence. The data is represented on disk using a for-      chronous or asynchronous writes. For certain systems that
              matthatlends itself to eﬃcient data retrieval. Typical write       require high throughput we rely on asynchronous replica-
              operation involves a write into a commit log for durability        tion. Here the writes far exceed the reads that come into
              and recoverability and an update into an in-memory data            the system. During the synchronous case we wait for a quo-
              structure. The write into the in-memory data structure is          rum of responses before we return a result to the client.
              performed only after a successful write into the commit log.          In any journaled system there needs to exist a mechanism
              We have a dedicated disk on each machine for the commit            for purging commit log entries. In Cassandra we use a rolling
              log since all writes into the commit log are sequential and        a commit log where a new commit log is rolled out after an
              so we can maximize disk throughput. When the in-memory             older one exceeds a particular, conﬁgurable, size. We have
              data structure crosses a certain threshold, calculated based       found that rolling commit logs after 128MB size seems to
              on data size and number of objects, it dumps itself to disk.       work very well in our production workloads. Every com-
              This write is performed on one of many commodity disks             mit log has a header which is basically a bit vector whose
              that machines are equipped with. All writes are sequential         size is ﬁxed and typically more than the number of column
              to disk and also generate an index for eﬃcient lookup based        families that a particular system will ever handle. In our
              on row key. These indices are also persisted along with the        implementation we have an in-memory data structure and a
              data ﬁle. Over time many such ﬁles could exist on disk and         data ﬁle that is generated per column family. Every time the
              a merge process runs in the background to collate the dif-         in-memory data structure for a particular column family is
              ferent ﬁles into one ﬁle. This process is very similar to the      dumped to disk we set its bit in the commit log stating that
              compaction process that happens in the Bigtable system.            this column family has been successfully persisted to disk.
                 Atypical read operation ﬁrst queries the in-memory data         This is an indication that this piece of information is already
              structure before looking into the ﬁles on disk. The ﬁles are       committed. These bit vectors are per commit log and also
              looked at in the order of newest to oldest. When a disk            maintained in memory. Every time a commit log is rolled
              lookup occurs we could be looking up a key in multiple ﬁles        its bit vector and all the bit vectors of commit logs rolled
              on disk. In order to prevent lookups into ﬁles that do not         prior to it are checked. If it is deemed that all the data
              contain the key, a bloom ﬁlter, summarizing the keys in            has been successfully persisted to disk then these commit
              the ﬁle, is also stored in each data ﬁle and also kept in          logs are deleted. The write operation into the commit log
              memory. This bloom ﬁlter is ﬁrst consulted to check if the         can either be in normal mode or in fast sync mode. In the
              key being looked up does indeed exist in the given ﬁle. A key      fast sync mode the writes to the commit log are buﬀered.
              in a column family could have many columns. Some special           This implies that there is a potential of data loss on ma-
              indexing is required to retrieve columns which are further         chine crash. In this mode we also dump the in-memory data
              away from the key. In order to prevent scanning of every           structure to disk in a buﬀered fashion. Traditional databases
              columnondiskwemaintaincolumnindiceswhichallowusto                  are not designed to handle particularly high write through-
              jumptotheright chunk on disk for column retrieval. As the          put. Cassandra morphs all writes to disk into sequential
              columns for a given key are being serialized and written out       writes thus maximizing disk write throughput. Since the
              to disk we generate indices at every 256K chunk boundary.          ﬁles dumped to disk are never mutated no locks need to be
              This boundary is conﬁgurable, but we have found 256K to            taken while reading them. The server instance of Cassandra
              work well for us in our production workloads.                      is practically lockless for read/write operations. Hence we
                                                                                 do not need to deal with or handle the concurrency issues
              5.7    Implementation Details                                      that exist in B-Tree based database implementations.
                 The Cassandra process on a single machine is primarily             The Cassandra system indexes all data based on primary
              consists of the following abstractions: partitioning module,       key. The data ﬁle on disk is broken down into a sequence
              the cluster membership and failure detection module and            of blocks. Each block contains at most 128 keys and is de-
              the storage engine module. Each of these modules rely on an        marcated by a block index. The block index captures the
              eventdrivensubstratewherethemessageprocessingpipeline              relative oﬀset of a key within the block and the size of its
              and the task pipeline are split into multiple stages along the     data. When an in-memory data structure is dumped to disk
              line of the SEDA[20] architecture. Each of these modules           a block index is generated and their oﬀsets written out to
              has been implemented from the ground up using Java. The            disk as indices. This index is also maintained in memory for
              cluster membership and failure detection module, is built on       fast access. A typical read operation always looks up data
              top of a network layer which uses non-blocking I/O. All sys-       ﬁrst in the in-memory data structure. If found the data is
              tem control messages rely on UDP based messaging while             returned to the application since the in-memory data struc-
              the application related messages for replication and request       ture contains the latest data for any key. If not found then
              routing relies on TCP. The request routing modules are im-         we perform disk I/O against all the data ﬁles on disk in re-
              plementedusingacertainstatemachine. Whenaread/write                verse time order. Since we are always looking for the latest
              data we look into the latest ﬁle ﬁrst and return if we ﬁnd           hooks to repair nodes when disk fail. This is however
              the data. Over time the number of data ﬁles will increase            an administrative operation.
              on disk. We perform a compaction process, very much like
              the Bigtable system, which merges multiple ﬁles into one;          • Although Cassandra is a completely decentralized sys-
              essentially merge sort on a bunch of sorted data ﬁles. The           tem we have learned that having some amount of co-
              system will always compact ﬁles that are close to each other         ordination is essential to making the implementation
              with respect to size i.e there will never be a situation where a     of some distributed features tractable. For example
              100GBﬁle is compacted with a ﬁle which is less than 50GB.            Cassandra is integrated with Zookeeper, which can be
              Periodically a major compaction process is run to compact            used for various coordination tasks in large scale dis-
              all related data ﬁles into one big ﬁle. This compaction pro-         tributed systems. We intend to use the Zookeeper ab-
              cess is a disk I/O intensive operation. Many optimizations           straction for some key features which actually do not
              can be put in place to not aﬀect in coming read requests.            come in the way of applications that use Cassandra as
                                                                                   the storage engine.
              6.   PRACTICALEXPERIENCES                                       6.1   FacebookInboxSearch
                In the process of designing, implementing and maintaining       For Inbox Search we maintain a per user index of all mes-
              Cassandra we gained a lot of useful experience and learned      sages that have been exchanged between the sender and the
              numerous lessons. One very fundamental lesson learned was       recipients of the message. There are two kinds of search fea-
              nottoaddanynewfeaturewithoutunderstandingtheeﬀects              tures that are enabled today (a) term search (b) interactions
              of its usage by applications. Most problematic scenarios do     - given the name of a person return all messages that the
              not stem from just node crashes and network partitions. We      user might have ever sent or received from that person. The
              share just a few interesting scenarios here.                    schema consists of two column families. For query (a) the
                 • Before launching the Inbox Search application we had       user id is the key and the words that make up the message
                   to index 7TB of inbox data for over 100M users, then       become the super column. Individual message identiﬁers
                   stored in our MySQL[1] infrastructure, and load it into    of the messages that contain the word become the columns
                   the Cassandra system. The whole process involved           within the super column. For query (b) again the user id is
                   running Map/Reduce[7] jobs against the MySQL data          the key and the recipients id’s are the super columns. For
                   ﬁles, indexing them and then storing the reverse-index     each of these super columns the individual message identi-
                   in Cassandra. The M/R process actually behaves as          ﬁers are the columns. In order to make the searches fast
                   the client of Cassandra. We exposed some background        Cassandra provides certain hooks for intelligent caching of
                   channels for the M/R process to aggregate the re-          data. For instance when a user clicks into the search bar
                   verse index per user and send over the serialized data     an asynchronous message is sent to the Cassandra cluster
                   over to the Cassandra instance, to avoid the serializa-    to prime the buﬀer cache with that user’s index. This way
                   tion/deserialization overhead. This way the Cassandra      when the actual search query is executed the search results
                   instance is only bottlenecked by network bandwidth.        are likely to already be in memory. The system currently
                                                                              stores about 50+TB of data on a 150 node cluster, which
                 • Most applications only require atomic operation per        is spread out between east and west coast data centers. We
                   key per replica. However there have been some appli-       show some production measured numbers for read perfor-
                   cations that have asked for transactional mainly for the   mance.
                   purpose of maintaining secondary indices. Most devel-
                   opers with years of development experience working             Latency Stat    Search Interactions  Term Search
                   with RDBMS’s ﬁnd this a very useful feature to have.           Min                   7.69ms               7.78ms
                   Weareworkingonamechanismtoexposesuchatomic                     Median               15.69ms              18.27ms
                   operations.                                                    Max                  26.13ms              44.41ms
                 • WeexperimentedwithvariousimplementationsofFail-            7.   CONCLUSION
                   ure Detectors such as the ones described in [15] and [5].
                   Our experience had been that the time to detect fail-        Wehave built, implemented, and operated a storage sys-
                   ures increased beyond an acceptable limit as the size      temproviding scalability, high performance, and wide appli-
                   of the cluster grew. In one particular experiment in a     cability. We have empirically demonstrated that Cassandra
                   cluster of 100 nodes time to taken to detect a failed      can support a very high update throughput while deliver-
                   node was in the order of two minutes. This is prac-        ing low latency. Future works involves adding compression,
                   tically unworkable in our environments. With the ac-       ability to support atomicity across keys and secondary index
                   crual failure detector with a slightly conservative value  support.
                   of PHI, set to 5, the average time to detect failures in
                   the above experiment was about 15 seconds.                 8.   ACKNOWLEDGEMENTS
                 • Monitoring is not to be taken for granted. The Cas-          Cassandra system has beneﬁtted greatly from feedback
                   sandra system is well integrated with Ganglia[12], a       from many individuals within Facebook. In addition we
                   distributed performance monitoring tool. We expose         thank Karthik Ranganathan who indexed all the existing
                   various system level metrics to Ganglia and this has       data in MySQL and moved it into Cassandra for our ﬁrst
                   helped us understand the behavior of the system when       production deployment. We would also like to thank Dan
                   subject to our production workload. Disks fail for no      Dumitriu from EPFL for his valuable suggestions about [19]
                   apparent reasons. The bootstrap algorithm has some         and [8].
             9.  REFERENCES                                                 Computing, 30:2004, 2004.
              [1] MySQL AB. Mysql.                                     [13] Benjamin Reed and Flavio Junquieira. Zookeeper.
              [2] Atul Adya, William J. Bolosky, Miguel Castro, Gerald [14] Peter Reiher, John Heidemann, David Ratner, Greg
                 Cermak, Ronnie Chaiken, John R. Douceur, Jon               Skinner, and Gerald Popek. Resolving ﬁle conﬂicts in
                 Howell, Jacob R. Lorch, Marvin Theimer, and                the ﬁcus ﬁle system. In USTC’94: Proceedings of the
                 Roger P. Wattenhofer. Farsite: Federated, available,       USENIX Summer 1994 Technical Conference on
                 and reliable storage for an incompletely trusted           USENIX Summer 1994 Technical Conference, pages
                 environment. In In Proceedings of the 5th Symposium        12–12, Berkeley, CA, USA, 1994. USENIX
                 on Operating Systems Design and Implementation             Association.
                 (OSDI, pages 1–14, 2002.                              [15] Robbert Van Renesse, Yaron Minsky, and Mark
              [3] Mike Burrows. The chubby lock service for                 Hayden. A gossip-style failure detection service. In
                                                                                   ˇ
                 loosely-coupled distributed systems. In OSDI ’06:          Service,T Proc. Conf. Middleware, pages 55–70, 1996.
                 Proceedings of the 7th symposium on Operating         [16] Mahadev Satyanarayanan, James J. Kistler, Puneet
                 systems design and implementation, pages 335–350,          Kumar, Maria E. Okasaki, Ellen H. Siegel, and
                 Berkeley, CA, USA, 2006. USENIX Association.               David C. Steere. Coda: A highly available ﬁle system
              [4] Fay Chang, Jeﬀrey Dean, Sanjay Ghemawat,                  for a distributed workstation environment. IEEE
                 Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows,         Trans. Comput., 39(4):447–459, 1990.
                 Tushar Chandra, Andrew Fikes, and Robert E.           [17] Ion Stoica, Robert Morris, David Liben-nowell,
                 Gruber. Bigtable: A distributed storage system for         David R. Karger, M. Frans Kaashoek, Frank Dabek,
                 structured data. In In Proceedings of the 7th              and Hari Balakrishnan. Chord: a scalable peer-to-peer
                 Conference on USENIX Symposium on Operating                lookup protocol for internet applications. IEEE/ACM
                 Systems Design and Implementation - Volume 7,              Transactions on Networking, 11:17–32, 2003.
                 pages 205–218, 2006.                                  [18] D. B. Terry, M. M. Theimer, Karin Petersen, A. J.
              [5] Abhinandan Das, Indranil Gupta, and Ashish                Demers, M. J. Spreitzer, and C. H. Hauser. Managing
                 Motivala. Swim: Scalable weakly-consistent                 update conﬂicts in bayou, a weakly connected
                 infection-style process group membership protocol. In      replicated storage system. In SOSP ’95: Proceedings
                 DSN ’02: Proceedings of the 2002 International             of the ﬁfteenth ACM symposium on Operating systems
                 Conference on Dependable Systems and Networks,             principles, pages 172–182, New York, NY, USA, 1995.
                 pages 303–312, Washington, DC, USA, 2002. IEEE             ACM.
                 Computer Society.                                     [19] Robbert van Renesse, Dan Mihai Dumitriu, Valient
              [6] Giuseppe de Candia, Deniz Hastorun, Madan                 Gough, and Chris Thomas. Eﬃcient reconciliation and
                 Jampani, Gunavardhan Kakulapati, Alex Pilchin,             ﬂow control for anti-entropy protocols. In Proceedings
                 Swaminathan Sivasubramanian, Peter Vosshall, and           of the 2nd Large Scale Distributed Systems and
                                               ˜                            Middleware Workshop (LADIS ’08), New York, NY,
                 Werner Vogels. Dynamo: amazonOs highly available
                 key-value store. In Proceedings of twenty-ﬁrst ACM         USA, 2008. ACM.
                 SIGOPS symposium on Operating systems principles,     [20] Matt Welsh, David Culler, and Eric Brewer. Seda: an
                 pages 205–220. ACM, 2007.                                  architecture for well-conditioned, scalable internet
              [7] Jeﬀrey Dean and Sanjay Ghemawat. Mapreduce:               services. In SOSP ’01: Proceedings of the eighteenth
                 simpliﬁed data processing on large clusters. Commun.       ACMsymposium on Operating systems principles,
                 ACM, 51(1):107–113, 2008.                                  pages 230–243, New York, NY, USA, 2001. ACM.
              [8] Xavier D´efago, P´eter Urb´an, Naohiro Hayashibara,
                 and Takuya Katayama. The φ accrual failure detector.
                 In RR IS-RR-2004-010, Japan Advanced Institute of
                 Science and Technology, pages 66–78, 2004.
              [9] Sanjay Ghemawat, Howard Gobioﬀ, and Shun-Tak
                 Leung. The google ﬁle system. In SOSP ’03:
                 Proceedings of the nineteenth ACM symposium on
                 Operating systems principles, pages 29–43, New York,
                 NY, USA, 2003. ACM.
             [10] Jim Gray and Pat Helland. The dangers of replication
                 and a solution. In In Proceedings of the 1996 ACM
                 SIGMOD International Conference on Management of
                 Data, pages 173–182, 1996.
             [11] David Karger, Eric Lehman, Tom Leighton, Matthew
                 Levine, Daniel Lewin, and Rina Panigrahy. Consistent
                 hashing and random trees: Distributed caching
                 protocols for relieving hot spots on the world wide
                 web. In In ACM Symposium on Theory of Computing,
                 pages 654–663, 1997.
             [12] Matthew L. Massie, Brent N. Chun, and David E.
                 Culler. The ganglia distributed monitoring system:
                 Design, implementation, and experience. Parallel
