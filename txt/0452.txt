                                                                                                                                                                 1
                           The Design and Implementation of FFTW3
                                                                 Matteo Frigo and Steven G. Johnson
                                                                               (Invited Paper)
                 Abstract—FFTW is an implementation of the discrete Fourier                     of a multi-dimensional array. (Most implementations sup-
               transform (DFT) that adapts to the hardware in order to                          port only a single DFT of contiguous data.)
               maximize performance. This paper shows that such an approach                  • FFTW supports DFTs of real data, as well as of real
               can yield an implementation that is competitive with hand-                       symmetric/antisymmetric data (also called discrete co-
               optimized libraries, and describes the software structure that                   sine/sine transforms).
               makes our current FFTW3 version ﬂexible and adaptive. We
               further discuss a new algorithm for real-data DFTs of prime size,             The interaction of the user with FFTW occurs in two
               a new way of implementing DFTs by means of machine-speciﬁc                 stages: planning, in which FFTW adapts to the hardware,
               “SIMD” instructions, and how a special-purpose compiler can                and execution, in which FFTW performs useful work for the
               derive optimized implementations of the discrete cosine and sine           user. To compute a DFT, the user ﬁrst invokes the FFTW
               transforms automatically from a DFT algorithm.                             planner, specifying the problem to be solved. The problem is
                 Index Terms—FFT, adaptive software, Fourier transform,                   a data structure that describes the “shape” of the input data—
               cosine transform, Hartley transform, I/O tensor.                           array sizes and memory layouts—but does not contain the data
                                                                                          itself. In return, the planner yields a plan, an executable data
                                       I. INTRODUCTION                                    structure that accepts the input data and computes the desired
                    FTW [1] is a widely used free-software library that                   DFT. Afterwards, the user can execute the plan as many times
               Fcomputes the discrete Fourier transform (DFT) and its                     as desired.
               various special cases. Its performance is competitive even with               The FFTW planner works by measuring the actual run time
               vendor-optimized programs, but unlike these programs, FFTW                 of many different plans and by selecting the fastest one. This
               is not tuned to a ﬁxed machine. Instead, FFTW uses a planner               process is analogous to what a programmer would do by hand
               to adapt its algorithms to the hardware in order to maximize               whentuningaprogramtoaﬁxedmachine,butinFFTW’scase
               performance. The input to the planner is a problem, a multi-               no manual intervention is required. Because of the repeated
               dimensional loop of multi-dimensional DFTs. The planner                    performance measurements, however, the planner tends to be
               applies a set of rules to recursively decompose a problem into             time-consuming. In performance-critical applications, many
               simpler sub-problems of the same type. “Sufﬁciently simple”                transforms of the same size are typically required, and there-
               problems are solved directly by optimized, straight-line code              fore a large one-time cost is usually acceptable. Otherwise,
               that is automatically generated by a special-purpose compiler.             FFTWprovidesamodeofoperationwheretheplannerquickly
               This paper describes the overall structure of FFTW as well as              returns a “reasonable” plan that is not necessarily the fastest.
               the speciﬁc improvements in FFTW3, our latest version.                        The planner generates plans according to rules that recur-
                 FFTWisfast, but its speed does not come at the expense of                sively decompose a problem into simpler sub-problems. When
               ﬂexibility. In fact, FFTW is probably the most ﬂexible DFT                 the problem becomes “sufﬁciently simple,” FFTW produces
               library available:                                                         a plan that calls a fragment of optimized straight-line code
                 • FFTW is written in portable C and runs well on many                    that solves the problem directly. These fragments are called
                    architectures and operating systems.                                  codelets in FFTW’s lingo. You can envision a codelet as
                 • FFTW computes DFTs in O(nlogn) time for any                            computing a “small” DFT, but many variations and special
                    length n. (Most other DFT implementations are either                  cases exist. For example, a codelet might be specialized to
                                                                                          compute the DFT of real input (as opposed to complex).
                    restricted to a subset of sizes or they become Θ(n2) for              FFTW’s speed depends therefore on two factors. First, the
                    certain values of n, for example when n is prime.)                    decomposition rules must produce a space of plans that is rich
                 • FFTW imposes no restrictions on the rank (dimension-                   enough to contain “good” plans for most machines. Second,
                    ality) of multi-dimensional transforms. (Most other im-               the codelets must be fast, since they ultimately perform all the
                    plementations are limited to one-dimensional, or at most              real work.
                    two- and three-dimensional data.)                                        FFTW’s codelets are generated automatically by a special-
                 • FFTW supports multiple and/or strided DFTs; for exam-                  purpose compiler called genfft. Most users do not interact
                    ple, to transform a 3-component vector ﬁeld or a portion              with genfft at all: the standard FFTW distribution contains
                 M. Frigo is with the IBM Austin Research Laboratory, 11501 Burnet        a set of about 150 pre-generated codelets that cover the most
               Road, Austin, TX 78758. He was supported in part by the Defense Advanced   common uses. Users with special needs can use genfft
               Research Projects Agency (DARPA) under contract No. NBCH30390004.          to generate their own codelets. genfft is useful because
                 S. G. Johnson is with the Massachusetts Institute of Technology, 77      of the following features. From a high-level mathematical
               Mass. Ave. Rm. 2-388, Cambridge, MA 02139. He was supported in part        description of a DFT algorithm, genfft derives an optimized
               by the Materials Research Science and Engineering Center program of the
               National Science Foundation under award DMR-9400334.                       implementation automatically. From a complex-DFT algo-
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             rithm, genfft automatically derives an optimized algorithm            result. The most important FFT (and the one primarily used in
             for the real-input DFT. We take advantage of this property to         FFTW) is known as the “Cooley-Tukey” algorithm, after the
             implement real-data DFTs (Section VII), as well as to exploit         two authors who rediscovered and popularized it in 1965 [14],
             machine-speciﬁc “SIMD” instructions (Section IX). Similarly,          although it had been previously known as early as 1805 by
             genfftautomatically derives codelets for the discrete cosine          Gauss as well as by later re-inventors [15]. The basic idea
             (DCT) and sine (DST) transforms (Section VIII). We summa-             behind this FFT is that a DFT of a composite size n = n1n2
             rize genfft in Section VI, while a full description appears           can be re-expressed in terms of smaller DFTs of sizes n and
                                                                                                                                               1
             in [2].                                                               n2—essentially, as a two-dimensional DFT of size n1 × n2
                We have produced three major implementations of FFTW,              where the output is transposed. The choices of factorizations
             each building on the experience of the previous system.               of n, combined with the many different ways to implement the
             FFTW1 [3] (1997) introduced the idea of generating codelets           data re-orderings of the transpositions, have led to numerous
             automatically, and of letting a planner search for the best           implementation strategies for the Cooley-Tukey FFT, with
             combination of codelets. FFTW2 (1998) incorporated a new              many variants distinguished by their own names [16], [17].
             version of genfft [2]. genfft did not change much in                  FFTWimplementsaspaceofmanysuchvariants, as described
             FFTW3 (2003), but the runtime structure was completely                later, but here we derive the basic algorithm, identify its key
             rewritten to allow for a much larger space of plans. This paper       features, and outline some important historical variations and
             describes the main ideas common to all FFTW systems, the              their relation to FFTW.
             runtime structure of FFTW3, and the modiﬁcations to genfft              The Cooley-Tukey algorithm can be derived as follows. If
             since FFTW2.                                                          n can be factored into n = n1n2, Eq. (1) can be rewritten by
                Previous work on adaptive systems includes [3]–[11]. In            letting j = j n +j and k = k +k n . We then have:
                                                                                                1 2     2            1     2 1
             particular, SPIRAL [9], [10] is another system focused on                    Y[k +k n ]=                                             (2)
                                                                                              1     2 1
             optimization of Fourier transforms and related algorithms,                      n −1n −1                                 
                                                                                              2         1
             but it has distinct differences from FFTW. SPIRAL searches                      X X                            j k     j k     j k
                                                                                                          X[j n +j ]ω 1 1ω 2 1ω 2 2 .
             at compile-time over a space of mathematically equivalent                                          1 2     2   n1      n       n2
                                                                                             j =0      j =0
             formulas expressed in a “tensor-product” language, whereas                       2        1
             FFTW searches at runtime over the formalism discussed in              Thus, the algorithm computes n2 DFTs of size n1 (the inner
             Section IV, which explicitly includes low-level details, such as      sum), multiplies the result by the so-called twiddle factors
                                                                                    j k
             strides and memory alignments, that are not as easily expressed       ω 2 1, and ﬁnally computes n1 DFTs of size n2 (the outer
                                                                                    n
             using tensor products. SPIRAL generates machine-dependent             sum). This decomposition is then continued recursively. The
             code, whereas FFTW’s codelets are machine-independent.                literature uses the term radix to describe an n1 or n2 that
             FFTW’s search uses dynamic programming [12, chapter 16],              is bounded (often constant); the small DFT of the radix is
             while the SPIRAL project has experimented with a wider                traditionally called a butterﬂy.
             range of search strategies including machine-learning tech-             Many well-known variations are distinguished by the radix
             niques [13].                                                          alone. A decimation in time (DIT) algorithm uses n2 as the
                The remainder of this paper is organized as follows. We            radix, while a decimation in frequency (DIF) algorithm uses n1
             begin with a general overview of fast Fourier transforms in           as the radix. If multiple radices are used, e.g. for n composite
             Section II. Then, in Section III, we compare the performance          but not a prime power, the algorithm is called mixed radix.
             of FFTW and other DFT implementations. Section IV de-                 A peculiar blending of radix 2 and 4 is called split radix,
             scribes the space of plans explored by FFTW and how the               which was proposed to minimize the count of arithmetic
             FFTW planner works. Section V describes our experiences               operations [16]. (Unfortunately, as we argue in this paper,
             in the practical usage of FFTW. Section VI summarizes how             minimal-arithmetic, ﬁxed-factorization implementations tend
             genfft works. Section VII explains how FFTW computes                  to no longer be optimal on recent computer architectures.)
             DFTs of real data. Section VIII describes how genfft                  FFTW implements both DIT and DIF, is mixed-radix with
             generates DCT and DST codelets, as well as how FFTW                   radices that are adapted to the hardware, and often uses much
             handles these transforms in the general case. Section IX tells        larger radices (radix-32 is typical) than were once common.
             how FFTW exploits SIMD instructions.                                  (On the other end of the scale, a “radix” of roughly √n has
                                                                                   been called a four-step FFT [18], and we have found that one
                                   II. FFT OVERVIEW                                step of such a radix can be useful for large sizes in FFTW;
                The (forward, one-dimensional) discrete Fourier transform          see Section IV-D.1.)
             of an array X of n complex numbers is the array Y given by              Akey difﬁculty in implementing the Cooley-Tukey FFT is
                                                                                   that the n dimension corresponds to discontiguous inputs j
                                                                                             1                                                     1
                                                                                   in X but contiguous outputs k in Y, and vice-versa for n .
                                           n−1                                                                      1                              2
                                   Y[k] = XX[j]ωjk ,                        (1)    This is a matrix transpose for a single decomposition stage,
                                                      n                            and the composition of all such transpositions is a (mixed-
                                           j=0                                     base) digit-reversal permutation (or bit-reversal, for radix-2).
             where 0 ≤ k < n and ωn = exp(−2π√−1/n). Implemented                   The resulting necessity of discontiguous memory access and
             directly, Eq. (1) would require Θ(n2) operations; fast Fourier        data re-ordering hinders efﬁcient use of hierarchical memory
             transforms are O(nlogn) algorithms to compute the same                architectures (e.g., caches), so that the optimal execution order
                                                                                2
                Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                of an FFT for given hardware is non-obvious, and various                                   4000                                                   ipps
                                                                                                                                                                  fftw, out of place
                approaches have been proposed.                                                             3500                                                   fftw, in place
                   One ordering distinction is between recursion and iteration.                                                                                   mkl, in place
                                                                                                           3000                                                   mkl, out of place
                As expressed above, the Cooley-Tukey algorithm could be                                                                                           fftw, no simd
                                                                                                                                                                  takahashi
                thought of as deﬁning a tree of smaller and smaller DFTs;                                                                                         ooura
                                                                                                           2500                                                   fftpack
                for example, a textbook radix-2 algorithm would divide size                                                                                       green
                n into two transforms of size n/2, which are divided into                                  2000                                                   arprec
                four transforms of size n/4, and so on until a base case is
                reached (in principle, size 1). This might naturally suggest                             speed (mflops)1500
                a recursive implementation in which the tree is traversed                                  1000
                “depth-ﬁrst”—one size n/2 transform is solved completely
                before processing the other one, and so on. However, most                                   500
                traditional FFT implementations are non-recursive (with rare
                                                                                                               2   4  8   16  32 64  128256 512 1024204840968192163843276865536131072262144
                exceptions [19]) and traverse the tree “breadth-ﬁrst” [17]—                                   0
                in the radix-2 example, they would perform n (trivial) size-
                1 transforms, then n/2 combinations into size-2 transforms,
                then n/4 combinations into size-4 transforms, and so on, thus                       Fig. 1.   Comparison of double-precision 1d complex DFTs, power-of-two
                making log2n passes over the whole array. In contrast, as we                        sizes, on a 2.8GHz Pentium IV. Intel C/Fortran compilers v. 7.1, optimization
                discuss in Section IV-D.1, FFTW3 employs an explicitly re-                          ﬂags -O3 -xW (maximum optimization, enable automatic vectorizer).
                cursive strategy that encompasses both depth-ﬁrst and breadth-                         3000
                ﬁrst styles, favoring the former since it has some theoretical                                                                                        fftw, out of place
                                                                                                                                                                      fftw, in place
                and practical advantages.                                                                                                                             fftw, no simd
                   Asecond ordering distinction lies in how the digit-reversal                         2500                                                           takahashi
                                                                                                                                                                      mkl, out of place
                is performed. The classic approach is a single, separate digit-                                                                                       fftpack
                reversal pass following or preceding the arithmetic compu-                             2000                                                           mkl, in place
                tations. Although this pass requires only O(n) time [20], it
                can still be non-negligible, especially if the data is out-of-                         1500
                cache; moreover, it neglects the possibility that data-reordering
                during the transform may improve memory locality. Perhaps                             speed (mflops)1000
                the oldest alternative is the Stockham auto-sort FFT [17], [21],
                which transforms back and forth between two arrays with each                            500
                butterﬂy, transposing one digit each time, and was popular to
                improve contiguity of access for vector computers [22]. Alter-
                                                                                                            6  9   12 15  18  24 36  80  108210 504100019604725103682700075600165375
                natively, an explicitly recursive style, as in FFTW, performs                              0
                the digit-reversal implicitly at the “leaves” of its computation
                when operating out-of-place (Section IV-D.1). To operate in-
                place with O(1) scratch storage, one can interleave small ma-                       Fig. 2. Comparison of double-precision 1d complex DFTs, non-power-of-two
                trix transpositions with the butterﬂies [23]–[26], and a related                    sizes, on a 2.8GHz Pentium IV. Compiler and ﬂags as in Fig. 1.
                strategy in FFTW is described by Section IV-D.3. FFTW can
                also perform intermediate re-orderings that blend its in-place
                and out-of-place strategies, as described in Section V-C.                           tions, on most modern general-purpose processors, comparing
                   Finally, we should mention that there are many FFTs                              complex and real-data transforms in one to three dimensions
                entirely distinct from Cooley-Tukey. Three notable such algo-                       and for both single and double precisions. We generally found
                rithms are the prime-factor algorithm for gcd(n ,n ) = 1 [27,                       FFTW to be superior to other publicly available codes and
                                                                             1    2                 comparable to vendor-tuned libraries. The complete results
                page 619], along with Rader’s [28] and Bluestein’s [27],
                [29] algorithms for prime n. FFTW implements the ﬁrst two                           can be found at [1]. In this section, we present data for a
                in its codelet generator for hard-coded n (Section VI) and                          small sampling of representative codes for complex-data one-
                the latter two for general prime n. A new generalization of                         dimensional transforms on a few machines.
                Rader’s algorithm for prime-size real-data transforms is also                          We show the benchmark results as a series of graphs.
                discussed in Section VII. FFTW does not employ the Wino-                            Speed is measured in “MFLOPS,” deﬁned for a transform
                grad FFT [30], which minimizes the number of multiplications                        of size n as (5nlog2n)/t, where t is the time in µs for
                at the expense of a large number of addditions. (This tradeoff                      one transform, not including one-time initialization costs. This
                is not beneﬁcial on current processors that have specialized                        count of ﬂoating-point operations is based on the asymptotic
                hardware multipliers.)                                                              number of operations for the radix-2 Cooley-Tukey algorithm
                                      III. BENCHMARK RESULTS                                        (see [17, page 45]), although the actual count is lower for most
                                                                                                    DFT implementations. The MFLOPS measure should thus be
                   We have performed extensive benchmarks of FFTW’s per-                            viewed as a convenient scaling factor rather than as an absolute
                formance, along with that of over 50 other FFT implementa-                          indicator of CPU performance.
                                                                                                 3
                    Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                          7000                                                                      ipps                              2000                                                               fftw, out of place
                          6500                                                                      fftw, out of place                                                                                   fftw, in place
                                                                                                    mkl, in place                                                                                        ooura
                          6000                                                                      fftw, in place                                                                                       cxml
                          5500                                                                      mkl, out of place                                                                                    green
                                                                                                    fftw, no simd                     1500                                                               fftpack
                          5000                                                                      fftpack
                          4500                                                                      green
                                                                                                    singleton
                          4000                                                                      sorensen
                          3500                                                                      numerical recipes                 1000
                          3000
                        speed (mflops)2500                                                                                           speed (mflops)
                          2000                                                                                                          500
                          1500
                          1000
                           500
                                2   4    8   16   32  64   128 256  512 1024 20484096 8192163843276865536131072262144                       2    4   8    16  32   64  128  256 512  10242048 40968192 163843276865536131072262144
                              0                                                                                                            0
                    Fig. 3.      Comparison of single-precision 1d complex DFTs, power-of-two                                Fig. 5.         Comparison of double-precision 1d complex DFTs, power-of-
                    sizes, on a 2.8GHz Pentium IV. Compiler and ﬂags as in Fig. 1. Note that                                 two sizes, on an 833MHz Alpha EV6. Compaq C V6.2-505. Compaq
                    fftpack, which was originally designed for vectorizing compilers (or vice                                Fortran X1.0.1-1155. Optimization ﬂags: -newc -w0 -O5 -ansi alias
                    versa), beneﬁts somewhat from the automatic vectorization in this case.                                  -ansi args -fp reorder -tune host -arch host.
                             4000                                                               fftw, out of place                    2000                                                               fftw, out of place
                                                                                                fftw, in place                                                                                           fftw, in place
                             3500                                                               green                                                                                                    cxml
                                                                                                vdsp                                                                                                     green
                                                                                                ooura                                                                                                    fftpack
                             3000                                                               fftpack                               1500                                                               sorensen
                                                                                                arprec                                                                                                   singleton
                                                                                                                                                                                                         numerical recipes
                             2500
                             2000                                                                                                     1000
                            speed (mflops)1500                                                                                       speed (mflops)
                             1000                                                                                                       500
                               500
                                                                                                                                            2    4   8    16  32   64  128  256 512  10242048 40968192 163843276865536131072262144
                                                                                                                                           0
                                   2    4   8    16  32   64  128  256 512  1024 20484096 8192163843276865536131072262144
                                  0
                    Fig. 4.       Comparison of double-precision 1d complex DFTs, power-of-two                               Fig. 6.      Comparison of single-precision 1d complex DFTs, power-of-two
                    sizes, on a 2GHz PowerPC 970 (G5). Apple gcc v. 3.3, g77 v. 3.4 20031105                                 sizes, on an 833MHz Alpha EV6. Compilers and ﬂags as in Fig. 5.
                    (experimental). Optimization ﬂags -O3 -mcpu=970 -mtune=970. The
                    Apple vDSP library uses separate real/imaginary arrays to store complex
                    numbers, and therefore its performance is not stricly comparable with the
                    other codes, which use an array of real/imaginary pairs.                                                 results on the same machine.
                                                                                                                                 In addition to FFTW v. 3.0.1, the other codes benchmarked
                                                                                                                             are as follows (some for only one precision or machine):
                        Fig. 1 shows the benchmark results for power-of-two sizes,                                           arprec, “four-step” FFT implementation [18] (from the C++
                    in double precision, on a 2.8GHz Pentium IV with the Intel                                               ARPREC library, 2002); cxml, the vendor-tuned Compaq
                    compilers; in Fig. 2 are results for selected non-power-of-                                              Extended Math Library on Alpha; fftpack, the Fortran library
                                                              a b c d                                                        from [22]; green, free code by J. Green (C, 1998); mkl, the
                    two sizes of the form 2 3 5 7                            on the same machine; in
                    Fig. 3 are the single-precision power-of-two results. Note that                                          Intel Math Kernel Library v. 6.1 (DFTI interface) on the
                    only the FFTW, MKL (Intel), IPPS (Intel), and Takahashi                                                  Pentium IV; ipps, the Intel Integrated Performance Primitives,
                    libraries on this machine were speciﬁcally designed to ex-                                               Signal Processing, v. 3.0 on the Pentium IV; numerical recipes,
                    ploit the SSE/SSE2 SIMD instructions (see Section IX); for                                               the C four1routinefrom[31];ooura,afreecodebyT.Ooura
                    comparison, we also include FFTW (out-of-place) with SIMD                                                (CandFortran, 2001); singleton, a Fortran FFT [32]; sorensen,
                    disabled (“fftw, no simd”). In Fig. 4 are the power-of-two                                               a split-radix FFT [33]; takahashi, the FFTE library v. 3.2 by
                    double-precision results on a 2GHz PowerPC 970 (G5) with                                                 D. Takahashi (Fortran, 2004) [34]; and vdsp, the Apple vDSP
                    the Apple gcc 3.3 compiler. In Fig. 5 are the power-of-                                                  library on the G5.
                    two double-precision results on an 833MHz Alpha EV6 with                                                     We now offer some remarks to aid the interpretation of the
                    the Compaq compilers, and in Fig. 6 are the single-precision                                             performance results. The performance of all routines drops for
                                                                                                                          4
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             large problems, reﬂecting the cache hierarchy of the machine.        the complex number at memory location X+k (with pointer
             Performance is low for small problems as well, because of the        arithmetic in units of complex numbers). By convention, we
             overhead of calling a routine to do little work. FFTW is the         deﬁne the zero-dimensional problem dft({},{},I,O) to yield
             only library that exploits SIMD instructions for non-power-          the assignment O[0] := I[0].
             of-two sizes, which gives it an advantage on the Pentium IV            dft(N,{(n,ι,o)} ∪ V,I,O) is recursively deﬁned as a
             for this case. IPPS is limited to in-place contiguous inputs,        “loop” of n problems: for all 0 ≤ k < n, yield all assignments
             whereas MKL and FFTW allow for strided input. Assuming               in dft(N,V,I+k·ι,O+k·o).
             contiguous input gives some speed advantage on a machine               If two assignments write to the same memory location, the
             such as the Pentium IV where index computation is somewhat           DFTproblem is undeﬁned. Such nonsensical problems are not
             slow.                                                                normally encountered in practice, however, as discussed in
                                                                                  Section IV-B.
                            IV. THE STRUCTURE OF FFTW3                              One property of this deﬁnition is the fact that an I/O
                In this section, we discuss in detail how FFTW works.             tensor t is equivalent to t ∪ {(1,ι,o)}. That is, length-1
             Speciﬁcally, we discuss how FFTW represents the problem              DFTdimensions and length-1 loops can be eliminated. FFTW
             to be solved (Sections IV-A and IV-B), the set of plans that         therefore internally canonicalizes I/O tensors by removing all
             the planner considers during its search (Sections IV-C and IV-       I/O dimensions where n = 1. (Similarly, all I/O tensors of the
             D), and the internal operation of the planner (Section IV-E).        form t ∪ {(0,ι,o)} are equivalent.)
             For simplicity, this section considers complex DFTs only; we           We call N the size of the problem. The rank of a problem
             discuss real DFTs in Section VII.                                    is deﬁned to be the rank of its size (i.e., the dimensionality of
                Of these components, the representation of the problem to         the DFT). Similarly, we call V the vector size of the problem,
             be solved is a critical choice. Indeed, we view our deﬁnition        and the vector rank of a problem is correspondingly deﬁned to
             of a “problem” as a fundamental contribution of this paper.          be the rank of its vector size. One unusual feature of FFTW
             Because only problems that can be expressed can be solved,           is that the vector rank is arbitrary: FFTW is not restricted
             the representation of a problem determines an upper bound to         to vector sizes of rank 1. Intuitively, the vector size can be
             the space of plans that the planner can explore, and therefore       interpreted as a set of “loops” wrapped around a single DFT,
             it ultimately constrains FFTW’s performance.                         and we therefore refer to a single I/O dimension of V as
                                                                                  a vector loop. (Alternatively, one can view the problem as
                                                                                  deﬁning a multi-dimensional DFT over a vector space.) The
             A. Representation of problems in FFTW                                problem does not specify the order of execution of these loops,
                DFTproblemsinFFTWareexpressedintermsofstructures                  however, and therefore FFTW is free to choose the fastest or
             called I/O tensors, which in turn are described in terms of          most convenient order.
                                                                                    An I/O tensor for which ι       = o for all k is said to be
             ancillary structures called I/O dimensions. (I/O tensors are                                        k      k
             unrelated to the tensor-product notation of SPIRAL.) In this         in-place. Occasionally, the need arises to replace input strides
             section, we deﬁne these terms precisely.                             with output strides and vice versa. We deﬁne copy-i(t) to be
                An I/O dimension d is a triple d = (n,ι,o), where n is a          the I/O tensor {(n,ι,ι) | (n,ι,o) ∈ t}. Similarly, we deﬁne
             nonnegative integer called the length, ι is an integer called the    copy-o(t) to be the I/O tensor {(n,o,o) | (n,ι,o) ∈ t}.
             input stride, and o is an integer called the output stride. An         The two pointers I and O specify the memory addresses
             I/O tensor t = {d1,d2,...,dρ} is a set of I/O dimensions. The        of the input and output arrays, respectively. If I = O, we say
             nonnegative integer ρ = |t| is called the rank of the I/O tensor.    that the problem is in-place, otherwise the problem is out-of-
             ADFTproblem, denoted by dft(N,V,I,O), consists of two                place. FFTW uses explicit pointers for three reasons. First, we
             I/O tensors N and V, and of two pointers I and O. Roughly            can distinguish in-place from out-of-place problems, which is
             speaking, this describes |V| nested loops of |N|-dimensional         important because many FFT algorithms are inherently either
             DFTswith input data starting at memory location I and output         in-place or out-of-place, but not both. Second, SIMD instruc-
             data starting at O. We now give a more precise deﬁnition by          tions usually impose constraints on the memory alignment of
             induction on |V|, yielding a set of assignments from input           the data arrays; from the pointer, FFTW determines whether
             to output. Conceptually, all of the right-hand sides of these        SIMD instructions are applicable. Third, performance may
             assignments are evaluated before writing their values to the         dependontheactualmemoryaddressofthedata,inadditionto
             left-hand sides, a ﬁction that deﬁnes the behavior precisely,        the data layout, so an explicit pointer is in principle necessary
             e.g., when I = O. (See also the examples in Section IV-B.)           for maximum performance.
                dft(N,{},I,O), with ρ = |N|, is the ρ-dimensional DFT,
             deﬁned as follows. Let N = {(nℓ,ιℓ,oℓ) | 1 ≤ ℓ ≤ ρ}; for all         B. DFT problem examples
             output indices 0 ≤ k < n , yield the assignment
                                   ℓ     ℓ                                          The I/O tensor representation is sufﬁciently general to cover
                      " ρ         #             " ρ         # ρ
                       X                 X X Yjk                                  many situations that arise in practice, including some that are
                   O       k ·o     :=         I      j · ι       ω ℓ ℓ ,
                             ℓ   ℓ                     ℓ   ℓ       n
                                                                    ℓ             not usually considered to be instances of the DFT. We consider
                       ℓ=1             j ,...,j   ℓ=1         ℓ=1
                                        1    ρ                                    a few examples here.
             where each input index jℓ is summed from 0 to nℓ−1, ωn is a            An n1 ×n2 two-dimensional matrix is typically stored in
             primitive n-th root of unity as in Section II, and X[k] denotes      Cusing row-major format: size-n2 contiguous arrays for each
                                                                               5
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             row, stored as n1 consecutive blocks starting from a pointer      means of some DFT algorithm such as Cooley-Tukey. These
             I/O (for input/output). This memory layout is described by        three steps need not be executed in the stated order, however,
             the in-place I/O tensor X = {(n1,n2,n2),(n2,1,1)}. Per-           and in fact, almost every permutation and interleaving of these
             forming the n1×n2 two-dimensional DFT of this array corre-        three steps leads to a correct DFT plan. The choice of the set
             sponds to the rank-2, vector-rank-0 problem: dft(X,{},I,O).       of plans explored by the planner is critical for the usability
             The transform data can also be non-contiguous; for exam-          of the FFTW system: the set must be large enough to contain
                                                      ′
             ple, one could transform an n1 × n         subset of the ma-      the fastest possible plans, but it must be small enough to keep
                                                      2
             trix, with n′  ≤ n2, starting at the upper-left corner, by:       the planning time acceptable.
                          2
             dft({(n ,n ,n ),(n′,1,1)},{},I,O).                                   The remainder of this section enumerates the class of plans
                     1  2   2     2                                            considered by current FFTW planner. This particular set of
               Another possibility is the rank-1, vector-rank-1 problem
             that performs a “loop” of n1 one-dimensional DFTs of size         plans is reasonably simple, it can express a wide variety of
             n2 operating on all the contiguous rows of the matrix:            algorithms, and it seems to perform well on most architectures.
             dft({(n ,1,1)},{(n ,n ,n )},I,O). Conversely, to perform          We do not claim that this set is the absolute optimum: many
                     2            1   2   2                                    more possibilities exist that are a topic of future research,
             one-dimensional DFTs of the (discontiguous) columns of the
             matrix, one would use: dft({(n1,n2,n2)},{(n2,1,1)},I,O);          and the space of plans will likely change in future FFTW
             if n2 = 3, for example, this could be thought of as the size-     releases. The plans that we now describe usually perform some
             n1 one-dimensional DFT of a three-component “vector ﬁeld”         simple “atomic” operation, and it may not be apparent how
             (with “vector components” stored contiguously).                   these operations ﬁt together to actually compute DFTs, or why
               Additionally,    the    rank-0,    vector-rank-2     problem    certain operations are useful at all. We shall discuss these
             dft({},X,I,O) denotes a copy (loop of rank-0 DFTs)                matters in Section IV-D. For now, we ask for the reader’s
             of n1n2 complex numbers from I to O. (If I = O, the               patience while we describe the precise set of plans generated
             runtime cost of this copy is zero.) Morever, this is equivalent   by FFTW.
             to the problem dft({},{(n1n2,1,1)},I,O)—it is possible to            1) No-op plans:     The simplest plans are those that
             combine vector loops that, together, denote a constant-offset     do nothing. FFTW generates no-op plans for problems
             sequence of memory locations, and FFTW thus canonicalizes         dft(N,V,I,O) in the following two cases:
             all such vector loops internally.                                    • when V = {(0,ι,o)}, that is, no data is to be trans-
               Generally, rank-0 transforms may describe some in-place              formed; or
             permutation, such as a matrix transposition, if I = O. For           • when N = {}, I = O, and the I/O tensor V is in-place.
             example, to transpose the n1×n2 matrix to n2×n1, both stored           In this case, the transform reduces to a copy of the input
             in row-major order starting at I, one would use the rank-0,            array into itself, which requires no work.
             vector-rank-2 problem: dft({},{(n1,n2,1),(n2,1,n1)},I,I)          It is possible for the user to specify a no-op problem if one is
             (these two vector loops cannot be combined into a single          desired (FFTW solves it really quickly). More often, however,
             loop).                                                            no-op problems are generated by FFTW itself as a by-product
               Finally, one can imagine problems where the different DFTs      of buffering plans. (See Section IV-C.7.)
             in the vector loop or a multi-dimensional transform operate          2) Rank-0 plans: The rank-0 problem dft({},V,I,O)
             on overlapping data. For example, the “two-dimensional”           denotes a permutation of the input array into the output array.
             dft({(n ,1,1),(n ,1,1)},{},I,O) transforms a “matrix”             FFTW does not solve arbitrary rank-0 problems, only the
                     1         2                                               following two special cases that arise in practice.
             whose subsequent rows overlap in n2 − 1 elements. The                • When |V| = 1 and I 6= O, FFTW produces a plan that
             behavior of FFTW is undeﬁned in such cases, which are, in
             any case, prohibited by the ordinary user interface (Section V-        copies the input array into the output array. Depending
             A).                                                                    on the strides, the plan consists of a loop or, possibly,
                                                                                    of a call to the ANSI C function memcpy, which is
             C. The space of plans in FFTW                                          specialized to copy contiguous regions of memory. (The
                                                                                    case I = O is discussed in Section IV-C.1.)
               The FFTW planner, when given a problem, explores a                 • When |V| = 2, I = O, and the strides denote a matrix-
             space of valid plans for that problem and selects the plan             transposition problem, FFTW creates a plan that trans-
             (a particular composition of algorithmic steps in a speciﬁed           poses the array in-place. FFTW implements the square
             order of execution) that happens to execute fastest. Many              transposition dft({},{(n,ι,o),(n,o,ι)},I,O) by means
             plans exist that solve a given problem, however. Which plans           of the “cache-oblivious” algorithm from [35], which is
             does FFTW consider, exactly? This section addresses this and           fast and, in theory, uses the cache optimally regardless of
             related questions.                                                     the cache size. A generalization of this idea is employed
               Roughly speaking, to solve a general DFT problem, one                for non-square transpositions with a large common factor
             must perform three tasks. First, one must reduce a problem of          or a small difference between the dimensions [36], and
             arbitrary vector rank to a set of loops nested around a problem        otherwise the algorithm from [37] is used.
             of vector rank 0, i.e., a single (possibly multi-dimensional)        An important rank-0 problem that is describable but not
             DFT. Second, one must reduce the multi-dimensional DFT to         currently solvable in-place by FFTW is the general in-place
             a sequence of of rank-1 problems, i.e., one-dimensional DFTs.     digit-reversal permutation [20], which could be used for some
             Third, one must solve the rank-1, vector rank-0 problem by        DFT algorithms.
                                                                             6
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
               3) Rank-1 plans: Rank-1 DFT problems denote ordinary            be destroyed. DIF plans that do not destroy the input could
             one-dimensional Fourier transforms. FFTW deals with most          be devised, but we did not implement them because our main
             rank-1 problems as follows. (Other kinds of rank-1 plans exist,   use of DIF plans is for in-place transforms (Section IV-D.3).
             which apply in certain special cases such as DFTs of prime           4) Plans for higher ranks: These plans reduce a multi-
             size. See Section IV-C.7.)                                        dimensional DFT problem to problems of lower rank, which
                  a) Direct   plans:   When the DFT rank-1 problem             are then solved recursively.
             is “small enough,” FFTW produces a direct plan that                  Formally, to solve dft(N,V,I,O), where N = N ∪
                                                                                                                                          1
             solves the problem directly. This situation occurs for prob-      N, |N | ≥ 1 and |N | ≥ 1, FFTW generates a plan
                                                                                 2     1                  2
             lems dft({(n,ι,o)},V,I,O) where |V| ≤ 1 and n ∈                   that ﬁrst solves dft(N ,V ∪ N ,I,O), and then solves
                                                                                                        1          2
             {2,...,16,32,64}. These plans operate by calling a fragment       dft(copy-o(N2),copy-o(V∪N1),O,O).
             of C code (a codelet) specialized to solve problems of one           In principle, FFTW generates a plan for every suitable
             particular size. In FFTW, codelets are generated automatically    choice of the subsets N and N , but in practice we impose
                                                                                                        1        2
             by genfft, but it is possible for a user to add hand-written      certain restrictions on the possible choices in order to reduce
             machine-speciﬁc codelets if desired.                              the planning time. (See Section V-B.) A typical heuristic is to
               We impose the restriction that |V| ≤ 1 because of engi-         choose two sub-problems N and N of roughly equal rank,
                                                                                                            1        2
             neering tradeoffs. Informally speaking, a codelet for |V| = 0     where each input stride in N is smaller than any input stride
             consists of straight-line code, while a codelet for |V| = 1       in N .                        1
                                                                                    2
             consists of a vector loop wrapped around straight-line code.         5) Plans for higher vector ranks: These plans extract a
             Either codelets implement the loop or they don’t—allowing for     vector loop to reduce a DFT problem to a problem of lower
             both possibilities would require the duplication of the whole     vector rank, which is then solved recursively.
             set of codelets. In practice, |V| = 1 is more common than            Formally, to solve dft(N,V,I,O), where V = {(n,ι,o)}∪
             |V| = 0, and therefore FFTW takes the position that all           V1,FFTWgeneratesaloopthat,forallk suchthat0 ≤ k < n,
             direct problems have vector rank 1, converting the rank-0 I/O     invokes a plan for dft(N,V1,I+k ·ι,O+k ·o).
             tensor {} into the rank-1 I/O tensor {(1,0,0)}. We have not          Any of the vector loops of V could be extracted in this
             investigated the performance implications of codelets of higher   way, leading to a number of possible plans. To reduce the loop
             vector rank. For now, FFTW handles the general vector-rank        permutations that the planner must consider, however, FFTW
             case via Section IV-C.5.                                          only considers the vector loop that has either the smallest or
                  b) Cooley-Tukey plans:     For problems of the form          the largest ι; this often corresponds to the smallest or largest o
             dft({(n,ι,o)},V,I,O) where n = rm, FFTW generates                 as well, or commonly vice versa (which makes the best loop
             a plan that implements a radix-r Cooley-Tukey algorithm           order nonobvious).
             (Section II). (FFTW generates a plan for each suitable value         6) Indirect plans: Indirect plans transform a DFT problem
             of r, possibly in addition to a direct plan. The planner then     that requires some data shufﬂing (or discontiguous operation)
             selects the fastest.)                                             into a problem that requires no shufﬂing plus a rank-0 problem
               OfthemanyknownvariantsoftheCooley-Tukeyalgorithm,               that performs the shufﬂing.
             FFTW implements the following two, distinguished mainly              Formally, to solve dft(N,V,I,O) where |N| > 0, FFTW
             by whether the codelets multiply their inputs or outputs by       generates a plan that ﬁrst solves dft({},N ∪ V,I,O), and
             twiddle factors. (Again, if both apply, FFTW tries both.) As for  then solves dft(copy-o(N),copy-o(V),O,O). This plan ﬁrst
             direct plans, we restrict |V| to be ≤ 1 because of engineering    rearranges the data, then solves the problem in place. If the
             tradeoffs. (In the following, we use n1 and n2 from Eq. (2).)     problem is in-place or the user has indicated that the input can
               A decimation in time (DIT) plan uses a radix r = n2             be destroyed, FFTW also generates a dual plan: ﬁrst solve
             (and thus m = n1): it ﬁrst solves dft({(m,r ·ι,o)},V ∪            dft(copy-i(N),copy-i(V),I,I), and then solve dft({},N ∪
             {(r,ι,m·o)},I,O), then multiplies the output array O by the       V,I,O) (solve in place, then rearrange).
             twiddle factors, and ﬁnally solves dft({(r,m · o,m · o)},V∪          7) Other plans: For completeness, we now brieﬂy mention
             {(m,o,o)},O,O). For performance, the last two steps are           the other kinds of plans that are implemented in FFTW.
             not planned independently, but are fused together in a single        Buffering plans solve a problem out-of-place to a temporary
             “twiddle” codelet—a fragment of C code that multiplies its        buffer and then copy the result to the output array. These plans
             input by the twiddle factors and performs a DFT of size r,        serve two purposes. First, it may be inconvenient or impossible
             operating in-place on O. FFTW contains one such codelet for       to solve a DFT problem without using extra memory space,
             each r ∈ {2,...,16,32,64}.                                        and these plans provide the necessary support for these cases
               A decimation in frequency (DIF) plan uses r = n1 (and           (e.g. in-place transforms). Second, if the input/output arrays
             thus m = n ); it operates backwards with respect to a             are noncontiguous in memory, operating on a contiguous
                            2
             DIT plan. The plan ﬁrst solves dft({(r,m·ι,m·ι)},V ∪              buffer might be faster because of better interaction with caches
             {(m,ι,ι)},I,I), then multiplies the input array I by the          and the rest of the memory subsystem. Similarly, buffered DIT
             twiddle factors, and ﬁnally solves dft({(m,ι,r · o)},V ∪          (or DIF) plans apply the twiddle codelets of Section IV-C.3.b
             {(r,m·ι,o)},I,O). Again, for performance, the ﬁrst two            by copying a batch of inputs to a contiguous buffer, executing
             steps are fused together in a single codelet. Because DIF plans   the codelets, and copying back.
                                                                                                                         2
             destroy the input array, however, FFTW generates them only           Generic plans implement a naive Θ(n ) algorithm to solve
             if I = O or if the user explicitly indicates that the input can   one-dimensional DFTs. Similarly, Rader plans implement the
                                                                             7
                 Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                         size-30 DFT, depth-ﬁrst:                                                         is illustrated by an example in Fig. 7 and discussed further
                             loop 3                                                                      below.
                              size-5 direct codelet, vector size 2                                         Depth-ﬁrst traversal has theoretical advantages for cache
                             size-2 twiddle codelet, vector size 5                                       utilization: eventually, the sub-DFT will ﬁt into cache and
                            size-3 twiddle codelet, vector size 10                                        (ideally) require no further cache misses [2], [3], [19], [35],
                                                                                                          regardless of the size of the cache. (Although we were initially
                         size-30 DFT, breadth-ﬁrst:                                                       motivated, in part, by these results, the point of FFTW’s
                              loop 3                                                                    self-optimization is that we need not rely on this or any
                            
                            
                                      size-5 direct codelet, vector size 2                              similar prediction.) Technically, the asymptotically optimal
                             loop3                                                                       “cache-oblivious” recursive algorithm would use a radix of
                                                                                                             √
                                       size-2 twiddle codelet, vector size 5                             Θ( n)for a transform of size n, analogous to the “four-step”
                            size-3 twiddle codelet, vector size 10                                        algorithm [18], [38], but we have found that a bounded radix
                                                                                                          generally works better in practice, except for at most a single
                                                                                                                             √
                 Fig. 7.  Twopossible decompositions for a size-30 DFT, both for the arbitrary            step of radix-       n.
                 choice of DIT radices 3 then 2 then 5, and prime-size codelets. Items grouped               A depth-ﬁrst style is also used for the multi-dimensional
                 by a “{” result from the plan for a single sub-problem. In the depth-ﬁrst case,          plans of Section IV-C.4, where in this case the planner can
                 the vector rank was reduced to 0 as per Section IV-C.5 before decomposing                (and often does) choose the optimal cache-oblivious algorithm:
                 sub-problems, and vice-versa in the breadth-ﬁrst case.
                                                                                                          it breaks the transform into sub-problems of roughly equal
                                                                                                          rank. In contrast, an iterative, “breadth-ﬁrst” approach might
                 algorithm from [28] to compute one-dimensional DFTs of                                   perform all of the 1d transforms for the ﬁrst dimension, then
                 prime size in O(nlogn) time (with Rader-DIT plans for the                                all of the 1d transforms for the second dimension, and so
                 twiddled DFTs of large prime factors). (A future release of                              on, which has extremely poor cache performance compared
                 FFTW also implements Bluestein’s “chirp-z” algorithm [27],                               to grouping the dimensions into smaller multi-dimensional
                 [29].)                                                                                   transforms.
                    Real/imaginary plans execute a vector loop of two spe-                                   Because its sub-problems contain a vector loop that can
                 cialized real-input DFT plans (Section VII) on the real and                              be executed in a variety of orders, however, FFTW3 can
                 imaginary parts of the input, and then combine the results. This                         also express breadth-ﬁrst traversal. For example, if the rule
                 can be more efﬁcient if, for example, the real and imaginary                             of Section IV-C.4 were applied repeatedly to ﬁrst reduce the
                 parts are stored by the user in separate arrays (a generalization                        rank to 1, and then the vector ranks were reduced by applying
                 of the storage format that we omitted above).                                            the loop rule of Section IV-C.5 to the sub-problems, the plan
                    Parallel (multi-threaded) plans are achieved by a special                             would implement the breadth-ﬁrst multi-dimensional approach
                 variant of Section IV-C.5 that executes the vector loop in                               described above. Similarly, a 1d algorithm resembling the
                 parallel, along with a couple of extra plans to execute twiddle-                         traditional breadth-ﬁrst Cooley-Tukey would result from ap-
                 codelet loops in parallel. Although shared- and distributed-                             plying Section IV-C.3.b to completely factorize the problem
                 memory parallel versions of FFTW exist, we do not further                                size before applying the loop rule to reduce the vector ranks.
                 describe them in this paper.                                                             As described in Section V-B, however, by default we limit the
                                                                                                          types of breadth-ﬁrst-style plans considered in order to reduce
                 D. Discussion                                                                            planner time, since they appear to be suboptimal in practice
                                                                                                          as well as in theory.
                    Although it may not be immediately apparent, the combi-                                  Even with the breadth-ﬁrst execution style described above,
                 nation of the recursive rules in Section IV-C can produce a                              though, there is still an important difference between FFTW
                 number of useful algorithms. To illustrate these compositions,                           and traditional iterative FFTs: FFTW has no separate bit-
                 we discuss in particular three issues: depth- vs. breadth-ﬁrst,                          reversal stage. For out-of-place transforms, the re-ordering
                 loop reordering, and in-place transforms. More possibilities                             occurs implicitly in the strides of Section IV-C.3.b (which
                 and explicit examples of plans that are “discovered” in practice                         are transferred to the strides of the nested vector loops in
                 are discussed in Section V-C.                                                            a recursive breadth-ﬁrst plan); in any case, the “leaves” of
                    1) Depth-ﬁrst and breadth-ﬁrst FFTs: If one views an                                  the recursion (direct plans) transform the input directly to
                 FFT algorithm as a directed acyclic graph (dag) of data                                  its correct location in the output, while the twiddle codelets
                 dependencies (e.g. the typical “butterﬂy” diagram), most tra-                            operate in-place. This is an automatic beneﬁt of a recursive
                 ditional Cooley-Tukey FFT implementations traverse the tree                              implementation. (Another possibility would be a Stockham-
                 in “breadth-ﬁrst” fashion (Section II). In contrast, FFTW1 and                           style transform, from Section II, but this is not currently
                 FFTW2 traversed the dag in “depth-ﬁrst” order, due to their                              implemented in FFTW.)
                 explicitly recursive implementation. That is, they completely                               2) Vector recursion: Another example of the effect of loop
                 solved a single one-dimensional sub-DFT before moving on to                              reordering is a style of plan that we sometimes call vector
                 the next. FFTW3 also evaluates its plans in an explicitly recur-                         recursion (unrelated to “vector-radix” FFTs [16]). The basic
                 sive fashion, but, because its problems now include arbitrary                            idea is that, if you have a loop (vector-rank 1) of transforms,
                 vector ranks, it is able to express both depth- and breadth-                             where the vector stride is smaller than the transform size,
                 ﬁrst traversal of the dag (as well as intermediate styles). This                         it is advantageous to push the loop towards the leaves of
                                                                                                       8
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             the transform decomposition, while otherwise maintaining            can be employed. We emphasize that all of these algorithms
             recursive depth-ﬁrst ordering, rather than looping “outside”        are “discovered” automatically by the planner simply by
             the transform; i.e., apply the usual FFT to “vectors” rather        composing the rules of Section IV-C.
             than numbers. Limited forms of this idea have appeared for
             computing multiple FFTs on vector processors (where the loop        E. The FFTW planner
             in question maps directly to a hardware vector) [22], and in
             another restricted form as an undocumented feature of FFTW2.          In this section, we discuss the implementation and operation
             Such plans are among the many possible compositions of              of the FFTW planner.
             our recursive rules: one or more steps of the Cooley-Tukey            The FFTW planner is a modular piece of code independent
             decomposition (Section IV-C.3.b) can execute before the low-        of the speciﬁc problems and plans supported by the system. In
             stride vector loop is extracted (Section IV-C.5), but with other    this way, we can reuse the same planner for complex DFTs,
             loops still extracted before decomposition. The low-stride          real-data DFTs, and other transforms. The separation between
             vector loop need not, however, be pushed all the way to the         planner and plans is achieved by means of ancillary entities
             leaves of the decomposition, and it is not unusual for the loop     called solvers, which can be viewed as the portion of the
             to be executed at some intermediate level instead.                  planner that is problem- and plan-speciﬁc. The choreography
                For example, low-stride vector loops appear in the decom-        of the planner, solvers, and plans is arranged as follows.
             position of a typical multi-dimensional transform (Section IV-        The planner is ﬁrst initialized with a list of solvers. Given a
             C.4): along some dimensions, the transforms are contiguous          problem, the planner calls each solver in sequence, requesting
             (stride 1) but the vector loop is not, while along other dimen-     a plan for the problem. Each solver returns either a pointer
             sions the vector stride is 1 but the transforms are discontigu-     to a plan or a null pointer, which indicates that the solver
             ous, and in this latter case vector recursion is often preferred.   cannot create a plan for that problem. The planner selects the
             As another example, Cooley-Tukey itself produces a unit             fastest plan (by performing explicit time measurements) and
             input-stride vector loop at the top-level DIT decomposition,        returns it to the user. The user calls the plan to compute Fourier
             but with a large output stride; this difference in strides makes    transforms as desired.
             it nonobvious whether vector recursion is advantageous for            A solver can generate a certain class of plans. (Approxi-
             the sub-problem, but for large transforms we often observe          mately, one solver exists for each item in the classiﬁcation
             the planner to choose this possibility.                             of plans from Section IV-C.) When invoked by the planner, a
                3) In-place plans: In-place 1d transforms can be obtained        solver creates the plan for the given problem (if possible) and
             by two routes from the possibilities described in Section IV-C:     it initializes any auxiliary data required by the plan (such as
             via combination of DIT and DIF plans (Section IV-C.3.b) with        twiddle factors). In many cases, creating a plan requires that a
             transposes (Section IV-C.2), or via buffering (Section IV-C.7).     plan for one or more sub-problems be available. For example,
                The transpose-based strategy for an in-place transform of        Cooley-Tukey plans require a plan for a smaller DFT. In these
             size pqm is outlined as follows. First, the transform is decom-     cases, the solver obtains the sub-plans by invoking the planner
             posed via a radix-p DIT plan into a vector of p transforms          recursively.
             of size qm, then these are decomposed in turn by a radix-q            By construction, the FFTW planner uses dynamic program-
             DIF plan into a vector (rank 2) of p×q transforms of size m.        ming [12, chapter 16]: it optimizes each sub-problem locally,
             These transforms of size m have input and output at different       independently of the larger context. Dynamic programming
             places/strides in the original array, and so cannot be solved       is not guaranteed to ﬁnd the fastest plan, because the perfor-
             independently. Instead, an indirect plan (Section IV-C.6) is        mance of plans is context-dependent on real machines: this
             used to express the sub-problem as pq in-place transforms of        is another engineering tradeoff that we make for the sake of
             size m, followed or preceded by an m×p×q rank-0 transform.          planning speed. The representation of problems discussed in
             The latter sub-problem is easily seen to be m in-place p × q        Section IV-A is well suited to dynamic programming, because
             transposes (ideally square, i.e. p = q). Related strategies for     a problem encodes all the information required to solve it—no
             in-place transforms based on small transposes were described        reference to a larger context is necessary.
             in [23]–[26]; alternating DIT/DIF, without concern for in-place       Like most dynamic-programming algorithms, the planner
             operation, was also considered in [39], [40].                       potentially evaluates the same sub-problem multiple times. To
                As an optimization, we include DIF-transpose codelets that       avoid this duplication of work, the FFTW planner uses the
             combinetheradix-q DIF twiddle codelet (in a loop of length p)       standard solution of memoization: it keeps a table of plans for
             with the p × q transpose, for p = q ∈ {2,3,4,5,6,8}. (DIF-          already computed problems and it returns the solution from
             transpose is to DIF + transpose roughly as [24] is to [25].)        the table whenever possible. Memoization is accomplished
             Another common special case is where m = 1, in which a              by FFTW in a slightly unorthodox fashion, however. The
             size-q direct plan (Section IV-C.3.a), not a DIF codelet, is        memoization table, which maps problems into plans, contains
             required (the twiddle factors are unity), and the transposes are    neither problems nor plans, because these data structures can
             performed at the leaves of the plan.                                be large and we wish to conserve memory. Instead, the planner
                Since the size-m transforms must be performed in-place,          stores a 128-bit hash of the problem and a pointer to the solver
             if they are too large for a direct plan the transpose scheme        that generated the plan in the ﬁrst place. When the hash of a
             can be used recursively or a buffered plan can be used for          problem matches a hash key in the table, the planner invokes
             this sub-problem. That is, a mixture of these two strategies        the corresponding solver to obtain a plan. For hashing, we
                                                                              9
                Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                    fftw_plan plan;                                                                          4000                                                fftw, patient mode
                    fftw_complex in[n], out[n];                                                                                                                  fftw, impatient mode
                                                                                                             3500                                                fftw, estimate mode
                    /* plan a 1d forward DFT: */                                                             3000
                    plan = fftw_plan_dft_1d(n, in, out,
                                        FFTW_FORWARD, FFTW_PATIENT);                                         2500
                    Initialize in[] with some data...                                                        2000
                    fftw_execute(plan); // compute DFT                                                      speed (mflops)1500
                    Write some new data to in[] ...                                                          1000
                    fftw_execute(plan); // reuse plan                                                         500
                Fig. 8.   Example of FFTW’s use. The user must ﬁrst create a plan, which
                                                                                                                  2  4   8   16 32  64  128256 512 1024204840968192163843276865536131072262144
                can be then used for many transforms of the same size.                                           0
                use the cryptographically strong MD5 algorithm [41]. In the                          Fig. 9.    Effect of planner tradeoffs: comparison of patient, impatient, and
                extremely unlikely event of a hash collision, the planner would                      estimate modes in FFTW for double-precision 1d complex DFTs, power-of-
                still return a valid plan, because the solver returned by the table                  two sizes, on a 2GHz PowerPC 970 (G5). Compiler and ﬂags as in Fig. 4.
                lookup would either construct a valid plan or fail, and in the
                latter case the planner would continue the search as usual.                          B. Planning-time tradeoffs
                                        V. FFTW3INPRACTICE                                               Depending upon the application, it is not always worthwhile
                                                                                                     to wait for the planner to produce an optimal plan, even
                    In this section, we discuss some of our practical experi-                        under the dynamic-programming approximation discussed in
                ences with FFTW, from user-interface design, to planning                             Section IV-E, so FFTW provides several other possibilities.
                time/optimality tradeoffs, to interesting planner choices that                       One option is to load from a ﬁle the memoization hash table
                are experimentally observed.                                                         of Section IV-E), so that the planner need not recompute it.
                                                                                                     For problems that have not been planned in advance, various
                                                                                                     time-saving approximations can be made in the planner itself.
                A. User interface                                                                        In patient mode (used for the benchmarks in Section III),
                    Theinternal complexity of FFTW is not exposed to the user,                       the planner tries essentially all combinations of the possible
                who only needs to specify her problem for the planner and                            plans, with dynamic programming.
                then, once a plan is generated, use it to compute any number                             Alternatively, the planner can operate in an impatient mode
                of transforms of that size. (See Fig. 8.)                                            that reduces the space of plans by eliminating some pos-
                                                                                                     sibilities that appear to inordinately increase planner time
                    Although the user can optionally specify a problem by                            relative to their observed beneﬁts. Most signiﬁcantly, only
                its full representation as deﬁned in Section IV, this level of                       one way to decompose multi-dimensional N or V (Sections
                generality is often only necessary internally to FFTW. Instead,                      IV-C.4 and IV-C.5) is considered, and vector recursion is
                we provide a set of interfaces that are totally ordered by                           disabled (Section IV-D.2). Furthermore, the planner makes
                increasing generality, from a single (vector-rank 0) 1d unit-                        an approximation: the time to execute a vector loop of ℓ
                stride complex transform (as in Fig. 8), to multi-dimensional                        transforms is taken to be ℓ multiplied by the time for one
                transforms, to vector-rank 1 transforms, all the way up to the                       transform. Altogether, impatient mode often requires a factor
                general case. (An alternate proposal has been to modify an                           of 10 less time to produce a plan than the full planner.
                FFT/data “descriptor” with a set of subroutines, one per degree                          Finally, there is an estimate mode that performs no mea-
                of freedom, before planning [42].)                                                   surements whatsoever, but instead minimizes a heuristic cost
                    With the more advanced interfaces, which allow the user to                       function: the number of ﬂoating-point operations plus the
                specify vector loops and even I/O tensors, it is possible for the                    number of “extraneous” loads/stores (such as for copying to
                user to deﬁne nonsensical problems with DFTs of overlapping                          buffers). This can reduce the planner time by several orders
                outputs (Section IV-B). The behavior of FFTW is undeﬁned                             of magnitude, but with a signiﬁcant penalty observed in plan
                in such a case; this is rarely a problem, in practice, because                       efﬁciency (see below). This penalty reinforces a conclusion
                only more sophisticated users exploit these interfaces, and such                     of [3]: there is no longer any clear connection between
                users are naturally capable of describing sensible transforms                        operation counts and FFT speed, thanks to the complexity
                to perform.                                                                          of modern computers. (Because this connection was stronger
                    As one additional feature, the user may control tradeoffs                        in the past, however, past work has often used the count of
                in planning speed versus plan optimality by a ﬂag argument                           arithmetic operations as a metric for comparing O(nlogn)
                (e.g. FFTW PATIENT in Fig. 8). These tradeoffs are discussed                         FFT algorithms, and great effort has been expended to prove
                below.                                                                               and achieve arithmetic lower bounds [16].)
                                                                                                  10
                   Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                            4000                                                   G5                                  input and contiguous output; instead, an indirect plan is used
                                                                                   G5, plan from Pentium IV
                            3500                                                   Pentium IV                          to ﬁrst copy input to output, and then the codelet is executed
                                                                                   Pentium IV, plan from G5            in-place on contiguous values. The same size on the G5 yields
                            3000                                                                                       the plan: radix-4 DIT; followed by an indirect plan to copy
                            2500                                                                                       and work with a contiguous size-16384 in-place sub-plan on
                                                                                                                       the output. The sub-plan is: radix-32 DIT; vector-recursion of
                            2000                                                                                       the size-32 loop through radix-16 DIT; followed by another
                           speed (mflops)1500                                                                          indirect plan to perform 16 transposes of size 32×32, and then
                                                                                                                       512 size-32 direct codelets. The sub-plan’s usage of indirect
                            1000                                                                                       plans fulﬁlls their original purpose of in-place transforms
                                                                                                                       (Section IV-D.3); indirect plans for large out-of-place DFTs
                              500                                                                                      were initially a surprise (and often boosted speed by 20% or
                                                                                                                       more).
                                  2   4   8   16  32  64  128 256 512 1024204840968192163843276865536131072262144524288
                                0                                                                                          Another surprise was that, whenever possible, the transposes
                                                                                                                       for in-place DFTs are almost always used at the leaves with a
                                                                                                                       direct codelet, as for the size-16384 sub-plan of the G5 plan
                   Fig. 10.    Effects of tuning FFTW on one machine and running it on another.                        above; our preconception was that the transpose would be
                   The graph shows the performance of one-dimensional DFTs on two machines:                            grouped at an intermediate point with an explicit DIF step
                   a 2GHz PowerPC 970 (G5), and a 2.8GHz Pentium IV. For each machine,                                 (as for the DIF-transpose codelets). As another example, an
                   we report both the speed of FFTW tuned to that machine and the speed tuned                          in-place size-65536 plan on the Pentium IV uses: radix-4 DIT,
                   to the other machine.
                                                                                                                       radix-4 DIF-transpose, two radix-16 DIT steps, and ﬁnally an
                       The relative performance of the 1d complex-data plans                                           indirect plan that ﬁrst performs 16 × 16 transposes and then
                                                                                                                       uses a size-16 direct codelet.
                   created in patient, impatient, and estimate modes are shown                                             Regarding vector recursion, we had ﬁrst guessed that a low-
                   in Fig. 9 for the PowerPC G5 from Section III. In this case,                                        stride vector loop would always be pushed all the way to the
                   estimate mode imposes median and maximum speed penalties                                            leaves of the recursion, and an early implementation enforced
                   of 20%and72%,respectively,whileimpatient modeimposesa                                               this constraint. It turns out that this is often not the case,
                   maximumpenaltyof11%.Inothercases, however, the penalty                                              however, and the loop is only pushed one or two levels down,
                   from impatient mode can be larger; for example, it has a 47%                                        as in the G5 plan above. Indirect plans add another level of
                   penalty for a 1024 × 1024 2d complex-data transform on the                                          complexity, because often the copy (rank-0) sub-plan executes
                   same machine, since vector recursion proves important there                                         its loops in a different order than the transform sub-plan. This
                   for the discontiguous (row) dimension of the transform.                                             happens, for example, when the (discontiguous) columns of a
                       It is critical to create a new plan for each architecture—                                      1024×1024 array are transformed in-place on the G5, whose
                   there is a substantial performance penalty if plans from one                                        resulting plan uses contiguous buffer storing 8 columns at a
                   machine are re-used on another machine. To illustrate this                                          time, a radix-16 DIT step, an indirect plan that ﬁrst copies
                   point, Fig. 10 displays the effects of using the optimal plan                                       to the buffer than transforms in-place with a size-64 direct
                   from one machine on another machine. In particular, it plots                                        codelet, and then copies back. Because the vector loop over
                   the speed of FFTW for one-dimensional complex transforms                                            the columns is stride-1, it is best to push that loop to the leaves
                   on the G5 and the Pentium IV. In addition to the optimal                                            of the copy operations; on the other hand, the direct codelet
                   plan chosen by the planner on the same machine, we plot the                                         operates on contiguous buffers so it prefers to have the size-16
                   speed on the G5 using the optimal plan from the Pentium IV                                          vector loop innermost. (A similar effect, with different radices,
                   and vice versa. In both cases, using the wrong machine’s plan                                       occurs in the Pentium IV plan for this problem.)
                   imposes a speed penalty of 20% or more for at least 1/3 of                                              While “explanations” can usually be fabricated in hindsight,
                   the cases benchmarked, up to a 40% or 34% penalty for the                                           we do not really understand the planner’s choices because we
                   G5 or Pentium IV, respectively.                                                                     cannot predict what plans will be produced. Indeed, this is the
                                                                                                                       whole point of implementing a planner.
                   C. Planner choices
                       It is interesting to consider examples of the sometimes                                                        VI. THE GENFFT CODELET GENERATOR
                   unexpected plans that are actually chosen in practice by the                                            The base cases of FFTW’s recursive plans are its “codelets,”
                   planner.                                                                                            and these form a critical component of FFTW’s performance.
                       For example, consider an out-of-place DFT of size 65536 =                                       They consist of long blocks of highly optimized, straight-
                     16
                   2 .OnourPentiumIV,theplanhastheoverall structure: DIT                                               line code, implementing many special cases of the DFT that
                   of radices 32 then 8 then 16, followed by a direct codelet of                                       give the planner a large space of plans in which to optimize.
                   size 16. However, the ﬁrst step actually uses buffered DIT, and                                     Not only was it impractical to write numerous codelets by
                   its size-32 vector loop is pushed down to the direct codelet                                        hand, but we also needed to rewrite them many times in order
                   “leaves” by vector recursion (Section IV-D.2). Moreover, the                                        to explore different algorithms and optimizations. Thus, we
                   size-16 direct codelet would normally have discontiguous                                            designed a special-purpose “FFT compiler” called genfft
                                                                                                                   11
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             that produces the codelets automatically from an abstract de-     (here, the registers are viewed as a form of cache). As a prac-
             scription. genfft is summarized in this section and described     tical matter, one consequence of this scheduler is that FFTW’s
             in more detail by [2].                                            machine-independent codelets are no slower than machine-
               As discussed in Section IV, FFTW uses many kinds                speciﬁc codelets generated by SPIRAL [43, Figure 3].
             of codelets: “direct” codelets (Section IV-C.3.a), “twiddle”         In the stock genfft implementation, the schedule is ﬁnally
             codelets in the DIT and DIF variants (Section IV-C.3.b), and      unparsed to C. A variation from [44] implements the rest of
             the more exotic “DIF-transpose codelets” (Section IV-D.3).        a compiler backend and outputs assembly code.
             (Additional kinds of codelets will be presented in Sections
             VII and VIII.)                                                                   VII. REAL-DATA TRANSFORMS
               In principle, all codelets implement some combination of the
             Cooley-Tukey algorithm from Eq. (2) and/or some other DFT            In this section, we brieﬂy outline how FFTW computes
             algorithm expressed by a similarly compact formula. However,      DFTsofrealdata(arealDFT),andwegiveanewO(nlogn)-
             a high performance implementation of the DFT must address         time algorithm to compute the one-dimensional DFT of real
             manymoreconcernsthan Eq. (2) alone suggests. For example,         arrays of prime length n.
             Eq. (2) contains multiplications by 1 that are more efﬁcient to      As is well known, the DFT Y of a real array of length n
             omit. Eq. (2) entails a run-time factorization of n, which can    has the Hermitian symmetry
             be precomputed if n is known in advance. Eq. (2) operates on
             complex numbers, but breaking the complex-number abstrac-                              Y[n−k]=Y∗[k] ,                         (3)
             tion into real and imaginary components turns out to expose       where Y∗[k] denotes the complex conjugate of Y[k]. (A
             certain non-obvious optimizations. Additionally, to exploit the   similar symmetry holds for multi-dimensional arrays as well.)
             long pipelines in current processors, the recursion implicit      By exploiting this symmetry, one can save roughly a factor of
             in Eq. (2) should be unrolled and re-ordered to a signiﬁcant      two in storage and, by eliminating redundant operations within
             degree. Many further optimizations are possible if the complex    the FFT, roughly a factor of two in time as well [45].
             input is known in advance to be purely real (or imaginary).          The implementation of real-data DFTs in FFTW parallels
             Ourdesign goal for genfft was to keep the expression of the       that of complex DFTs discussed in Section IV. For direct
             DFTalgorithm independent of such concerns. This separation        plans, we use optimized codelets generated by genfft, which
             allowed us to experiment with various DFT algorithms and          automatically derives specialized real-data algorithms from the
             implementation strategies independently and without (much)        corresponding complex algorithm (Section VI). For Cooley-
             tedious rewriting.                                                Tukey plans, we use a mixed-radix generalization of [45],
               genfft is structured as a compiler whose input consists         which works by eliminating the redundant computations in
             of the kind and size of the desired codelet, and whose            a standard Cooley-Tukey algorithm applied to real data [22],
             output is C code. genfft operates in four phases: creation,       [46], [47].
             simpliﬁcation, scheduling, and unparsing.                            When the transform length is a prime number, FFTW
               In the creation phase, genfft produces a representation of      uses an adaptation of Rader’s algorithm [28] that reduces the
             the codelet in the form of a directed acyclic graph (dag). The    storage and time requirements roughly by a factor of two with
             dag is produced according to well-known DFT algorithms:           respect to the complex case. The remainder of this section
             Cooley-Tukey (Eq. (2)), prime-factor [27, page 619], split-       describes this algorithm, which to our knowledge has not been
             radix [16], and Rader [28]. Each algorithm is expressed in        published before.
             a straightforward math-like notation, using complex numbers,         The algorithm ﬁrst reduces the real DFT to the discrete
             with no attempt at optimization.                                  Hartley transform (DHT) by means of the well-known reduc-
               In the simpliﬁcation phase, genfft applies local rewriting      tion of [48], and then it executes a DHT variant of Rader’s
             rules to each node of the dag in order to simplify it. This       algorithm. The DHT was originally proposed by [48] as a
             phase performs algebraic transformations (such as eliminating     faster alternative to the real DFT, but [45] argued that a
             multiplications by 1), common-subexpression elimination, and      well-implemented real DFT is always more efﬁcient than an
             a few DFT-speciﬁc transformations. These simpliﬁcations are       algorithm that reduces the DFT to the DHT. For prime sizes,
             sufﬁciently powerful to derive DFT algorithms specialized for     however, no real-data variant of Rader’s algorithm appears to
             real and/or symmetric data automatically from the complex         be known, and for this case we propose that a DHT is useful.
             algorithms. We take advantage of this property to implement          To compute DHTs of prime size, recall the deﬁnition of
             real-data DFTs (Section VII), to exploit machine-speciﬁc          DHT:
             “SIMD” instructions (Section IX), and to generate codelets                               n−1                 
             for the discrete cosine (DCT) and sine (DST) transforms                          Y[k] = XX[j]cas 2πjk            ,            (4)
             (Section VIII).                                                                                            n
               In the scheduling phase, genfft produces a topological                                 j=0
             sort of the dag (a “schedule”). The goal of this phase is to ﬁnd  where cas(x) = cos(x) + sin(x). If n is prime, then there
             a schedule such that a C compiler can subsequently perform        exists a generator g of the multiplicative group modulo n: for
             a good register allocation. The scheduling algorithm used by      all j ∈ {1,2,...,n−1}, there exists a unique integer p ∈
             genfftoffers certain theoretical guarantees because it has its    {0,1,...,n−2} such that that j = gp (mod n). Similarly,
             foundations in the theory of cache-oblivious algorithms [35]      one can write k = g−q (mod n) if k 6= 0. For nonzero k, we
                                                                            12
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             can thus rewrite Eq. (4) as follows.                                This deﬁnition can be rewritten in this way:
                                                                                                  n−1
                                       n−2                 −(q−p)                               X 2πi(2j+1)(2k+1)/(8n)
                    Y[g−q] = X[0]+XX[gp]cas 2πg n                     ,   (5)             Y[k] = j=0X[j]e
                                       p=0                                                           n−1
                                                                                                     X −2πi(2j+1)(2k+1)/(8n)
             where the summation is a cyclic convolution of a permutation                         +      X[j]e                         .
             of the input array with a ﬁxed real sequence. This cyclic                               j=0
             convolution can be computed by means of two real DFTs,              In other words, the outputs of a DCT-IV of length n are just
             in which case the algorithm takes O(nlogn) time, or by              a subset of the outputs of a DFT of length 8n whose inputs
             any other method [49]. (FFTW computes convolutions via              have been made suitably symmetric and interleaved with zeros.
             DFTs.) The output element Y[0], which is the sum of all             Similar reductions apply to all other kinds of trigonometric
             input elements, cannot be computed via Eq. (5) and must be          transforms.
             calculated separately.                                                Consequently, to generate code for a trigonometric trans-
                An adaptation of Bluestein’s prime-size algorithm to the         form, genfft ﬁrst reduces it to a DFT and then it gen-
             DHTalso exists [50], but the known method does not exhibit          erates a dag for the DFT, imposing the necessary symme-
             asymptotic savings over the complex-data algorithm.                 tries, setting the appropriate inputs to 0, and pruning the
                                                                                 dag to the appropriate subset of the outputs. The symbolic
                                                                                 simplications performed by genfft are powerful enough
                         VIII. TRIGONOMETRIC TRANSFORMS                          to eliminate all redundant computations, thus producing a
                                                                                 specialized DCT/DST algorithm. This strategy requires no
                Along with the DHT, there exist a number of other useful         prior knowledge of trigonometric-transform algorithms and is
             transforms of real inputs to real outputs: namely, DFTs of real-    exceptionally easy to implement.
             symmetric (or anti-symmetric) data, otherwise known as the            Historically, the generator of FFTW2 (1999) implemented
             discrete cosine and sine transforms (DCTs and DSTs), types          experimental, undocumented support for the DCT/DST I and
             I–VIII [27], [51]–[53]. We collectively refer to these trans-       II in this way. Vuduc and Demmel independently rediscovered
             forms as trigonometric transforms. Types I–IV are equivalent        that genfft could derive trigonometric transforms from
             to (∼ double-length) DFTs of even size with the different           the complex DFT while implementing ideas similar to those
             possible half-sample shifts in the input and/or output. Types       described in this section [54].
             V–VIII [52] are similar, except that their “logical” DFTs are
             of odd size; these four types seem to see little practical use, so  B. General trigonometric transforms
             we do not implement them. (In order to make the transforms
             unitary, additional factors of √2 multiplying some terms are          Type II and III trigonometric transforms of length n are
                                                                  √              computed using a trick from [22], [55] to re-express them
             required, beyond an overall normalizaton of 1/ n. Some              in terms of a size-n real-input DFT. Types I and IV are more
             authors include these factors, breaking the direct equivalence      difﬁcult, because we have observed that convenient algorithms
             with the DFT.)                                                      to embed them in an equal-length real-input DFT have poor
                Each type of symmetric DFT has two kinds of plans in             numerical properties: the type-I algorithm from [22], [31] and
             FFTW: direct plans (using specialized codelets generated by         the type-IV algorithm from [56] both have L         (root mean
                                                                                                                                   2
             genfft), and general-length plans that re-express a rank-                                                            √
             1 transform of length n in terms of a real-input DFT plus           square) relative errors that seem to grow as O(    n). We have
             pre/post-processing. (Here, n denotes the number of non-            not performed a detailed error analysis, but we believe the
             redundant real inputs.)                                             problem is due to the fact that both of these methods multiply
                In the rest of this section, we show how genfft gen-             the data by a bare cosine (as opposed to a unit-magnitude
             erates the codelets required by trigonometric direct plans          twiddle factor), with a resulting loss of relative precision near
             (Section VIII-A), and we discuss how FFTW implements                the cosine zero. Instead, to compute a type-IV trigonometric
             trigonometric transforms in the general case (Section VIII-B).      transform, we use one of two algorithms: for even n, we use
                                                                                 the method from [57] to express it as pair of type-III problems
                                                                                 of size n/2, which are solved as above; for odd n, we use a
                                                                                 method from [58] to re-express the type-IV problem as a size-
             A. Automatic generation of trigonometric-transform codelets         nreal-input DFT (with a complicated re-indexing that requires
                genfft does not employ any special trigonometric-                no twiddle factors at all). For the type-I DCT/DST, however,
             transform algorithm. Instead, it takes the position that all these  we could not ﬁnd any accurate algorithm to re-express the
             transforms are just DFTs in disguise. For example, a DCT-IV         transform in terms of an equal-length real-input DFT, and thus
             can be reduced to a DFT as follows. Consider the deﬁnition          weresort to the “slow” method of embedding it in a real-input
                                                                                 DFToflength 2n. All of our methods are observed to achieve
             of the DCT-IV:                                                                   √
                                                                                 the same O( logn) L error as the Cooley-Tukey FFT [59].
                                                                   !                                     2
                               n−1                     1       1                 One can also compute symmetric DFTs by directly special-
                     Y[k] = 2XX[j]cos π j+ 2               k+2                   izing the Cooley-Tukey algorithm, removing redundant opera-
                               j=0                      n                        tions as we did for real inputs, to decompose the transform into
                                                                              13
             Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
             smaller symmetric transforms [53], [56], [57]. Such a recursive   a non-unit stride. Second, because the algorithm ﬁnds two-
             strategy, however, would require eight new sets of codelets to    way parallelism in the real and imaginary parts of a single
             handle the different types of DCT and DST, and we judged          DFT (as opposed to performing two DFTs in parallel), we
             the cost in code size to be unacceptable.                         can completely parallelize DFTs of any size, not just even
                                                                               sizes or powers of 2.
                            IX. HOW FFTW3 USES SIMD                               This SIMDalgorithmis implemented in the codelets: FFTW
               This section discusses how FFTW exploits special SIMD           contains SIMD versions of both direct and twiddle codelets (as
             (Single-Instruction Multiple Data) instructions, which perform    deﬁned in Section IV-C.3). It may seem strange to implement
             the same operation in parallel on a data vector. These instruc-   the complex DFT in terms of the real DFT, which re-
             tions are implemented by many recent microprocessors, such        quires much more involved algorithms. Our codelet generator
             as the Intel Pentium III (SSE) and IV (SSE2), the AMD K6          genfft, however, derives real codelets automatically from
             and successors (3DNow!), and some PowerPC models (Al-             complex algorithms, so this is not a problem for us.
             tiVec). The design of FFTW3 allowed us to efﬁciently support         On machines that support vectors of length 4, we view
             such instructions simply by plugging in new types of codelets,    SIMD data as vectors of two complex numbers, and each
             without disturbing the overall structure.                         codelet executes two iterations of its loop in parallel. (A
               SIMD instructions are superﬁcially similar to “vector pro-      similar strategy of codelets that operate on 2-vectors was
             cessors”, which are designed to perform the same operation        argued in [11] to have beneﬁts even without SIMD.) The
             in parallel on an all elements of a data array (a “vector”). The  source of this 2-way parallelism is the codelet loop, which
             performance of “traditional” vector processors was best for       can arise from the Cooley-Tukey decomposition of a single
             long vectors that are stored in contiguous memory locations,      1d DFT, the decomposition of a multi-dimensional DFT, or
             and special algorithms were developed to implement the DFT        a user-speciﬁed vector loop. Four-way SIMD instructions are
             efﬁciently on this kind of hardware [22], [26]. Unlike in         problematic, because the input or the output are not generally
             vector processors, however, the SIMD vector length is small       stride-1, and arbitrary-stride SIMD memory operations are
             and ﬁxed (usually 2 or 4). Because microprocessors depend         more expensive than stride-1 operations. Rather than relying
             on caches for performance, one cannot naively use SIMD            onspecial algorithms that preserve unit stride, however, FFTW
             instructions to simulate a long-vector algorithm: while on vec-   relies on the planner to ﬁnd plans that minimize the number
             tor machines long vectors generally yield better performance,     of arbitrary-stride memory accesses.
             the performance of a microprocessor drops as soon as the             Although compilers that perform some degree of automatic
             data vectors exceed the capacity of the cache. Consequently,      vectorization are common for SIMD architectures, these typ-
             SIMD instructions are better seen as a restricted form of         ically require simple loop-based code, and we are not aware
             instruction-level parallelism than as a degenerate ﬂavor of       of any that is effective at vectorizing FFTW, nor indeed of
             vector parallelism, and different DFT algorithms are required.    any automatically vectorized code that is competitive on these
               In FFTW, we experimented with two new schemes to im-            2-way and 4-way SIMD architectures.
             plement SIMD DFTs. The ﬁrst scheme, initially developed by                         X. CONCLUDING REMARKS
             S. Kral, involves a variant of genfft that automatically ex-
             tracts SIMD parallelism from a sequential DFT program [44].          For many years, research on FFT algorithms focused on
             The major problem with this compiler is that it is machine-       the question of ﬁnding the best single algorithm, or the
             speciﬁc: it outputs assembly code, exploiting the peculiarities   best strategy for implementing an algorithm such as Cooley-
             of the target instruction set.                                    Tukey. Unfortunately, because computer hardware is continu-
               The second scheme relies on an abstraction layer consisting     ally changing, the answer to this question has been continually
             of C macros in the style of [60], and it is therefore semi-       changing as well. Instead, we believe that a more stable answer
             portable (the C compiler must support SIMD extensions in          may be possible by changing the question: instead of asking
             order for this scheme to work). To understand this SIMD           what is the best algorithm, one should ask what is the smallest
             scheme, consider ﬁrst a machine with length-2 vectors, such       collection of simple algorithmic fragments whose composition
             as the Pentium IV using the SSE2 instruction set (which can       spans the optimal algorithm on as many computer architectures
             perform arithmetic on pairs of double-precision ﬂoating-point     as possible.
             numbers). We view a complex DFT as a pair of real DFTs:              FFTW is a step in that direction, but is not the ultimate
                      DFT(A+i·B)=DFT(A)+i·DFT(B) ,                       (6)   answer; several open problems remain. Besides the obvious
                                                                               point that many possible algorithmic choices remain to be
             where A and B are two real arrays. Our algorithm computes         explored, we do not believe our existing algorithmc fragments
             the two real DFTs in parallel using SIMD instructions, and        to be as simple or as general as they should. The key to almost
             then it combines the two outputs according to Eq. (6).            every FFT algorithm lies in two elements: strides (re-indexing)
               This SIMD algorithm has two important properties. First, if     and twiddle factors. We believe that our current formalism for
             the data is stored as an array of complex numbers, as opposed     problems expresses strides well, but we do not know how to
             to two separate real and imaginary arrays, the SIMD loads         express twiddle factors properly. Because of this limitation,
             and stores always operate on correctly-aligned contiguous         we are currently forced to distinguish between decimation-in-
             locations, even if the the complex numbers themselves have        time and decimation-in-frequency Cooley-Tukey, which causes
                                                                            14
                Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                redundant coding. Our ultimate goal (for version 2π) is                             [18] D. H. Bailey, “FFTs in external or hierarchical memory,” J. Supercom-
                to eliminate this redundancy so that we can express many                                  puting, vol. 4, no. 1, pp. 23–35, May 1990.
                possible re-arrangements of the twiddle factors.                                    [19] R. C. Singleton, “On computing the fast Fourier transform,”
                                                                                                          Comm. ACM, vol. 10, pp. 647–654, 1967.
                                                                                                    [20] A. H. Karp, “Bit reversal on uniprocessors,” SIAM Rev., vol. 38, no. 1,
                                          ACKNOWLEDGMENTS                                                 pp. 1–26, 1996.
                                                                                                    [21] T. G. Stockham, “High speed convolution and correlation,” Proc. AFIPS
                   Weareindebted to F. Franchetti and S. Kral for their efforts                           Spring Joint Computer Conference, vol. 28, pp. 229–233, 1966.
                in developing experimental SIMD versions of FFTW. Thanks                            [22] P. N. Swarztrauber, “Vectorizing the FFTs,” in Parallel Computations,
                                                                                                          G. Rodrigue, Ed.     New York: Academic Press, 1982, pp. 51–83.
                to Greg Allen and the University of Texas for providing access                      [23] H. W. Johnson and C. S. Burrus, “An in-place in-order radix-2 FFT,” in
                to a PowerPC 970. SGJ is grateful to J. D. Joannopoulos for                               Proc. IEEE Int’l Conf. Acoustics, Speech, and Signal Processing, 1984,
                his unfailing encouragement of this project. Finally, we are                              pp. 28A.2.1–4.
                                                                                                    [24] C. Temperton, “Self-sorting in-place fast Fourier transforms,” SIAM
                indebted to the anonymous reviewers for helpful suggestions                               J. Scientiﬁc and Statistical Computing, vol. 12, no. 4, pp. 808–823,
                that improved the quality of this paper.                                                  1991.
                                                                                                    [25] Z. Qian, C. Lu, M. An, and R. Tolimieri, “Self-sorting in-place FFT
                                                                                                          algorithm with minimum working space,” IEEE Trans. Acoustics, Speech
                                               REFERENCES                                                 and Signal Processing, vol. 42, no. 10, pp. 2835–2836, 1994.
                                                                                                    [26] M. Hegland, “A self-sorting in-place fast Fourier transform algorithm
                 [1] M.     Frigo   and    S.   G.   Johnson,    “The    FFTW web page,”                  suitable for vector and parallel processing,” Numerische Mathematik,
                      http://www.fftw.org/, 2004.                                                         vol. 68, no. 4, pp. 507–547, 1994.
                 [2] M. Frigo, “A fast Fourier transform compiler,” in Proc. ACM SIG-               [27] A. V. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal
                      PLAN’99 Conference on Programming Language Design and Imple-                        Processing, 2nd ed.    Upper Saddle River, NJ: Prentice-Hall, 1999.
                      mentation (PLDI), vol. 34, no. 5.    Atlanta, Georgia: ACM, May 1999,         [28] C. M. Rader, “Discrete Fourier transforms when the number of data
                      pp. 169–180.                                                                        samples is prime,” Proc. IEEE, vol. 56, pp. 1107–1108, June 1968.
                 [3] M. Frigo and S. G. Johnson, “FFTW: An adaptive software architecture           [29] L. I. Bluestein, “A linear ﬁltering approach to the computation of
                      for the FFT,” in Proc. IEEE Int’l Conf. Acoustics, Speech, and Signal               the discrete Fourier transform,” Northeast Electronics Research and
                      Processing, vol. 3, Seattle, WA, May 1998, pp. 1381–1384.                           Eng. Meeting Record, vol. 10, pp. 218–219, 1968.
                 [4] G. Jayasumana, “Searching for the best Cooley-Tukey FFT algorithms,”           [30] S.    Winograd, “On computing the discrete Fourier transform,”
                      in Proc. IEEE Int’l Conf. Acoustics, Speech, and Signal Processing,                 Math. Computation, vol. 32, no. 1, pp. 175–199, Jan. 1978.
                      vol. 4, 1987, pp. 2408–2411.                                                  [31] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling,
                 [5] H. Massalin, “Superoptimizer: A look at the smallest program,” in                    Numerical Recipes in C: The Art of Scientiﬁc Computing, 2nd ed. New
                      Proc. 2nd Int’l Conf. Architectural Support for Programming Languages               York, NY: Cambridge Univ. Press, 1992.
                      and Operating System (ASPLOS), 1987, pp. 122–127.                             [32] R. C. Singleton, “An algorithm for computing the mixed radix fast
                                             ´                                                            Fourier transform,” IEEE Trans. Audio and Electroacoustics, vol. AU-
                 [6] J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel, “Optimizing matrix                17, no. 2, pp. 93–103, June 1969.
                      multiply using PHiPAC: a portable, high-performance, ANSI C coding            [33] H. V. Sorensen, M. T. Heideman, and C. S. Burrus, “On computing the
                      methodology,” in Proc. Int’l Conf. Supercomputing, Vienna, Austria,                 split-radix FFT,” IEEE Trans. Acoustics, Speech and Signal Processing,
                      July 1997.                                                                          vol. 34, no. 1, pp. 152–156, Feb. 1986.
                 [7] R. Whaley and J. Dongarra, “Automatically tuned linear algebra soft-           [34] D. Takahashi, “A blocking algorithm for FFT on cache-based pro-
                      ware,” Computer Science Department, Univ. Tennessee, Knoxville, TN,                 cessors,” in Proc. 9th Int’l Conf. High-Performance Computing and
                      Tech. Rep. CS-97-366, 1997.                                                         Networking, ser. Lecture Notes in Computer Science, vol. 2110.        Am-
                 [8] S. K. S. Gupta, C. Huang, P. Sadayappan, and R. W. Johnson, “A                       sterdam, The Netherlands: Springer-Verlag, 2001, pp. 551–554.
                      framework for generating distributed-memory parallel programs for             [35] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran, “Cache-
                      block recursive algorithms,” J. Parallel and Distributed Computing,                 oblivious algorithms,” in Proc. 40th Ann. Symp. Foundations of Com-
                      vol. 34, no. 2, pp. 137–153, May 1996.                                              puter Science (FOCS ’99), New York, USA, Oct. 1999.
                           ¨
                 [9] M. Puschel, B. Singer, J. Xiong, J. M. F. Moura, J. Johnson, D. Padua,         [36] M. Dow, “Transposing a matrix on a vector computer,” Parallel Com-
                      M. M. Veloso, and R. W. Johnson, “SPIRAL: A generator for platform-                 puting, vol. 21, no. 12, pp. 1997–2005, 1995.
                      adapted libraries of signal processing algorithms,” Journal of High           [37] E. G. Cate and D. W. Twigg, “Algorithm 513: Analysis of in-situ
                      Performance Computing and Applications, vol. 18, no. 1, pp. 21–45,                  transposition,” ACM Trans. Math. Software (TOMS), vol. 3, no. 1, pp.
                      2004.                                                                               104–110, 1977.
                            ¨
                [10] M. Puschel, J. M. F. Moura, J. Johnson, D. Padua, M. Veloso, B. W.             [38] W. M. Gentleman and G. Sande, “Fast Fourier transforms—for fun and
                                                                 ˇ ´
                      Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, K. Chen,                   proﬁt,” Proc. AFIPS Fall Joint Computer Conference, vol. 29, pp. 563–
                      R. W. Johnson, and N. Rizzolo, “SPIRAL: Code generation for DSP                     578, 1966.
                      transforms,” Proceedings of the IEEE, vol. 93, no. 2, pp. 232–275, 2005,
                      special issue on “Program Generation, Optimization, and Adaptation”.          [39] K. Nakayama, “An improved fast Fourier transform algorithm using
                [11] K. S. Gatlin, “Portable high performance programming via architecture-               mixed frequency and time decimations,” IEEE Trans. Acoustics, Speech
                      cognizant divide-and-conquer algorithms,” Ph.D. dissertation, University            and Signal Processing, vol. 36, no. 2, pp. 290–292, 1988.
                      of California, San Diego, 2000.                                               [40] A. Saidi, “Decimation-in-time-frequency FFT algorithm,” in Proc. IEEE
                [12] T. H. Cormen, C. E. Leiserson, and R. L. Rivest, Introduction to                     Int’l Conf. Acoustics, Speech, and Signal Processing, vol. 3, 1994, pp.
                      Algorithms.    Cambridge, Massachusetts: The MIT Press, 1990.                       453–456.
                [13] B. Singer and M. Veloso, “Learning to construct fast signal processing         [41] R. Rivest, “The MD5 message-digest algorithm,” Network Working
                      implementations,” J. Machine Learning Research, vol. 3, pp. 887–919,                Group Request for Comments (RFC) 1321, Apr. 1992.
                      2002, special issue on the Eighteenth Int’l Conf. Machine Learning            [42] P. T. P. Tang, “A comprehensive DFT API for scientiﬁc computing,” in
                      (ICML 2001).                                                                        The Architecture of Scientiﬁc Software, ser. IFIP Conference Proceed-
                [14] J. W. Cooley and J. W. Tukey, “An algorithm for the machine compu-                   ings, R. F. Boisvert and P. T. P. Tang, Eds., vol. 188.   Ottawa, Canada:
                      tation of the complex Fourier series,” Math. Computation, vol. 19, pp.              Kluwer, Oct. 2001, pp. 235–256.
                      297–301, Apr. 1965.                                                           [43] J. Xiong, D. Padua, and J. Johnson, “SPL: A language and compiler
                [15] M. T. Heideman, D. H. Johnson, and C. S. Burrus, “Gauss and the                      for DSP algorithms,” in Proc. ACM SIGPLAN’01 Conf. Programming
                      history of the fast Fourier transform,” IEEE ASSP Magazine, vol. 1,                 Language Design and Implementation (PLDI), 2001, pp. 298–308.
                      no. 4, pp. 14–21, 1984.                                                       [44] F. Franchetti, S. Kral, J. Lorenz, and C. Ueberhuber, “Efﬁcient utilization
                [16] P. Duhamel and M. Vetterli, “Fast Fourier transforms: a tutorial review              of SIMD extensions,” Proceedings of the IEEE, vol. 93, no. 2, pp. 409–
                      and a state of the art,” Signal Processing, vol. 19, pp. 259–299, Apr.              425, 2005, special issue on “Program Generation, Optimization, and
                      1990.                                                                               Adaptation”.
                [17] C. van Loan, Computational Frameworks for the Fast Fourier Transform.          [45] H. V. Sorensen, D. L. Jones, M. T. Heideman, and C. S. Burrus,
                      Philadelphia: SIAM, 1992.                                                           “Real-valued fast Fourier transform algorithms,” IEEE Trans. Acoustics,
                                                                                                 15
               Published in Proc. IEEE, vol. 93, no. 2, pp. 216–231 (2005).
                    Speech, and Signal Processing, vol. ASSP-35, no. 6, pp. 849–863, June                             Steven G. Johnson joined the faculty of Applied
                    1987.                                                                                             Mathematics at MIT in 2004. He received his Ph. D.
               [46] C. Temperton, “Fast mixed-radix real Fourier transforms,” Journal of                              in 2001 from the Dept. of Physics at MIT, where
                    Computational Physics, vol. 52, pp. 340–350, 1983.                                                he also received undergraduate degrees in computer
               [47] G. D. Bergland, “A fast Fourier transform algorithm for real-valued                               science and mathematics. His recent work, besides
                    series,” Comm. ACM, vol. 11, no. 10, pp. 703–710, 1968.                                           FFTW, has focused on the theory of photonic
               [48] R. N. Bracewell, The Hartley Transform.           New York: Oxford                                crystals: electromagnetism in nano-structured media.
                    Univ. Press, 1986.                                                                                This has ranged from general research in semi-
               [49] H. J. Nussbaumer, Fast Fourier transform and convolution algorithms,                              analytical and numerical methods for electromag-
                    2nd ed.   Springer-Verlag, 1982.                                                                  netism, to the design of integrated optical devices,
               [50] J.-I. Guo, “An efﬁcient design for one-dimensional discrete Hartley                               to the developement of optical ﬁbers that guide light
                    transform using parallel additions,” IEEE Trans. Signal Processing,      within an air core to circumvent limits of solid materials. His Ph. D. thesis was
                    vol. 48, no. 10, pp. 2806–2813, Oct. 2000.                               published as a book, Photonic Crystals: The Road from Theory to Practice,
               [51] Z. Wang, “Fast algorithms for the discrete W transform and for the       in 2002.
                    discrete Fourier transform,” IEEE Trans. Acoustics, Speech and Signal
                    Processing, vol. 32, no. 4, pp. 803–816, 1984.
               [52] S. A. Martucci, “Symmetric convolution and the discrete sine and cosine
                    transforms,” IEEE Trans. Signal Processing, vol. 42, pp. 1038–1051,
                    1994.
               [53] K. R. Rao and P. Yip, Discrete Cosine Transform: Algorithms, Advan-
                    tages, Applications.  Boston, MA: Academic Press, 1990.
               [54] R. Vuduc and J. Demmel, “Code generators for automatic tuning of
                    numerical kernels: experiences with FFTW,” in Proc. Semantics, Appli-
                    cation, and Implementation of Code Generators Workshop, Montreal,
                    Sept. 2000.
               [55] J. Makhoul, “A fast cosine transform in one and two dimensions,” IEEE
                    Trans. Acoustics, Speech and Signal Processing, vol. 28, no. 1, pp. 27–
                    34, 1980.
               [56] S. C. Chan and K. L. Ho, “Direct methods for computing discrete
                    sinusoidal transforms,” IEE Proc. F, vol. 137, no. 6, pp. 433–442, 1990.
               [57] Z. Wang, “On computing the discrete Fourier and cosine transforms,”
                    IEEE Trans. Acoustics, Speech and Signal Processing, vol. 33, no. 4,
                    pp. 1341–1344, 1985.
               [58] S. C. Chan and K. L. Ho, “Fast algorithms for computing the discrete
                    cosine transform,” IEEE Trans. Circuits and Systems II: Analog and
                    Digital Signal Processing, vol. 39, no. 3, pp. 185–190, 1992.
               [59] J. C. Schatzman, “Accuracy of the discrete Fourier transform and the
                    fast Fourier transform,” SIAM J. Scientiﬁc Computing, vol. 17, no. 5,
                    pp. 1150–1166, 1996.
               [60] F. Franchetti, H. Karner, S. Kral, and C. W. Ueberhuber, “Architecture
                    independent short vector FFTs,” in Proc. IEEE Int’l Conf. Acoustics,
                    Speech, and Signal Processing, vol. 2, 2001, pp. 1109–1112.
                                       Matteo Frigo received his Ph. D. in 1999 from
                                       the Dept. of Electrical Engineering and Computer
                                       Science at the Massachusetts Institute of Technology
                                       (MIT). Besides FFTW, his research interests include
                                       the theory and implementation of Cilk (a multi-
                                       threaded system for parallel programming), cache-
                                       oblivious algorithms, and software radios. In addi-
                                       tion, he has implemented a gas analyzer that is used
                                       for clinical tests on lungs.
                                          Joint recipient, with Steven G. Johnson, of the
                                       1999 J. H. Wilkinson Prize for Numerical Software,
               in recognition of their work on FFTW.
                                                                                          16
