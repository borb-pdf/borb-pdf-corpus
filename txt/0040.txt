                          Distinctive Image Features
                       fromScale-InvariantKeypoints
                                 DavidG.Lowe
                             ComputerScience Department
                             University of British Columbia
                              Vancouver, B.C., Canada
                                 lowe@cs.ubc.ca
                                 January 5, 2004
                                   Abstract
                This paper presents a method for extracting distinctive invariant features from
                images that can be used to perform reliable matching between different views of
                an object or scene. The features are invariant to image scale and rotation, and
                are shown to provide robust matching across a a substantial range of afﬁne dis-
                tortion, change in 3D viewpoint, addition of noise, and change in illumination.
                The features are highly distinctive, in the sense that a single feature can be cor-
                rectly matched with high probability against a large database of features from
                many images. This paper also describes an approach to using these features
                for object recognition. The recognition proceeds by matching individual fea-
                tures to a database of features from known objects using a fast nearest-neighbor
                algorithm, followed by a Hough transform to identify clusters belonging to a sin-
                gle object, and ﬁnally performing veriﬁcation through least-squares solution for
                consistent pose parameters. This approach to recognition can robustly identify
                objects amongclutter and occlusion while achieving near real-time performance.
             Accepted for publication in the International Journal of Computer Vision, 2004.
                                     1
          1 Introduction
          Image matching is a fundamental aspect of many problems in computer vision, including
          object or scene recognition, solving for 3D structure from multiple images, stereo correspon-
          dence, and motion tracking. This paper describes image features that have many properties
          that make them suitable for matching differing images of an object or scene. The features are
          invariant to image scaling and rotation, and partially invariant to change in illumination and
          3Dcameraviewpoint. Theyare well localized in both the spatial and frequency domains, re-
          ducing the probability of disruption by occlusion, clutter, or noise. Large numbers of features
          can be extracted from typical images with efﬁcient algorithms. In addition, the features are
          highly distinctive, which allows a single feature to be correctly matched with high probability
          against a large database of features, providing a basis for object and scene recognition.
            Thecost of extracting these features is minimized by taking a cascade ﬁltering approach,
          in which the more expensive operations are applied only at locations that pass an initial test.
          Following are the major stages of computation used to generate the set of image features:
           1. Scale-space extrema detection: Theﬁrststageofcomputation searches overallscales
             and image locations. It is implemented efﬁciently by using a difference-of-Gaussian
             function to identify potential interest points that are invariant to scale and orientation.
           2. Keypointlocalization: At each candidate location, a detailed model is ﬁt to determine
             location and scale. Keypoints are selected based on measures of their stability.
           3. Orientation assignment: One or more orientations are assigned to each keypoint lo-
             cation based on local image gradient directions. All future operations are performed
             onimagedatathat has been transformed relative to the assigned orientation, scale, and
             location for each feature, thereby providing invariance to these transformations.
           4. Keypoint descriptor: The local image gradients are measured at the selected scale
             in the region around each keypoint. These are transformed into a representation that
             allows for signiﬁcant levels of local shape distortion and change in illumination.
          This approach has been named the Scale Invariant Feature Transform (SIFT), as it transforms
          image data into scale-invariant coordinates relative to local features.
            An important aspect of this approach is that it generates large numbers of features that
          densely cover the image over the full range of scales and locations. A typical image of size
          500x500 pixels will give rise to about 2000 stable features (although this number depends on
          both image content and choices for various parameters). The quantity of features is partic-
          ularly important for object recognition, where the ability to detect small objects in cluttered
          backgrounds requires that at least 3 features be correctly matched from each object for reli-
          able identiﬁcation.
            For image matching and recognition, SIFT features are ﬁrst extracted from a set of ref-
          erence images and stored in a database. A new image is matched by individually comparing
          each feature from the new image to this previous database and ﬁnding candidate match-
          ing features based on Euclidean distance of their feature vectors. This paper will discuss
          fast nearest-neighbor algorithms that can perform this computation rapidly against large
          databases.
            The keypoint descriptors are highly distinctive, which allows a single feature to ﬁnd its
          correct match with good probability in a large database of features. However, in a cluttered
                             2
          image, many features from the background will not have any correct match in the database,
          giving rise to many false matches in addition to the correct ones. The correct matches can
          be ﬁltered from the full set of matches by identifying subsets of keypoints that agree on the
          object and its location, scale, and orientation in the new image. The probability that several
          features will agree on these parameters by chance is much lower than the probability that
          any individual feature match will be in error. The determination of these consistent clusters
          can be performed rapidly by using an efﬁcient hash table implementation of the generalized
          Houghtransform.
            Each cluster of 3 or more features that agree on an object and its pose is then subject
          to further detailed veriﬁcation. First, a least-squared estimate is made for an afﬁne approxi-
          mation to the object pose. Any other image features consistent with this pose are identiﬁed,
          and outliers are discarded. Finally, a detailed computation is made of the probability that a
          particular set of features indicates the presence of an object, given the accuracy of ﬁt and
          number of probable false matches. Object matches that pass all these tests can be identiﬁed
          as correct with high conﬁdence.
          2 Relatedresearch
          Thedevelopment of image matching by using a set of local interest points can be traced back
          to the work of Moravec (1981) on stereo matching using a corner detector. The Moravec
          detector was improved by Harris and Stephens (1988) to make it more repeatable under small
          image variations and near edges. Harris also showed its value for efﬁcient motion tracking
          and 3D structure from motion recovery (Harris, 1992), and the Harris corner detector has
          since been widely used for many other image matching tasks. While these feature detectors
          are usually called corner detectors, they are not selecting just corners, but rather any image
          location that has large gradients in all directions at a predetermined scale.
            The initial applications were to stereo and short-range motion tracking, but the approach
          was later extended to more difﬁcult problems. Zhang et al. (1995) showed that it was possi-
          ble to match Harris corners over a large image range by using a correlation window around
          each corner to select likely matches. Outliers were then removed by solving for a funda-
          mental matrix describing the geometric constraints between the two views of rigid scene and
          removing matches that did not agree with the majority solution. At the same time, a similar
          approach was developed by Torr (1995) for long-range motion matching, in which geometric
          constraints were used to remove outliers for rigid objects moving within an image.
            The ground-breaking work of Schmid and Mohr (1997) showed that invariant local fea-
          ture matching could be extended to general image recognition problems in which a feature
          was matched against a large database of images. They also used Harris corners to select
          interest points, but rather than matching with a correlation window, they used a rotationally
          invariant descriptor of the local image region. This allowed features to be matched under
          arbitrary orientation change between the two images. Furthermore, they demonstrated that
          multiple feature matches could accomplish general recognition under occlusion and clutter
          by identifying consistent clusters of matched features.
            The Harris corner detector is very sensitive to changes in image scale, so it does not
          provide a good basis for matching images of different sizes. Earlier work by the author
          (Lowe, 1999) extended the local feature approach to achieve scale invariance. This work
          also described a new local descriptor that provided more distinctive features while being less
                             3
          sensitive to local image distortions such as 3D viewpoint change. This current paper provides
          amorein-depthdevelopmentandanalysisofthisearlierwork,whilealsopresentinganumber
          of improvements in stability and feature invariance.
            There is a considerable body of previous research on identifying representations that are
          stable under scale change. Some of the ﬁrst work in this area was by Crowley and Parker
          (1984), who developed a representation that identiﬁed peaks and ridges in scale space and
          linked these into a tree structure. The tree structure could then be matched between images
          with arbitrary scale change. More recent work on graph-based matching by Shokoufandeh,
          Marsic and Dickinson (1999) provides more distinctive feature descriptors using wavelet co-
          efﬁcients. Theproblem ofidentifying anappropriate andconsistent scaleforfeature detection
          has been studied in depth by Lindeberg (1993, 1994). He describes this as a problem of scale
          selection, and we make use of his results below.
            Recently, there has been an impressive body of work on extending local features to be
          invariant to full afﬁne transformations (Baumberg, 2000; Tuytelaars and Van Gool, 2000;
          Mikolajczyk andSchmid,2002; Schaffalitzky andZisserman, 2002; BrownandLowe,2002).
          This allows for invariant matching to features on a planar surface under changes in ortho-
          graphic 3D projection, in most cases by resampling the image in a local afﬁne frame. How-
          ever, none of these approaches are yet fully afﬁne invariant, as they start with initial feature
          scales and locations selected in a non-afﬁne-invariant manner due to the prohibitive cost of
          exploring the full afﬁne space. The afﬁne frames are are also more sensitive to noise than
          those of the scale-invariant features, so in practice the afﬁne features have lower repeatability
          than the scale-invariant features unless the afﬁne distortion is greater than about a 40 degree
          tilt of a planar surface (Mikolajczyk, 2002). Wider afﬁne invariance may not be important for
          manyapplications, as training views are best taken at least every 30 degrees rotation in view-
          point (meaning that recognition is within 15 degrees of the closest training view) in order to
          capture non-planar changes and occlusion effects for 3D objects.
            While the method to be presented in this paper is not fully afﬁne invariant, a different
          approach is used in which the local descriptor allows relative feature positions to shift signif-
          icantly with only small changes in the descriptor. This approach not only allows the descrip-
          tors to be reliably matched across a considerable range of afﬁne distortion, but it also makes
          the features more robust against changes in 3D viewpoint for non-planar surfaces. Other
          advantages include much more efﬁcient feature extraction and the ability to identify larger
          numbers of features. On the other hand, afﬁne invariance is a valuable property for matching
          planar surfaces under very large view changes, and further research should be performed on
          the best ways to combine this with non-planar 3D viewpoint invariance in an efﬁcient and
          stable manner.
            Manyotherfeature types have been proposed for use in recognition, some of which could
          be used in addition to the features described in this paper to provide further matches under
          differing circumstances. One class of features are those that make use of image contours or
          region boundaries, which should make them less likely to be disrupted by cluttered back-
          grounds near object boundaries. Matas et al., (2002) have shown that their maximally-stable
          extremal regions can produce large numbers of matching features with good stability. Miko-
          lajczyk et al., (2003) have developed a new descriptor that uses local edges while ignoring
          unrelated nearby edges, providing the ability to ﬁnd stable features even near the boundaries
          of narrow shapes superimposed on background clutter. Nelson and Selinger (1998) have
          shown good results with local features based on groupings of image contours. Similarly,
                             4
                  Pope and Lowe (2000) used features based on the hierarchical grouping of image contours,
                  which are particularly useful for objects lacking detailed texture.
                    The history of research on visual recognition contains work on a diverse set of other
                  image properties that can be used as feature measurements. Carneiro and Jepson (2002)
                  describe phase-based local features that represent the phase rather than the magnitude of local
                  spatial frequencies, which is likely to provide improved invariance to illumination. Schiele
                  and Crowley (2000) have proposed the use of multidimensional histograms summarizing the
                  distribution of measurements within image regions. This type of feature may be particularly
                  useful for recognition of textured objects with deformable shapes. Basri and Jacobs (1997)
                  have demonstrated the value of extracting local region boundaries for recognition. Other
                  useful properties to incorporate include color, motion, ﬁgure-ground discrimination, region
                  shape descriptors, and stereo depth cues. The local feature approach can easily incorporate
                  novel feature types because extra features contribute to robustness when they provide correct
                  matches, but otherwise do little harm other than their cost of computation. Therefore, future
                  systems are likely to combine many feature types.
                  3 Detection of scale-space extrema
                  Asdescribed in the introduction, we will detect keypoints using a cascade ﬁltering approach
                  that uses efﬁcient algorithms to identify candidate locations that are then examined in further
                  detail. The ﬁrst stage of keypoint detection is to identify locations and scales that can be
                  repeatably assigned under differing views of the same object. Detecting locations that are
                  invariant to scale change of the image can be accomplished by searching for stable features
                  across all possible scales, using a continuous function of scale known as scale space (Witkin,
                  1983).
                    It has been shown by Koenderink (1984) and Lindeberg (1994) that under a variety of
                  reasonable assumptions the only possible scale-space kernel is the Gaussian function. There-
                  fore, the scale space of an image is deﬁned as a function, L(x;y;σ), that is produced from
                  the convolution of a variable-scale Gaussian, G(x;y;σ), with an input image, I(x;y):
                                      L(x;y;σ) = G(x;y;σ)∗I(x;y);
                  where ∗ is the convolution operation in x and y, and
                                                  1  −(x2+y2)=2σ2
                                      G(x;y;σ) = 2πσ2e        :
                  Toefﬁciently detect stable keypoint locations in scale space, we have proposed (Lowe, 1999)
                  using scale-space extrema in the difference-of-Gaussian function convolved with the image,
                  D(x;y;σ), which can be computed from the difference of two nearby scales separated by a
                  constant multiplicative factor k:
                               D(x;y;σ) = (G(x;y;kσ)−G(x;y;σ))∗I(x;y)
                                         = L(x;y;kσ)−L(x;y;σ):                   (1)
                    Thereareanumberofreasonsforchoosingthisfunction. First,itisaparticularly efﬁcient
                  function to compute, as the smoothed images, L; need to be computed in any case for scale
                  space feature description, and D can therefore be computed by simple image subtraction.
                                                  5
                       . . .
                   Scale
                   (next
                   octave)
                 Scale
                 (first
                 octave)
                                                 Difference of
                          Gaussian               Gaussian (DOG)
              Figure 1: For each octave of scale space, the initial image is repeatedly convolved with Gaussians to
              produce the set of scale space images shown on the left. Adjacent Gaussian images are subtracted
              to produce the difference-of-Gaussian images on the right. After each octave, the Gaussian image is
              down-sampledbyafactorof2,andtheprocessrepeated.
                In addition, the difference-of-Gaussian function provides a close approximation to the
              scale-normalized Laplacian of Gaussian, σ2∇2G, as studied by Lindeberg (1994). Lindeberg
              showed that the normalization of the Laplacian with the factor σ2 is required for true scale
              invariance. In detailed experimental comparisons, Mikolajczyk (2002) found that the maxima
              and minima of σ2∇2G produce the most stable image features compared to a range of other
              possible image functions, such as the gradient, Hessian, or Harris corner function.
                Therelationship between D and σ2∇2Gcan beunderstood from the heat diffusion equa-
              tion (parameterized in terms of σ rather than the more usual t = σ2):
                                   ∂G =σ∇2G:
                                   ∂σ
              From this, we see that ∇2G can be computed from the ﬁnite difference approximation to
              ∂G=∂σ,using the difference of nearby scales at kσ and σ:
                           σ∇2G= ∂G ≈ G(x;y;kσ)−G(x;y;σ)
              and therefore,     ∂σ       kσ−σ
                           G(x;y;kσ)−G(x;y;σ) ≈ (k−1)σ2∇2G:
              This shows that when the difference-of-Gaussian function has scales differing by a con-
              stant factor it already incorporates the σ2 scale normalization required for the scale-invariant
                                       6
                                                          Scale
                         Figure 2: Maxima and minima of the difference-of-Gaussian images are detected by comparing a
                         pixel (marked with X) to its 26 neighbors in 3x3 regions at the current and adjacent scales (marked
                         with circles).
                         Laplacian. The factor (k − 1) in the equation is a constant over all scales and therefore does
                         not inﬂuence extrema location. The approximation error will go to zero as k goes to 1, but
                         in practice we have found that the approximation has almost no impact on the stability of
                                                                                                                 √
                         extrema detection or localization for even signiﬁcant differences in scale, such as k =   2.
                             An efﬁcient approach to construction of D(x;y;σ) is shown in Figure 1. The initial
                         image is incrementally convolved with Gaussians to produce images separated by a constant
                         factor k in scale space, shown stacked in the left column. We choose to divide each octave
                                                                                                                   1=s
                         of scale space (i.e., doubling of σ) into an integer number, s, of intervals, so k = 2       :
                         Wemust produce s + 3 images in the stack of blurred images for each octave, so that ﬁnal
                         extrema detection covers a complete octave. Adjacent image scales are subtracted to produce
                         the difference-of-Gaussian images shown on the right. Once a complete octave has been
                         processed, we resample the Gaussian image that has twice the initial value of σ (it will be 2
                         images from the top of the stack) by taking every second pixel in each row and column. The
                         accuracy of sampling relative to σ is no different than for the start of the previous octave,
                         while computation is greatly reduced.
                         3.1   Local extrema detection
                         In order to detect the local maxima and minima of D(x;y;σ), each sample point is compared
                         to its eight neighbors in the current image and nine neighbors in the scale above and below
                         (see Figure 2). It is selected only if it is larger than all of these neighbors or smaller than all
                         of them. The cost of this check is reasonably low due to the fact that most sample points will
                         be eliminated following the ﬁrst few checks.
                             Animportant issue is to determine the frequency of sampling in the image and scale do-
                         mains that is needed to reliably detect the extrema. Unfortunately, it turns out that there is
                         no minimum spacing of samples that will detect all extrema, as the extrema can be arbitrar-
                         ily close together. This can be seen by considering a white circle on a black background,
                         which will have a single scale space maximum where the circular positive central region of
                         the difference-of-Gaussian function matches the size and location of the circle. For a very
                         elongated ellipse, there will be two maxima near each end of the ellipse. As the locations of
                         maximaareacontinuous function oftheimage,forsomeellipse withintermediate elongation
                         there will be a transition from a single maximum to two, with the maxima arbitrarily close to
                                                                       7
                              100                                                                       3500
                                80                                                                      3000
                                                                                                        2500
                                60
                                                                                                        2000
                                40
                           Repeatability (%)                                                            1500
                                20                    Matching location and scale                                                Total number of keypoints
                                                   Nearest descriptor in database                   Number of keypoints per image 1000Nearest descriptor in database
                                 0 1       2        3       4       5       6       7       8            500 1        2       3       4       5       6       7       8
                                             Number of scales sampled per octave                                       Number of scales sampled per octave
                                 Figure 3: The top line of the ﬁrst graph shows the percent of keypoints that are repeatably detected at
                                 the same location and scale in a transformed image as a function of the number of scales sampled per
                                 octave. The lower line shows the percent of keypoints that have their descriptors correctly matched to
                                 a large database. The second graph shows the total number of keypoints detected in a typical image
                                 as a function of the number of scale samples.
                                 each other near the transition.
                                       Therefore, we must settle for a solution that trades off efﬁciency with completeness.
                                 In fact, as might be expected and is conﬁrmed by our experiments, extrema that are close
                                 together are quite unstable to small perturbations of the image. We can determine the best
                                 choices experimentally by studying a range of sampling frequencies and using those that
                                 provide the most reliable results under a realistic simulation of the matching task.
                                 3.2      Frequency of sampling in scale
                                 The experimental determination of sampling frequency that maximizes extrema stability is
                                 shown in Figures 3 and 4. These ﬁgures (and most other simulations in this paper) are based
                                 onamatchingtaskusingacollection of32realimagesdrawnfromadiverserange, including
                                 outdoor scenes, human faces, aerial photographs, and industrial images (the image domain
                                 wasfoundtohavealmostnoinﬂuenceonanyoftheresults). Eachimagewasthensubjecttoa
                                 range of transformations, including rotation, scaling, afﬁne stretch, change in brightness and
                                 contrast, and addition of image noise. Because the changes were synthetic, it was possible
                                 to precisely predict where each feature in an original image should appear in the transformed
                                 image, allowing for measurement of correct repeatability and positional accuracy for each
                                 feature.
                                       Figure 3 shows these simulation results used to examine the effect of varying the number
                                 of scales per octave at which the image function is sampled prior to extrema detection. In
                                 this case, each image was resampled following rotation by a random angle and scaling by
                                 a random amount between 0.2 of 0.9 times the original size. Keypoints from the reduced
                                 resolution image were matched against those from the original image so that the scales for all
                                 keypoints would be be present in the matched image. In addition, 1% image noise was added,
                                 meaning that each pixel had a random number added from the uniform interval [-0.01,0.01]
                                 where pixel values are in the range [0,1] (equivalent to providing slightly less than 6 bits of
                                 accuracy for image pixels).
                                                                                                8
                       100
                       80
                       60
                       40
                    Repeatability (%)Matching location and scale
                              Nearest descriptor in database
                       20
                        0 1   1.2  1.4  1.6  1.8   2
                            Prior smoothing for each octave (sigma)
            Figure4: Thetoplineinthegraphshowsthepercentofkeypointlocationsthatarerepeatablydetected
            in a transformed image as a function of the prior image smoothing for the ﬁrst level of each octave.
            Thelowerline showsthe percentof descriptors correctly matched against a large database.
              Thetopline in the ﬁrst graph of Figure 3 shows the percent of keypoints that are detected
            at a matching location and scale in the transformed image. For all examples in this paper, we
            deﬁne a matching scale as being within a factor of √2 of the correct scale, and a matching
            location as being within σ pixels, where σ is the scale of the keypoint (deﬁned from equation
            (1) as the standard deviation of the smallest Gaussian used in the difference-of-Gaussian
            function). The lower line on this graph shows the number of keypoints that are correctly
            matched to a database of 40,000 keypoints using the nearest-neighbor matching procedure
            to be described in Section 6 (this shows that once the keypoint is repeatably located, it is
            likely to be useful for recognition and matching tasks). As this graph shows, the highest
            repeatability is obtained when sampling 3 scales per octave, and this is the number of scale
            samples used for all other experiments throughout this paper.
              It might seem surprising that the repeatability does not continue to improve as more
            scales are sampled. The reason is that this results in many more local extrema being detected,
            but these extrema are on average less stable and therefore are less likely to be detected in
            the transformed image. This is shown by the second graph in Figure 3, which shows the
            average number of keypoints detected and correctly matched in each image. The number of
            keypoints rises with increased sampling of scales and the total number of correct matches also
            rises. Since the success of object recognition often depends more on the quantity of correctly
            matched keypoints, as opposed to their percentage correct matching, for many applications it
            will be optimal to use a larger number of scale samples. However, the cost of computation
            also rises with this number, so for the experiments in this paper we have chosen to use just 3
            scale samples per octave.
              Tosummarize, these experiments show that the scale-space difference-of-Gaussian func-
            tion has a large number of extrema and that it would be very expensive to detect them all.
            Fortunately, we can detect the most stable and useful subset even with a coarse sampling of
            scales.
                                   9
                        3.3   Frequency of sampling in the spatial domain
                        Just as we determined the frequency of sampling per octave of scale space, so we must de-
                        termine the frequency of sampling in the image domain relative to the scale of smoothing.
                        Given that extrema can be arbitrarily close together, there will be a similar trade-off between
                        sampling frequency and rate of detection. Figure 4 shows an experimental determination of
                        the amount of prior smoothing, σ, that is applied to each image level before building the
                        scale space representation for an octave. Again, the top line is the repeatability of keypoint
                        detection, and the results show that the repeatability continues to increase with σ. However,
                        there is a cost to using a large σ in terms of efﬁciency, so we have chosen to use σ = 1:6,
                        which provides close to optimal repeatability. This value is used throughout this paper and
                        wasused for the results in Figure 3.
                            Of course, if we pre-smooth the image before extrema detection, we are effectively dis-
                        carding the highest spatial frequencies. Therefore, to make full use of the input, the image
                        can be expanded to create more sample points than were present in the original. We dou-
                        ble the size of the input image using linear interpolation prior to building the ﬁrst level of
                        the pyramid. While the equivalent operation could effectively have been performed by us-
                        ing sets of subpixel-offset ﬁlters on the original image, the image doubling leads to a more
                        efﬁcient implementation. We assume that the original image has a blur of at least σ = 0:5
                        (the minimum needed to prevent signiﬁcant aliasing), and that therefore the doubled image
                        has σ = 1:0 relative to its new pixel spacing. This means that little additional smoothing is
                        needed prior to creation of the ﬁrst octave of scale space. The image doubling increases the
                        number of stable keypoints by almost a factor of 4, but no signiﬁcant further improvements
                        were found with a larger expansion factor.
                        4 Accuratekeypoint localization
                        Once a keypoint candidate has been found by comparing a pixel to its neighbors, the next
                        step is to perform a detailed ﬁt to the nearby data for location, scale, and ratio of principal
                        curvatures. This information allows points to be rejected that have low contrast (and are
                        therefore sensitive to noise) or are poorly localized along an edge.
                            The initial implementation of this approach (Lowe, 1999) simply located keypoints at
                        the location and scale of the central sample point. However, recently Brown has developed
                        a method (Brown and Lowe, 2002) for ﬁtting a 3D quadratic function to the local sample
                        points to determine the interpolated location of the maximum, and his experiments showed
                        that this provides a substantial improvement to matching and stability. His approach uses the
                        Taylor expansion (up to the quadratic terms) of the scale-space function, D(x;y;σ), shifted
                        so that the origin is at the sample point:
                                                                  ∂DT       1    ∂2D
                                                                               T
                                                    D(x)=D+            x+ x         2 x                         (2)
                                                                  ∂x        2    ∂x
                                                                                                     T
                        where D and its derivatives are evaluated at the sample point and x = (x;y;σ)   is the offset
                        from this point. The location of the extremum, xˆ, is determined by taking the derivative of
                        this function with respect to x and setting it to zero, giving
                                                                   ∂2D−1∂D
                                                            xˆ = −    2       :                                 (3)
                                                                   ∂x      ∂x
                                                                    10
           (a)                  (b)
           (c)                  (d)
           Figure 5: This ﬁgure shows the stages of keypoint selection. (a) The 233x189 pixel original image.
           (b) The initial 832 keypoints locations at maxima and minima of the difference-of-Gaussianfunction.
           Keypoints are displayed as vectors indicating scale, orientation, and location. (c) After applying
           a threshold on minimum contrast, 729 keypoints remain. (d) The ﬁnal 536 keypoints that remain
           following an additional threshold on ratio of principal curvatures.
           As suggested by Brown, the Hessian and derivative of D are approximated by using dif-
           ferences of neighboring sample points. The resulting 3x3 linear system can be solved with
           minimal cost. If the offset xˆ is larger than 0.5 in any dimension, then it means that the ex-
           tremum lies closer to a different sample point. In this case, the sample point is changed and
           the interpolation performed instead about that point. The ﬁnal offset xˆ is added to the location
           of its sample point to get the interpolated estimate for the location of the extremum.
             The function value at the extremum, D(xˆ), is useful for rejecting unstable extrema with
           low contrast. This can be obtained by substituting equation (3) into (2), giving
                                 1∂DT
                          D(xˆ) = D + 2 ∂x xˆ:
           For the experiments in this paper, all extrema with a value of |D(xˆ)| less than 0.03 were
           discarded (as before, we assume image pixel values in the range [0,1]).
             Figure 5 shows the effects of keypoint selection on a natural image. In order to avoid too
           much clutter, a low-resolution 233 by 189 pixel image is used and keypoints are shown as
           vectors giving the location, scale, and orientation of each keypoint (orientation assignment is
           described below). Figure 5 (a) shows the original image, which is shown at reduced contrast
           behind the subsequent ﬁgures. Figure 5 (b) shows the 832 keypoints at all detected maxima
                               11
                     and minima of the difference-of-Gaussian function, while (c) shows the 729 keypoints that
                     remain following removal of those with a value of |D(xˆ)| less than 0.03. Part (d) will be
                     explained in the following section.
                     4.1   Eliminating edge responses
                     For stability, it is not sufﬁcient to reject keypoints with low contrast. The difference-of-
                     Gaussian function will have a strong response along edges, even if the location along the
                     edge is poorly determined and therefore unstable to small amounts of noise.
                         Apoorly deﬁned peak in the difference-of-Gaussian function will have a large principal
                     curvature across the edge but a small one in the perpendicular direction. The principal curva-
                     tures can be computed from a 2x2 Hessian matrix, H, computed at the location and scale of
                     the keypoint:
                                                         " D     D #
                                                     H=      xx    xy                               (4)
                                                            D    D
                                                             xy    yy
                     Thederivatives are estimated by taking differences of neighboring sample points.
                         The eigenvalues of H are proportional to the principal curvatures of D. Borrowing from
                     the approach used by Harris and Stephens (1988), we can avoid explicitly computing the
                     eigenvalues, as we are only concerned with their ratio. Let α be the eigenvalue with the
                     largest magnitude and β bethe smaller one. Then, wecan compute the sumofthe eigenvalues
                     from the trace of H and their product from the determinant:
                                                Tr(H) = D    +D =α+β;
                                                           xx    yy
                                                                       2
                                             Det(H) = D D −(D ) =αβ:
                                                         xx yy      xy
                     Intheunlikely eventthatthedeterminant isnegative, thecurvatures have different signs sothe
                     point is discarded as not being an extremum. Let r be the ratio between the largest magnitude
                     eigenvalue and the smaller one, so that α = rβ. Then,
                                               2          2            2          2
                                         Tr(H) = (α+β) = (rβ+β) = (r+1) ;
                                         Det(H)       αβ          rβ2         r
                     which depends only on the ratio of the eigenvalues rather than their individual values. The
                                   2
                     quantity (r+1) =r is at a minimum when the two eigenvalues are equal and it increases with
                     r. Therefore, to check that the ratio of principal curvatures is below some threshold, r, we
                     only need to check
                                                           2          2
                                                     Tr(H) < (r+1) :
                                                     Det(H)       r
                     This is very efﬁcient to compute, with less than 20 ﬂoating point operations required to
                     test each keypoint. The experiments in this paper use a value of r = 10, which eliminates
                     keypoints that have a ratio between the principal curvatures greater than 10. The transition
                     from Figure 5 (c) to (d) shows the effects of this operation.
                                                             12
                   5 Orientation assignment
                   By assigning a consistent orientation to each keypoint based on local image properties, the
                   keypoint descriptor can be represented relative to this orientation and therefore achieve in-
                   variance to image rotation. This approach contrasts with the orientation invariant descriptors
                   of SchmidandMohr(1997),inwhicheachimagepropertyisbasedonarotationally invariant
                   measure. The disadvantage of that approach is that it limits the descriptors that can be used
                   and discards image information by not requiring all measures to be based on a consistent
                   rotation.
                      Following experimentation with a number of approaches to assigning a local orientation,
                   the following approach was found to give the most stable results. The scale of the keypoint
                   is used to select the Gaussian smoothed image, L, with the closest scale, so that all compu-
                   tations are performed in a scale-invariant manner. For each image sample, L(x;y), at this
                   scale, the gradient magnitude, m(x;y), and orientation, θ(x;y), is precomputed using pixel
                   differences:
                                 q                       2                       2
                         m(x;y) =  (L(x+1;y)−L(x−1;y)) +(L(x;y+1)−L(x;y−1))
                        θ(x;y) = tan−1((L(x;y +1)−L(x;y −1))=(L(x+1;y)−L(x−1;y)))
                      Anorientation histogram isformedfromthegradient orientations ofsample points within
                   a region around the keypoint. The orientation histogram has 36 bins covering the 360 degree
                   range of orientations. Each sample added to the histogram is weighted by its gradient magni-
                   tude and by a Gaussian-weighted circular window with a σ that is 1.5 times that of the scale
                   of the keypoint.
                      Peaks in the orientation histogram correspond to dominant directions of local gradients.
                   The highest peak in the histogram is detected, and then any other local peak that is within
                   80%ofthehighest peak is used to also create a keypoint with that orientation. Therefore, for
                   locations with multiple peaks ofsimilar magnitude, there willbe multiple keypoints created at
                   the same location and scale but different orientations. Only about 15% of points are assigned
                   multiple orientations, but these contribute signiﬁcantly to the stability of matching. Finally, a
                   parabola is ﬁt to the 3 histogram values closest to each peak to interpolate the peak position
                   for better accuracy.
                      Figure 6 shows the experimental stability of location, scale, and orientation assignment
                   under differing amounts of image noise. As before the images are rotated and scaled by
                   random amounts. The top line shows the stability of keypoint location and scale assign-
                   ment. The second line shows the stability of matching when the orientation assignment is
                   also required to be within 15 degrees. As shown by the gap between the top two lines, the
                   orientation assignment remains accurate 95% of the time even after addition of ±10% pixel
                   noise (equivalent to a camera providing less than 3 bits of precision). The measured vari-
                   ance of orientation for the correct matches is about 2.5 degrees, rising to 3.9 degrees for 10%
                   noise. The bottom line in Figure 6 shows the ﬁnal accuracy of correctly matching a keypoint
                   descriptor to a database of 40,000 keypoints (to be discussed below). As this graph shows,
                   the SIFT features are resistant to even large amounts of pixel noise, and the major cause of
                   error is the initial location and scale detection.
                                                     13
                           100
                           80
                           60
                           40
                        Repeatability (%)Matching location and scale
                           20   Matching location, scale, and orientation
                                     Nearest descriptor in database
                            0
                            0%    2%    4%    6%    8%    10%
                                         Image noise
               Figure 6: The top line in the graph shows the percent of keypoint locations and scales that are repeat-
               ably detected as a function of pixel noise. The second line shows the repeatability after also requiring
               agreementin orientation. The bottom line shows the ﬁnal percent of descriptors correctly matched to
               a large database.
               6 Thelocalimagedescriptor
               Theprevious operations have assigned an image location, scale, and orientation to each key-
               point. These parameters impose a repeatable local 2D coordinate system in which to describe
               the local image region, and therefore provide invariance to these parameters. The next step is
               to compute a descriptor for the local image region that is highly distinctive yet is as invariant
               as possible to remaining variations, such as change in illumination or 3D viewpoint.
                 One obvious approach would be to sample the local image intensities around the key-
               point at the appropriate scale, and to match these using a normalized correlation measure.
               However, simple correlation of image patches is highly sensitive to changes that cause mis-
               registration of samples, such as afﬁne or 3D viewpoint change or non-rigid deformations. A
               better approach has been demonstrated by Edelman, Intrator, and Poggio (1997). Their pro-
               posed representation was based upon a model of biological vision, in particular of complex
               neurons in primary visual cortex. These complex neurons respond to a gradient at a particular
               orientation and spatial frequency, but the location of the gradient on the retina is allowed to
               shift over a small receptive ﬁeld rather than being precisely localized. Edelman et al. hypoth-
               esized that the function of these complex neurons was to allow for matching and recognition
               of 3D objects from a range of viewpoints. They have performed detailed experiments using
               3Dcomputer models of object and animal shapes which show that matching gradients while
               allowing for shifts in their position results in much better classiﬁcation under 3D rotation. For
               example, recognition accuracy for 3D objects rotated in depth by 20 degrees increased from
               35%for correlation of gradients to 94% using the complex cell model. Our implementation
               described below was inspired by this idea, but allows for positional shift using a different
               computational mechanism.
                                         14
                                                                             Keypoint descriptor
                                 Image gradients
                     Figure 7: A keypoint descriptor is created by ﬁrst computing the gradient magnitude and orientation
                     at each image sample point in a region around the keypoint location, as shown on the left. These are
                     weightedbyaGaussianwindow,indicatedbytheoverlaidcircle. Thesesamplesarethenaccumulated
                     into orientation histogramssummarizingthecontentsover4x4subregions,asshownontheright,with
                     thelengthofeacharrowcorrespondingtothesumofthegradientmagnitudesnearthatdirectionwithin
                     the region. This ﬁgure shows a 2x2 descriptor array computed from an 8x8 set of samples, whereas
                     the experiments in this paper use 4x4 descriptors computed from a 16x16 sample array.
                     6.1   Descriptor representation
                     Figure 7 illustrates the computation of the keypoint descriptor. First the image gradient mag-
                     nitudes and orientations are sampled around the keypoint location, using the scale of the
                     keypoint to select the level of Gaussian blur for the image. In order to achieve orientation
                     invariance, the coordinates of the descriptor and the gradient orientations are rotated relative
                     to the keypoint orientation. For efﬁciency, the gradients are precomputed for all levels of the
                     pyramid as described in Section 5. These are illustrated with small arrows at each sample
                     location on the left side of Figure 7.
                         AGaussian weighting function with σ equal to one half the width of the descriptor win-
                     dow is used to assign a weight to the magnitude of each sample point. This is illustrated
                     with a circular window on the left side of Figure 7, although, of course, the weight falls off
                     smoothly. The purpose of this Gaussian window is to avoid sudden changes in the descriptor
                     with small changes in the position of the window, and to give less emphasis to gradients that
                     are far from the center of the descriptor, as these are most affected by misregistration errors.
                         The keypoint descriptor is shown on the right side of Figure 7. It allows for signiﬁcant
                     shift in gradient positions by creating orientation histograms over 4x4 sample regions. The
                     ﬁgure shows eight directions for each orientation histogram, with the length of each arrow
                     corresponding to the magnitude of that histogram entry. A gradient sample on the left can
                     shift up to 4 sample positions while still contributing to the same histogram on the right,
                     thereby achieving the objective of allowing for larger local positional shifts.
                         It is important to avoid all boundary affects in which the descriptor abruptly changes as a
                     sample shifts smoothly from being within one histogram to another or from one orientation
                     to another. Therefore, trilinear interpolation is used to distribute the value of each gradient
                     sample into adjacent histogram bins. In other words, each entry into a bin is multiplied by a
                     weight of 1 − d for each dimension, where d is the distance of the sample from the central
                     value of the bin as measured in units of the histogram bin spacing.
                                                             15
                         The descriptor is formed from a vector containing the values of all the orientation his-
                     togram entries, corresponding to the lengths of the arrows on the right side of Figure 7. The
                     ﬁgureshowsa2x2arrayoforientation histograms, whereas our experiments below show that
                     the best results are achieved with a 4x4 array of histograms with 8 orientation bins in each.
                     Therefore, the experiments in this paper use a 4x4x8 = 128 element feature vector for each
                     keypoint.
                         Finally, the feature vector is modiﬁed to reduce the effects of illumination change. First,
                     the vector is normalized to unit length. A change in image contrast in which each pixel value
                     is multiplied by a constant will multiply gradients by the same constant, so this contrast
                     change will be canceled by vector normalization. A brightness change in which a constant
                     is added to each image pixel will not affect the gradient values, as they are computed from
                     pixel differences. Therefore, the descriptor is invariant to afﬁne changes in illumination.
                     However, non-linear illumination changes can also occur due to camera saturation or due to
                     illumination changes that affect 3D surfaces with differing orientations by different amounts.
                     These effects can cause a large change in relative magnitudes for some gradients, but are less
                     likely to affect the gradient orientations. Therefore, we reduce the inﬂuence of large gradient
                     magnitudes by thresholding the values in the unit feature vector to each be no larger than
                     0.2, and then renormalizing to unit length. This means that matching the magnitudes for
                     large gradients is no longer as important, and that the distribution of orientations has greater
                     emphasis. The value of 0.2 was determined experimentally using images containing differing
                     illuminations for the same 3D objects.
                     6.2   Descriptor testing
                     There are two parameters that can be used to vary the complexity of the descriptor: the
                     numberoforientations, r,inthehistograms, andthewidth, n,ofthen×narrayoforientation
                     histograms. The size of the resulting descriptor vector is rn2. As the complexity of the
                     descriptor grows, it will be able to discriminate better in a large database, but it will also be
                     more sensitive to shape distortions and occlusion.
                         Figure 8 shows experimental results in which the number of orientations and size of the
                     descriptor were varied. The graph was generated for a viewpoint transformation in which a
                     planar surface is tilted by 50 degrees away from the viewer and 4% image noise is added.
                     This is near the limits of reliable matching, as it is in these more difﬁcult cases that descriptor
                     performance is most important. The results show the percent of keypoints that ﬁnd a correct
                     matchtothesingle closest neighbor among a database of 40,000 keypoints. The graph shows
                     that a single orientation histogram (n = 1) is very poor at discriminating, but the results
                     continue to improve up to a 4x4 array of histograms with 8 orientations. After that, adding
                     more orientations or a larger descriptor can actually hurt matching by making the descriptor
                     more sensitive to distortion. These results were broadly similar for other degrees of view-
                     point change and noise, although in some simpler cases discrimination continued to improve
                     (from already high levels) with 5x5 and higher descriptor sizes. Throughout this paper we
                     use a 4x4 descriptor with 8 orientations, resulting in feature vectors with 128 dimensions.
                     While the dimensionality of the descriptor may seem high, we have found that it consistently
                     performs better than lower-dimensional descriptors on a range of matching tasks and that the
                     computational cost of matching remains low when using the approximate nearest-neighbor
                     methods described below.
                                                             16
                                      60
                                      50
                                      40
                                      30
                                                               With 16 orientations
                                      20                        With 8 orientations
                                                                With 4 orientations
                                   Correct nearest descriptor (%) 10
                                       0 1           2           3          4           5
                                              Width n of descriptor (angle 50 deg, noise 4%)
                     Figure 8: This graph shows the percent of keypoints giving the correct match to a database of 40,000
                     keypoints as a function of width of the n × n keypoint descriptor and the number of orientations in
                     each histogram. The graph is computed for images with afﬁne viewpoint change of 50 degrees and
                     addition of 4% noise.
                     6.3   Sensitivity to afﬁne change
                     The sensitivity of the descriptor to afﬁne change is examined in Figure 9. The graph shows
                     the reliability of keypoint location and scale selection, orientation assignment, and nearest-
                     neighbor matching to a database as a function of rotation in depth of a plane away from a
                     viewer. It can be seen that each stage of computation has reduced repeatability with increas-
                     ing afﬁne distortion, but that the ﬁnal matching accuracy remains above 50% out to a 50
                     degree change in viewpoint.
                         To achieve reliable matching over a wider viewpoint angle, one of the afﬁne-invariant
                     detectors could be used to select and resample image regions, as discussed in Section 2. As
                     mentionedthere, noneoftheseapproaches istrulyafﬁne-invariant, astheyallstartfrominitial
                     feature locations determined in a non-afﬁne-invariant manner. In what appears to be the most
                     afﬁne-invariant method, Mikolajczyk (2002) has proposed and run detailed experiments with
                     the Harris-afﬁne detector. He found that its keypoint repeatability is below that given here out
                     to about a 50 degree viewpoint angle, but that it then retains close to 40% repeatability out to
                     an angle of 70 degrees, which provides better performance for extreme afﬁne changes. The
                     disadvantages are a much higher computational cost, a reduction in the number of keypoints,
                     and poorer stability for small afﬁne changes due to errors in assigning a consistent afﬁne
                     frame under noise. In practice, the allowable range of rotation for 3D objects is considerably
                     less than for planar surfaces, so afﬁne invariance is usually not the limiting factor in the
                     ability to match across viewpoint change. If a wide range of afﬁne invariance is desired, such
                     as for a surface that is known to be planar, then a simple solution is to adopt the approach of
                     Pritchard and Heidrich (2003) in which additional SIFT features are generated from 4 afﬁne-
                     transformed versions of the training image corresponding to 60 degree viewpoint changes.
                     This allows for the use of standard SIFT features with no additional cost when processing
                     the image to be recognized, but results in an increase in the size of the feature database by a
                     factor of 3.
                                                             17
                                               100
                                                80
                                                60
                                                40
                                           Repeatability (%)       Matching location and scale
                                                20     Matching location, scale, and orientation
                                                                 Nearest descriptor in database
                                                 0 0          10          20         30         40         50
                                                                   Viewpoint angle (degrees)
                          Figure 9: This graph shows the stability of detection for keypoint location, orientation, and ﬁnal
                          matching to a database as a function of afﬁne distortion. The degree of afﬁne distortion is expressed
                          in terms of the equivalent viewpoint rotation in depth for a planar surface.
                          6.4    Matching to large databases
                          An important remaining issue for measuring the distinctiveness of features is how the re-
                          liability of matching varies as a function of the number of features in the database being
                          matched. Most of the examples in this paper are generated using a database of 32 images
                          with about 40,000 keypoints. Figure 10 shows how the matching reliability varies as a func-
                          tion of database size. This ﬁgure was generated using a larger database of 112 images, with
                          a viewpoint depth rotation of 30 degrees and 2% image noise in addition to the usual random
                          image rotation and scale change.
                              The dashed line shows the portion of image features for which the nearest neighbor in
                          the database was the correct match, as a function of database size shown on a logarithmic
                          scale. The leftmost point is matching against features from only a single image while the
                          rightmost point is selecting matches from a database of all features from the 112 images. It
                          can be seen that matching reliability does decrease as a function of the number of distractors,
                          yet all indications are that many correct matches will continue to be found out to very large
                          database sizes.
                              The solid line is the percentage of keypoints that were identiﬁed at the correct match-
                          ing location and orientation in the transformed image, so it is only these points that have
                          any chance of having matching descriptors in the database. The reason this line is ﬂat is
                          that the test was run over the full database for each value, while only varying the portion
                          of the database used for distractors. It is of interest that the gap between the two lines is
                          small, indicating that matching failures are due more to issues with initial feature localization
                          and orientation assignment than to problems with feature distinctiveness, even out to large
                          database sizes.
                                                                           18
                                       100
                                        80
                                        60
                                        40     Matching location, scale, and orientation
                                   Repeatability (%)   Nearest descriptor in database
                                        20
                                         0
                                         1000               10000               100000
                                                Number of keypoints in database (log scale)
                     Figure 10: The dashed line shows the percent of keypoints correctly matched to a database as a
                     function of database size (using a logarithmic scale). The solid line shows the percent of keypoints
                     assigned the correct location, scale, and orientation. Images had random scale and rotation changes,
                     an afﬁne transform of 30 degrees, and image noise of 2% added prior to matching.
                     7 Application to object recognition
                     Themajortopic of this paper is the derivation of distinctive invariant keypoints, as described
                     above. To demonstrate their application, we will now give a brief description of their use
                     for object recognition in the presence of clutter and occlusion. More details on applications
                     of these features to recognition are available in other papers (Lowe, 1999; Lowe, 2001; Se,
                     LoweandLittle, 2002).
                         Object recognition is performed by ﬁrst matching each keypoint independently to the
                     database of keypoints extracted from training images. Many of these initial matches will be
                     incorrect due to ambiguous features or features that arise from background clutter. Therefore,
                     clusters of at least 3 features are ﬁrst identiﬁed that agree on an object and its pose, as these
                     clusters have a much higher probability of being correct than individual feature matches.
                     Then, each cluster is checked by performing a detailed geometric ﬁt to the model, and the
                     result is used to accept or reject the interpretation.
                     7.1   Keypoint matching
                     Thebestcandidate matchforeach keypoint is found by identifying its nearest neighbor in the
                     database of keypoints from training images. The nearest neighbor is deﬁned as the keypoint
                     with minimum Euclidean distance for the invariant descriptor vector as was described in
                     Section 6.
                         However, many features from an image will not have any correct match in the training
                     database because they arise from background clutter or were not detected in the training im-
                     ages. Therefore, it would be useful to have a way to discard features that do not have any
                     good match to the database. A global threshold on distance to the closest feature does not
                     perform well, as some descriptors are much more discriminative than others. A more ef-
                     fective measure is obtained by comparing the distance of the closest neighbor to that of the
                                                             19
                                                0.8
                                                0.7
                                                0.6         PDF for correct matches
                                                           PDF for incorrect matches
                                                0.5
                                           PDF  0.4
                                                0.3
                                                0.2
                                                0.1
                                                 0 0    0.1   0.2   0.3  0.4   0.5   0.6  0.7   0.8  0.9    1
                                                             Ratio of distances (closest/next closest)
                          Figure 11: The probability that a match is correct can be determined by taking the ratio of distance
                          fromtheclosest neighborto the distance of the second closest. Using a database of 40,000 keypoints,
                          the solid line shows the PDF of this ratio for correct matches, while the dotted line is for matches that
                          wereincorrect.
                          second-closest neighbor. If there are multiple training images of the same object, then we
                          deﬁne the second-closest neighbor as being the closest neighbor that is known to come from
                          a different object than the ﬁrst, such as by only using images known to contain different ob-
                          jects. This measure performs well because correct matches need to have the closest neighbor
                          signiﬁcantly closer than the closest incorrect match to achieve reliable matching. For false
                          matches, there will likely be a number of other false matches within similar distances due to
                          the high dimensionality of the feature space. We can think of the second-closest match as
                          providing an estimate of the density of false matches within this portion of the feature space
                          and at the same time identifying speciﬁc instances of feature ambiguity.
                              Figure 11 shows the value of this measure for real image data. The probability density
                          functions for correct and incorrect matches are shown in terms of the ratio of closest to
                          second-closest neighbors of each keypoint. Matches for which the nearest neighbor was
                          a correct match have a PDF that is centered at a much lower ratio than that for incorrect
                          matches. For our object recognition implementation, we reject all matches in which the
                          distance ratio is greater than 0.8, which eliminates 90% of the false matches while discarding
                          less than 5% of the correct matches. This ﬁgure was generated by matching images following
                          random scale and orientation change, a depth rotation of 30 degrees, and addition of 2%
                          image noise, against a database of 40,000 keypoints.
                          7.2    Efﬁcient nearest neighbor indexing
                          Noalgorithms are known that can identify the exact nearest neighbors of points in high di-
                          mensional spaces that are any more efﬁcient than exhaustive search. Our keypoint descriptor
                          has a 128-dimensional feature vector, and the best algorithms, such as the k-d tree (Friedman
                          et al., 1977) provide no speedup over exhaustive search for more than about 10 dimensional
                          spaces. Therefore, we have used an approximate algorithm, called the Best-Bin-First (BBF)
                          algorithm (Beis and Lowe, 1997). This is approximate in the sense that it returns the closest
                                                                           20
                     neighbor with high probability.
                         TheBBFalgorithmusesamodiﬁedsearchorderingforthek-dtreealgorithm sothatbins
                     in feature space are searched in the order of their closest distance from the query location.
                     This priority search order was ﬁrst examined by Arya and Mount (1993), and they provide
                     further study of its computational properties in (Arya et al., 1998). This search order requires
                     the use of a heap-based priority queue for efﬁcient determination of the search order. An
                     approximate answercanbereturned withlowcostbycuttingofffurther search afteraspeciﬁc
                     numberofthenearest binshavebeenexplored. Inourimplementation, wecutoffsearchafter
                     checking the ﬁrst 200 nearest-neighbor candidates. For a database of 100,000 keypoints, this
                     provides a speedup over exact nearest neighbor search by about 2 orders of magnitude yet
                     results in less than a 5% loss in the number of correct matches. One reason the BBFalgorithm
                     worksparticularly well for this problem is that we only consider matches in which the nearest
                     neighbor is less than 0.8 times the distance to the second-nearest neighbor (as described in
                     the previous section), and therefore there is no need to exactly solve the most difﬁcult cases
                     in which many neighbors are at very similar distances.
                     7.3   Clustering with the Hough transform
                     Tomaximizethe performance of object recognition for small or highly occluded objects, we
                     wish to identify objects with the fewest possible number of feature matches. We have found
                     that reliable recognition is possible with as few as 3 features. A typical image contains 2,000
                     or more features which may come from many different objects as well as background clutter.
                     While the distance ratio test described in Section 7.1 will allow us to discard many of the
                     false matches arising from background clutter, this does not remove matches from other valid
                     objects, and we often still need to identify correct subsets of matches containing less than 1%
                     inliers among 99% outliers. Many well-known robust ﬁtting methods, such as RANSAC or
                     Least Median of Squares, perform poorly when the percent of inliers falls much below 50%.
                     Fortunately, much better performance can be obtained by clustering features in pose space
                     using the Hough transform (Hough, 1962; Ballard, 1981; Grimson 1990).
                         The Hough transform identiﬁes clusters of features with a consistent interpretation by
                     using each feature to vote for all object poses that are consistent with the feature. When
                     clusters of features are found to vote for the same pose of an object, the probability of the
                     interpretation being correct is much higher than for any single feature. Each of our keypoints
                     speciﬁes 4 parameters: 2D location, scale, and orientation, and each matched keypoint in the
                     database has a record of the keypoint’s parameters relative to the training image in which it
                     wasfound. Therefore, we can create a Hough transform entry predicting the model location,
                     orientation, and scale from the match hypothesis. This prediction has large error bounds,
                     as the similarity transform implied by these 4 parameters is only an approximation to the
                     full 6 degree-of-freedom pose space for a 3D object and also does not account for any non-
                     rigid deformations. Therefore, we use broad bin sizes of 30 degrees for orientation, a factor
                     of 2 for scale, and 0.25 times the maximum projected training image dimension (using the
                     predicted scale) for location. To avoid the problem of boundary effects in bin assignment,
                     each keypoint match votes for the 2 closest bins in each dimension, giving a total of 16
                     entries for each hypothesis and further broadening the pose range.
                         In most implementations of the Hough transform, a multi-dimensional array is used to
                     represent the bins. However, many of the potential bins will remain empty, and it is difﬁcult
                     to compute the range of possible bin values due to their mutual dependence (for example,
                                                             21
                         the dependency of location discretization on the selected scale). These problems can be
                         avoided by using a pseudo-random hash function of the bin values to insert votes into a one-
                         dimensional hash table, in which collisions are easily detected.
                         7.4   Solution for afﬁne parameters
                         TheHoughtransform isused to identify all clusters with at least 3 entries in a bin. Each such
                         cluster is then subject to a geometric veriﬁcation procedure in which a least-squares solution
                         is performed for the best afﬁne projection parameters relating the training image to the new
                         image.
                            An afﬁne transformation correctly accounts for 3D rotation of a planar surface under
                         orthographic projection, but the approximation can be poor for 3D rotation of non-planar
                         objects. A more general solution would be to solve for the fundamental matrix (Luong and
                         Faugeras, 1996; Hartley and Zisserman, 2000). However, a fundamental matrix solution
                         requires at least 7 point matches as compared to only 3 for the afﬁne solution and in practice
                         requires even more matches for good stability. We would like to perform recognition with
                         as few as 3 feature matches, so the afﬁne solution provides a better starting point and we
                         can account for errors in the afﬁne approximation by allowing for large residual errors. If
                         we imagine placing a sphere around an object, then rotation of the sphere by 30 degrees
                         will move no point within the sphere by more than 0.25 times the projected diameter of the
                         sphere. For the examples of typical 3D objects used in this paper, an afﬁne solution works
                         well given that we allow residual errors up to 0.25 times the maximum projected dimension
                         of the object. A more general approach is given in (Brown and Lowe, 2002), in which the
                         initial solution is based on a similarity transform, which then progresses to solution for the
                         fundamental matrix in those cases in which a sufﬁcient number of matches are found.
                                                                            T                        T
                            Theafﬁnetransformation of a model point [x y] to an image point [u v]      can be written
                         as
                                                   " u # = " m1 m2 #" x #+" tx #
                                                     v         m3 m4         y         ty
                                                               T
                         where the model translation is [tx ty]  and the afﬁne rotation, scale, and stretch are repre-
                         sented by the mi parameters.
                            Wewishtosolvefor the transformation parameters, so the equation above can be rewrit-
                         ten to gather the unknowns into a column vector:
                                                                         m1 
                                                   x y 0 0 1 0  m2   
                                                                                        u
                                                 0 0 x y 0 1  m 
                                                                         3 = v 
                                                                                       
                                                         : : :           m4           .
                                                                                        .
                                                                                          .
                                                          : : :             tx 
                                                                              ty
                         This equation shows a single match, but any number of further matches can be added, with
                         each match contributing two more rows to the ﬁrst and last matrix. At least 3 matches are
                         needed to provide a solution.
                            Wecanwritethis linear system as
                                                                   Ax=b
                                                                      22
                          Figure 12: The training images for two objects are shown on the left. These can be recognized in a
                          cluttered image with extensive occlusion, shown in the middle. The results of recognition are shown
                          on the right. A parallelogram is drawn around each recognized object showing the boundaries of the
                          original training image under the afﬁne transformation solved for during recognition. Smaller squares
                          indicate the keypoints that were used for recognition.
                          Theleast-squares solution for the parameters x can be determined by solving the correspond-
                          ing normal equations,
                                                                       T −1 T
                                                               x=[A A] A b;
                          which minimizes the sum of the squares of the distances from the projected model locations
                          to the corresponding image locations. This least-squares approach could readily be extended
                          to solving for 3D pose and internal parameters of articulated and ﬂexible objects (Lowe,
                          1991).
                              Outliers can now be removed by checking for agreement between each image feature and
                          the model. Given the more accurate least-squares solution, we now require each match to
                          agree within half the error range that was used for the parameters in the Hough transform
                          bins.  If fewer than 3 points remain after discarding outliers, then the match is rejected.
                          As outliers are discarded, the least-squares solution is re-solved with the remaining points,
                          and the process iterated. In addition, a top-down matching phase is used to add any further
                          matches that agree with the projected model position. These may have been missed from the
                          Houghtransform bin due to the similarity transform approximation or other errors.
                              Theﬁnaldecision to accept or reject a model hypothesis is based on a detailed probabilis-
                          tic model given in a previous paper (Lowe, 2001). This method ﬁrst computes the expected
                          numberoffalsematchestothemodelpose,giventheprojected size ofthemodel, thenumber
                          of features within the region, and the accuracy of the ﬁt. A Bayesian analysis then gives the
                          probability that the object is present based on the actual number of matching features found.
                          We accept a model if the ﬁnal probability for a correct interpretation is greater than 0.98.
                          For objects that project to small regions of an image, 3 features may be sufﬁcient for reli-
                          able recognition. For large objects covering most of a heavily textured image, the expected
                          number of false matches is higher, and as many as 10 feature matches may be necessary.
                          8 Recognition examples
                          Figure 12 shows an example of object recognition for a cluttered and occluded image con-
                          taining 3D objects. The training images of a toy train and a frog are shown on the left.
                                                                         23
           Figure 13: This example shows location recognition within a complex scene. The training images for
           locationsareshownattheupperleftandthe640x315pixeltestimagetakenfromadifferentviewpoint
           is on the upper right. The recognized regions are shown on the lower image, with keypoints shown
           as squares and an outer parallelogram showing the boundaries of the training images under the afﬁne
           transform used for recognition.
           Themiddleimage(ofsize 600x480 pixels) contains instances of these objects hidden behind
           others and with extensive background clutter so that detection of the objects may not be im-
           mediate even for human vision. The image on the right shows the ﬁnal correct identiﬁcation
           superimposed on a reduced contrast version of the image. The keypoints that were used for
           recognition are shown as squares with an extra line to indicate orientation. The sizes of the
           squares correspond to the image regions used to construct the descriptor. An outer parallel-
           ogram is also drawn around each instance of recognition, with its sides corresponding to the
           boundaries of the training images projected under the ﬁnal afﬁne transformation determined
           during recognition.
             Another potential application of the approach is to place recognition, in which a mobile
           device or vehicle could identify its location by recognizing familiar locations. Figure 13 gives
           an example of this application, in which training images are taken of a number of locations.
           As shown on the upper left, these can even be of such seemingly non-distinctive items as a
           woodenwalloratreewithtrashbins. Thetest image (ofsize 640 by 315 pixels) on the upper
           right was taken from a viewpoint rotated about 30 degrees around the scene from the original
           positions, yet the training image locations are easily recognized.
                               24
            All steps of the recognition process can be implemented efﬁciently, so the total time
          to recognize all objects in Figures 12 or 13 is less than 0.3 seconds on a 2GHz Pentium 4
          processor. We have implemented these algorithms on a laptop computer with attached video
          camera, andhavetestedthemextensivelyoverawiderangeofconditions. Ingeneral,textured
          planar surfaces can be identiﬁed reliably over a rotation in depth of up to 50 degrees in any
          direction and under almost any illumination conditions that provide sufﬁcient light and do not
          produceexcessive glare. For3Dobjects, the range ofrotation indepth forreliable recognition
          is only about 30 degrees inany direction and illumination change ismoredisruptive. Forthese
          reasons, 3D object recognition is best performed by integrating features from multiple views,
          such as with local feature view clustering (Lowe, 2001).
            These keypoints have also been applied to the problem of robot localization and map-
          ping, which has been presented in detail in other papers (Se, Lowe and Little, 2001). In this
          application, a trinocular stereo system is used to determine 3D estimates for keypoint loca-
          tions. Keypoints are used only when they appear in all 3 images with consistent disparities,
          resulting in very few outliers. As the robot moves, it localizes itself using feature matches to
          the existing 3D map, and then incrementally adds features to the map while updating their 3D
          positions using a Kalman ﬁlter. This provides a robust and accurate solution to the problem
          of robot localization in unknown environments. This work has also addressed the problem of
          place recognition, in which a robot can be switched on and recognize its location anywhere
          within a large map (Se, Lowe and Little, 2002), which is equivalent to a 3D implementation
          of object recognition.
          9 Conclusions
          The SIFT keypoints described in this paper are particularly useful due to their distinctive-
          ness, which enables the correct match for a keypoint to be selected from a large database of
          other keypoints. This distinctiveness is achieved by assembling a high-dimensional vector
          representing the image gradients within a local region of the image. The keypoints have been
          shown to be invariant to image rotation and scale and robust across a substantial range of
          afﬁne distortion, addition of noise, and change in illumination. Large numbers of keypoints
          can be extracted from typical images, which leads to robustness in extracting small objects
          amongclutter. Thefactthatkeypoints aredetected over acomplete range of scales means that
          small local features are available for matching small and highly occluded objects, while large
          keypoints perform well for images subject to noise and blur. Their computation is efﬁcient,
          so that several thousand keypoints can be extracted from a typical image with near real-time
          performance on standard PC hardware.
            Thispaperhasalsopresented methodsforusing thekeypoints forobject recognition. The
          approach we have described uses approximate nearest-neighbor lookup, a Hough transform
          for identifying clusters that agree on object pose, least-squares pose determination, and ﬁ-
          nal veriﬁcation. Other potential applications include view matching for 3D reconstruction,
          motion tracking and segmentation, robot localization, image panorama assembly, epipolar
          calibration, and any others that require identiﬁcation of matching locations between images.
            There are many directions for further research in deriving invariant and distinctive image
          features. Systematic testing is needed on data sets with full 3D viewpoint and illumination
          changes. Thefeatures described in this paper use only a monochrome intensity image, so fur-
          ther distinctiveness could be derived from including illumination-invariant color descriptors
                            25
           (Funt and Finlayson, 1995; Brown and Lowe,2002). Similarly, local texture measures appear
           to play an important role in human vision and could be incorporated into feature descriptors
           in a more general form than the single spatial frequency used by the current descriptors. An
           attractive aspect of the invariant local feature approach to matching is that there is no need
           to select just one feature type, and the best results are likely to be obtained by using many
           different features, all of which can contribute useful matches and improve overall robustness.
             Another direction for future research will be to individually learn features that are suited
           to recognizing particular objects categories. This will be particularly important for generic
           object classes that must cover a broad range of possible appearances. The research of We-
           ber, Welling, and Perona (2000) and Fergus, Perona, and Zisserman (2003) has shown the
           potential of this approach by learning small sets of local features that are suited to recogniz-
           ing generic classes of objects. In the long term, feature sets are likely to contain both prior
           and learned features that will be used according to the amount of training data that has been
           available for various object classes.
                          Acknowledgments
           I wouldparticularlyliketo thankMatthewBrown,whohassuggestednumerousimprovementstoboth
           the content and presentation of this paper and whose own work on feature localization and invariance
           has contributed to this approach. In addition, I would like to thank many others for their valuable
           suggestions, including Stephen Se, Jim Little, Krystian Mikolajczyk, Cordelia Schmid, Tony Linde-
           berg, and Andrew Zisserman. This research was supported by the Natural Sciences and Engineering
           Research Council of Canada (NSERC) and throughthe Institute for Robotics and Intelligent Systems
           (IRIS) Network of Centres of Excellence.
           References
           Arya, S., and Mount, D.M. 1993. Approximate nearest neighbor queries in ﬁxed dimensions. In
             Fourth Annual ACM-SIAMSymposiumonDiscreteAlgorithms(SODA’93),pp. 271-280.
           Arya, S., Mount, D.M., Netanyahu, N.S., Silverman, R., and Wu, A.Y. 1998. An optimal algorithm
             for approximatenearest neighbor searching. Journal of the ACM, 45:891-923.
           Ballard, D.H. 1981. Generalizing the Hough transform to detect arbitrary patterns. Pattern Recogni-
             tion, 13(2):111-122.
           Basri, R., and Jacobs, D.W. 1997. Recognition using region correspondences. International Journal
             of Computer Vision, 25(2):145-166.
           Baumberg, A. 2000. Reliable feature matching across widely separated views. In Conference on
             ComputerVision and Pattern Recognition, Hilton Head, South Carolina, pp. 774-781.
           Beis, J. and Lowe, D.G. 1997. Shape indexing using approximate nearest-neighbour search in high-
             dimensional spaces. In Conference on Computer Vision and Pattern Recognition, Puerto Rico,
             pp. 1000-1006.
           Brown, M. and Lowe, D.G. 2002. Invariant features from interest point groups. In British Machine
             Vision Conference, Cardiff, Wales, pp. 656-665.
           Carneiro, G., and Jepson, A.D. 2002. Phase-based local features. In European Conference on Com-
             puter Vision (ECCV), Copenhagen,Denmark, pp. 282-296.
           Crowley, J. L. and Parker, A.C. 1984. A representation for shape based on peaks and ridges in the
             difference of low-pass transform. IEEE Trans. on Pattern Analysis and Machine Intelligence,
             6(2):156-170.
                               26
           Edelman, S., Intrator, N. and Poggio, T. 1997. Complex cells and object recognition. Unpublished
             manuscript: http://kybele.psych.cornell.edu/∼edelman/archive.html
           Fergus, R., Perona, P., and Zisserman, A. 2003. Object class recognition by unsupervised scale-
             invariant learning. In IEEE Conference on Computer Vision and Pattern Recognition, Madison,
             Wisconsin, pp. 264-271.
           Friedman, J.H., Bentley, J.L. and Finkel, R.A. 1977. An algorithm for ﬁnding best matches in loga-
             rithmic expected time. ACM Transactions on Mathematical Software, 3(3):209-226.
           Funt, B.V. and Finlayson, G.D. 1995. Color constant color indexing. IEEE Trans. on Pattern Analysis
             and MachineIntelligence, 17(5):522-529.
           Grimson, E. 1990. Object Recognition by Computer: The Role of Geometric Constraints, The MIT
             Press: Cambridge, MA.
           Harris, C. 1992. Geometry from visual motion. In Active Vision, A. Blake and A. Yuille (Eds.), MIT
             Press, pp. 263-284.
           Harris, C. and Stephens, M. 1988. A combined corner and edge detector. In Fourth Alvey Vision
             Conference,Manchester, UK, pp. 147-151.
           Hartley, R. and Zisserman, A. 2000. Multiple view geometry in computer vision, Cambridge Univer-
             sity Press: Cambridge, UK.
           Hough,P.V.C. 1962. Method and means for recognizingcomplex patterns. U.S. Patent 3069654.
           Koenderink,J.J. 1984. The structure of images. Biological Cybernetics, 50:363-396.
           Lindeberg, T. 1993. Detecting salient blob-like image structures and their scales with a scale-space
             primal sketch: a method for focus-of-attention. International Journal of Computer Vision, 11(3):
             283-318.
           Lindeberg, T. 1994. Scale-space theory: A basic tool for analysing structures at different scales.
             Journal of Applied Statistics, 21(2):224-270.
           Lowe,D.G.1991. Fitting parameterizedthree-dimensionalmodelsto images. IEEE Trans.on Pattern
             Analysis and Machine Intelligence, 13(5):441-450.
           Lowe,D.G.1999. Object recognitionfrom local scale-invariant features. In International Conference
             onComputerVision,Corfu,Greece,pp. 1150-1157.
           Lowe, D.G. 2001. Local feature view clustering for 3D object recognition. IEEE Conference on
             ComputerVision and Pattern Recognition, Kauai, Hawaii, pp. 682-688.
           Luong, Q.T., and Faugeras, O.D. 1996. The fundamental matrix: Theory, algorithms, and stability
             analysis. International Journal of Computer Vision, 17(1):43-76.
           Matas, J., Chum, O., Urban, M., and Pajdla, T. 2002. Robust wide baseline stereo from maximally
             stable extremal regions. In British Machine Vision Conference, Cardiff, Wales, pp. 384-393.
           Mikolajczyk, K. 2002. Detection of local features invariant to afﬁne transformations, Ph.D. thesis,
             Institut National Polytechnique de Grenoble, France.
           Mikolajczyk, K., and Schmid, C. 2002. An afﬁne invariant interest point detector. In European
             Conferenceon ComputerVision (ECCV), Copenhagen,Denmark,pp. 128-142.
           Mikolajczyk, K., Zisserman, A., and Schmid, C. 2003. Shape recognition with edge-based features.
             In Proceedings of the British Machine Vision Conference, Norwich, U.K.
           Moravec, H. 1981. Rover visual obstacle avoidance. In International Joint Conference on Artiﬁcial
             Intelligence, Vancouver, Canada, pp. 785-790.
           Nelson, R.C., and Selinger, A. 1998. Large-scale tests of a keyed, appearance-based 3-D object
             recognition system. Vision Research, 38(15):2469-88.
           Pope, A.R., and Lowe, D.G. 2000. Probabilistic models of appearance for 3-D object recognition.
             International Journal of Computer Vision, 40(2):149-167.
                               27
           Pritchard,D.,andHeidrich,W.2003. Clothmotioncapture. ComputerGraphicsForum(Eurographics
             2003), 22(3):263-271.
           Schaffalitzky, F., and Zisserman, A. 2002. Multi-view matching for unordered image sets, or ‘How
             do I organize my holiday snaps?”’ In European Conference on Computer Vision, Copenhagen,
             Denmark,pp. 414-431.
           Schiele, B., and Crowley, J.L. 2000. Recognition without correspondence using multidimensional
             receptive ﬁeld histograms. International Journal of Computer Vision, 36(1):31-50.
           Schmid,C.,andMohr,R.1997. Localgrayvalueinvariantsforimageretrieval. IEEETrans.onPattern
             Analysis and Machine Intelligence, 19(5):530-534.
           Se, S., Lowe, D.G., and Little, J. 2001. Vision-based mobile robot localization and mapping using
             scale-invariant features. In International Conference on Robotics and Automation, Seoul, Korea,
             pp. 2051-58.
           Se, S., Lowe, D.G., and Little, J. 2002. Global localization using distinctive visual features. In
             InternationalConferenceonIntelligentRobotsandSystems,IROS2002,Lausanne,Switzerland,
             pp. 226-231.
           Shokoufandeh,A.,Marsic,I.,andDickinson,S.J. 1999. View-basedobjectrecognitionusingsaliency
             maps. Imageand Vision Computing,17:445-460.
           Torr, P. 1995. Motion Segmentation and Outlier Detection, Ph.D. Thesis, Dept. of Engineering Sci-
             ence, University of Oxford, UK.
           Tuytelaars, T., and Van Gool, L. 2000. Wide baseline stereo based on local, afﬁnely invariant regions.
             In British Machine Vision Conference, Bristol, UK, pp. 412-422.
           Weber, M., Welling, M. and Perona, P. 2000. Unsupervised learning of models for recognition. In
             EuropeanConferenceonComputerVision,Dublin,Ireland,pp. 18-32.
           Witkin, A.P. 1983. Scale-space ﬁltering. In International Joint Conference on Artiﬁcial Intelligence,
             Karlsruhe, Germany,pp. 1019-1022.
           Zhang,Z., Deriche, R., Faugeras, O., and Luong,Q.T. 1995. A robust techniquefor matchingtwo un-
             calibratedimagesthroughtherecoveryoftheunknownepipolargeometry. ArtiﬁcialIntelligence,
             78:87-119.
                               28
