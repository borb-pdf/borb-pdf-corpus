                     Distributed Representations of Words and Phrases
                                   andtheir Compositionality
                          TomasMikolov           Ilya Sutskever        KaiChen
                            GoogleInc.            GoogleInc.          GoogleInc.
                          MountainView           MountainView        MountainView
                       mikolov@google.com    ilyasu@google.com     kai@google.com
                                  GregCorrado                  Jeffrey Dean
                                   GoogleInc.                  GoogleInc.
                                  MountainView                MountainView
                             gcorrado@google.com            jeff@google.com
                                                Abstract
                         The recently introduced continuous Skip-gram model is an efÔ¨Åcient method for
                         learning high-quality distributed vector representations that capture a large num-
                         ber of precise syntactic and semantic word relationships. In this paper we present
                         several extensions that improve both the quality of the vectors and the training
                         speed. By subsampling of the frequent words we obtain signiÔ¨Åcant speedup and
                         also learn more regular word representations. We also describe a simple alterna-
                         tive to the hierarchical softmax called negative sampling.
                         Aninherent limitation of word representations is their indifference to word order
                         and their inability to represent idiomatic phrases. For example, the meanings of
                         ‚ÄúCanada‚Äùand‚ÄúAir‚Äùcannotbeeasilycombinedtoobtain‚ÄúAirCanada‚Äù. Motivated
                         bythis example,wepresenta simplemethodforÔ¨Åndingphrasesintext,andshow
                         that learning good vector representations for millions of phrases is possible.
                   1 Introduction
                   Distributed representations of words in a vector space help learning algorithms to achieve better
                   performancein naturallanguageprocessingtasks by groupingsimilar words. Oneoftheearliestuse
                   of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea
                   has since been applied to statistical language modeling with considerable success [1]. The follow
                   up work includes applications to automatic speech recognition and machine translation [14, 7], and
                   a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].
                   Recently, Mikolov et al. [8] introduced the Skip-gram model, an efÔ¨Åcient method for learning high-
                   quality vector representations of words from large amounts of unstructured text data. Unlike most
                   of the previously used neural network architectures for learning word vectors, training of the Skip-
                   gram model (see Figure 1) does not involve dense matrix multiplications. This makes the training
                   extremelyefÔ¨Åcient: an optimizedsingle-machineimplementationcantrain on morethan100billion
                   wordsinoneday.
                   The word representations computed using neural networks are very interesting because the learned
                   vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of
                   these patterns can be represented as linear translations. For example, the result of a vector calcula-
                   tion vec(‚ÄúMadrid‚Äù) - vec(‚ÄúSpain‚Äù) + vec(‚ÄúFrance‚Äù) is closer to vec(‚ÄúParis‚Äù) than to any other word
                   vector [9, 8].
                                                    1
                                                           											
						
                                                                              
                                                                              
                                                         
                                                                              
                                                                              
                         Figure 1: The Skip-gram model architecture. The training objective is to learn word vector representations
                         that are good at predicting the nearby words.
                         In this paper we present several extensions of the original Skip-gram model. We show that sub-
                         sampling of frequent words during training results in a signiÔ¨Åcant speedup (around 2x - 10x), and
                         improves accuracy of the representations of less frequent words. In addition, we present a simpli-
                         Ô¨ÅedvariantofNoiseContrastiveEstimation(NCE)[4]fortrainingtheSkip-grammodelthatresults
                         in faster training and better vector representations for frequent words, compared to more complex
                         hierarchical softmax that was used in the prior work [8].
                         Word representations are limited by their inability to represent idiomatic phrases that are not com-
                         positions of the individual words. For example, ‚ÄúBoston Globe‚Äù is a newspaper, and so it is not a
                         natural combination of the meanings of ‚ÄúBoston‚Äù and ‚ÄúGlobe‚Äù. Therefore, using vectors to repre-
                         sent the whole phrases makes the Skip-grammodelconsiderablymoreexpressive. Other techniques
                         that aim to represent meaning of sentences by composing the word vectors, such as the recursive
                         autoencoders[15], would also beneÔ¨Åt from using phrase vectors instead of the word vectors.
                         Theextensionfromwordbasedtophrasebasedmodelsisrelativelysimple. Firstweidentifyalarge
                         numberofphrases using a data-driven approach, and then we treat the phrases as individual tokens
                         during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-
                         cal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is
                         ‚ÄúMontreal‚Äù:‚ÄúMontrealCanadiens‚Äù::‚ÄúToronto‚Äù:‚ÄúTorontoMapleLeafs‚Äù. It is consideredto havebeen
                         answered correctly if the nearest representation to vec(‚ÄúMontreal Canadiens‚Äù) - vec(‚ÄúMontreal‚Äù) +
                         vec(‚ÄúToronto‚Äù)is vec(‚ÄúTorontoMaple Leafs‚Äù).
                         Finally, we describe another interesting property of the Skip-gram model. We found that simple
                         vector addition can often produce meaningful results. For example, vec(‚ÄúRussia‚Äù) + vec(‚Äúriver‚Äù) is
                         close to vec(‚ÄúVolga River‚Äù), and vec(‚ÄúGermany‚Äù) + vec(‚Äúcapital‚Äù) is close to vec(‚ÄúBerlin‚Äù). This
                         compositionality suggests that a non-obvious degree of language understanding can be obtained by
                         using basic mathematical operations on the word vector representations.
                         2 TheSkip-gramModel
                         The training objective of the Skip-gram model is to Ô¨Ånd word representations that are useful for
                         predicting the surrounding words in a sentence or a document. More formally, given a sequence of
                         trainingwordsw ;w ;w ;:::;w ,theobjectiveoftheSkip-grammodelistomaximizetheaverage
                         log probability 1  2   3      T
                                                           T
                                                        1 X      X logp(w |w)                                    (1)
                                                       T                       t+j  t
                                                          t=1 ‚àíc‚â§j‚â§c;j6=0
                         where c is the size of the training context (which can be a function of the center word w ). Larger
                                                                                                           t
                         c results in more training examples and thus can lead to a higher accuracy, at the expense of the
                                                                      2
                             training time. The basic Skip-gram formulation deÔ¨Ånes p(w          |w ) using the softmax function:
                                                                                             t+j   t
                                                                                      ‚Ä≤   ‚ä§     
                                                                                exp v        v
                                                                                              w
                                                                                        wO     I
                                                              p(w |w ) =                                                          (2)
                                                                   O I
                                                                             PW exp v‚Ä≤ ‚ä§v
                                                                                                 w
                                                                                w=1         w      I
                             where v     and v‚Ä≤ are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations of w, and W is the num-
                                      w        w
                             ber of words in the vocabulary. This formulation is impractical because the cost of computing
                             ‚àálogp(w |w )isproportionalto W, whichis oftenlarge(105‚Äì107 terms).
                                        O I
                             2.1   HierarchicalSoftmax
                             Acomputationally efÔ¨Åcient approximation of the full softmax is the hierarchical softmax. In the
                             context of neural network language models, it was Ô¨Årst introduced by Morin and Bengio [12]. The
                             main advantage is that instead of evaluating W output nodes in the neural network to obtain the
                             probability distribution, it is needed to evaluate only about log2(W) nodes.
                             The hierarchical softmax uses a binary tree representation of the output layer with the W words as
                             its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These
                             deÔ¨Ånearandomwalkthatassignsprobabilitiesto words.
                             More precisely, each word w can be reached by an appropriate path from the root of the tree. Let
                             n(w;j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so
                             n(w;1) = root and n(w;L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary
                             Ô¨Åxedchild of n and let [[x]] be 1 if x is true and -1 otherwise. Then the hierarchical softmax deÔ¨Ånes
                             p(w |w )asfollows:
                                  O I
                                                           L(w)‚àí1
                                                             Y                                         ‚Ä≤      ‚ä§    
                                              p(w|w ) =            œÉ [[n(w;j +1) = ch(n(w;j))]] ¬∑ v             v                   (3)
                                                     I                                                  n(w;j)    wI
                                                             j=1
                                                                                         P
                             where œÉ(x) = 1=(1+exp(‚àíx)). It can be veriÔ¨Åed that             W p(w|w ) = 1. Thisimplies that the
                                                                                            w=1         I
                             cost of computing logp(w |w ) and ‚àálogp(w |w ) is proportional to L(w ), which on average
                                                         O I                    O I                             O
                             is no greater than logW. Also, unlike the standard softmax formulation of the Skip-gram which
                             assigns two representations v      and v‚Ä≤ to each word w, the hierarchical softmax formulation has
                                                             w        w
                             one representation v     for each word w and one representation v‚Ä≤ for every inner node n of the
                             binary tree.          w                                                n
                             The structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-
                             mance. Mnih and Hinton explored a number of methods for constructing the tree structure and the
                             effect on both the training time and the resulting model accuracy [10]. In our work we use a binary
                             Huffman tree, as it assigns short codes to the frequent words which results in fast training. It has
                             been observed before that grouping words together by their frequency works well as a very simple
                             speeduptechniquefor the neural network based language models [5, 8].
                             2.2   NegativeSampling
                             An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-
                             troduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].
                             NCEposits that a good model should be able to differentiate data from noise by means of logistic
                             regression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models
                             byrankingthe data above noise.
                             While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-
                             gram model is only concerned with learning high-quality vector representations, so we are free to
                             simplify NCEaslongasthevectorrepresentationsretaintheirquality. We deÔ¨ÅneNegativesampling
                             (NEG)bytheobjective
                                                                           k
                                                            ‚Ä≤  ‚ä§         X                h          ‚Ä≤  ‚ä§     i
                                                    logœÉ(v       v   ) +      E            logœÉ(‚àív       v   )                      (4)
                                                            w     wI           w ‚àºP (w)              w     wI
                                                              O                  i   n                 i
                                                                          i=1
                                                                                 3
                                                          Country and Capital Vectors Projected by PCA
                                         2
                                                                   China
                                                                                                               Beijing
                                        1.5                   Russia
                                                              Japan
                                         1                                                                          Moscow
                                                              Turkey                                          Ankara    Tokyo
                                        0.5
                                                         Poland
                                         0           Germany
                                                    France                                                     Warsaw
                                                                                                                  Berlin
                                       -0.5     Italy                                                          Paris
                                                       Greece                                                  Athens
                                        -1     Spain                                                        Rome
                                       -1.5  Portugal                                                        Madrid
                                                                                                         Lisbon
                                        -2-2        -1.5        -1        -0.5         0         0.5        1          1.5        2
                              Figure 2: Two-dimensional PCAprojection of the 1000-dimensional Skip-gram vectors of countries and their
                              capital cities. The Ô¨Ågure illustrates ability of the model to automatically organize concepts and learn implicitly
                              the relationships between them, as during the training we did not provide any supervised information about
                              what a capital city means.
                              which is used to replace every logP(w |w ) term in the Skip-gram objective. Thus the task is to
                                                                          O I
                              distinguish the target word w      from draws from the noise distribution P (w) using logistic regres-
                                                              O                                               n
                              sion, where there are k negative samples for each data sample. Our experimentsindicate that values
                              of k in the range 5‚Äì20 are useful for small training datasets, while for large datasets the k can be as
                              small as 2‚Äì5. The main difference between the Negative sampling and NCE is that NCE needs both
                              samplesandthenumericalprobabilitiesofthenoisedistribution,while Negativesamplingusesonly
                              samples. AndwhileNCEapproximatelymaximizesthelogprobabilityofthesoftmax,thisproperty
                              is not important for our application.
                              BothNCEandNEGhavethenoisedistributionP (w)asafreeparameter. Weinvestigatedanumber
                                                                                   n
                              of choices for P (w) and found that the unigram distribution U(w) raised to the 3=4rd power (i.e.,
                                                n
                                    3=4
                              U(w)      =Z) outperformed signiÔ¨Åcantly the unigram and the uniform distributions, for both NCE
                              and NEGoneverytaskwetriedincludinglanguagemodeling(notreportedhere).
                              2.3   Subsampling of Frequent Words
                              In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,
                              ‚Äúin‚Äù, ‚Äúthe‚Äù, and ‚Äúa‚Äù). Such words usually provide less information value than the rare words. For
                              example, while the Skip-gram model beneÔ¨Åts from observing the co-occurrences of ‚ÄúFrance‚Äù and
                              ‚ÄúParis‚Äù, it beneÔ¨Åts much less from observing the frequent co-occurrences of ‚ÄúFrance‚Äù and ‚Äúthe‚Äù, as
                              nearly every word co-occurs frequently within a sentence with ‚Äúthe‚Äù. This idea can also be applied
                              in the opposite direction; the vector representations of frequent words do not change signiÔ¨Åcantly
                              after training on several million examples.
                              To counter the imbalance between the rare and frequent words, we used a simple subsampling ap-
                              proach: each word w in the training set is discarded with probability computed by the formula
                                                     i
                                                                        P(w ) = 1‚àís t                                                   (5)
                                                                             i            f(w )
                                                                                               i
                                                                                    4
                                            Method       Time[min]      Syntactic [%]   Semantic [%]    Total accuracy [%]
                                             NEG-5            38             63              54                 59
                                            NEG-15            97             63              58                 61
                                          HS-Huffman          41             53              40                 47
                                             NCE-5            38             60              45                 53
                                                                                        ‚àí5
                                                             Thefollowing results use 10   subsampling
                                             NEG-5            14             61              58                 60
                                            NEG-15            36             61              61                 61
                                          HS-Huffman          21             52              59                 55
                             Table 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task
                             as deÔ¨Åned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive
                             sample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical
                             Softmaxwith the frequency-basedHuffmancodes.
                             where f(w ) is the frequency of word w and t is a chosen threshold, typically around 10‚àí5.
                                         i                                 i
                             We chose this subsampling formula because it aggressively subsamples words whose frequency
                             is greater than t while preserving the ranking of the frequencies. Although this subsampling for-
                             mulawaschosenheuristically,we foundit to work well in practice. It accelerates learning and even
                             signiÔ¨Åcantly improves the accuracy of the learned vectors of the rare words, as will be shown in the
                             following sections.
                             3 Empirical Results
                             In this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative
                             Sampling,andsubsamplingofthetrainingwords. Weusedtheanalogicalreasoningtask1 introduced
                             by Mikolov et al. [8]. The task consists of analogies such as ‚ÄúGermany‚Äù : ‚ÄúBerlin‚Äù :: ‚ÄúFrance‚Äù : ?,
                             whicharesolvedbyÔ¨Åndingavectorxsuchthatvec(x)isclosesttovec(‚ÄúBerlin‚Äù)-vec(‚ÄúGermany‚Äù)
                             +vec(‚ÄúFrance‚Äù)accordingto the cosine distance (we discard the input words from the search). This
                             speciÔ¨Åc example is considered to have been answered correctly if x is ‚ÄúParis‚Äù. The task has two
                             broad categories: the syntactic analogies (such as ‚Äúquick‚Äù : ‚Äúquickly‚Äù :: ‚Äúslow‚Äù : ‚Äúslowly‚Äù) and the
                             semantic analogies, such as the country to capital city relationship.
                             For training the Skip-gram models, we have used a large dataset consisting of various news articles
                             (an internal Google dataset with one billion words). We discarded from the vocabulary all words
                             that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.
                             The performance of various Skip-gram models on the word analogy test set is reported in Table 1.
                             The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical
                             reasoningtask, and has even slightly better performancethan the Noise Contrastive Estimation. The
                             subsampling of the frequent words improves the training speed several times and makes the word
                             representations signiÔ¨Åcantly more accurate.
                             It can be argued that the linearity of the skip-gram model makes its vectors more suitable for such
                             linear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned
                             by the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this
                             task signiÔ¨Åcantlyas the amountofthetrainingdataincreases,suggestingthat non-linearmodelsalso
                             have a preference for a linear structure of the word representations.
                             4 LearningPhrases
                             As discussed earlier, many phrases have a meaning that is not a simple composition of the mean-
                             ings of its individual words. To learn vector representation for phrases, we Ô¨Årst Ô¨Ånd words that
                             appearfrequentlytogether,and infrequentlyin other contexts. For example, ‚ÄúNew York Times‚Äù and
                             ‚ÄúToronto Maple Leafs‚Äù are replaced by unique tokens in the training data, while a bigram ‚Äúthis is‚Äù
                             will remain unchanged.
                                 1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt
                                                                                 5
                                                                  Newspapers
                                      NewYork            NewYorkTimes          Baltimore       Baltimore Sun
                                       SanJose        SanJose Mercury News     Cincinnati   Cincinnati Enquirer
                                                                  NHLTeams
                                        Boston            Boston Bruins         Montreal    Montreal Canadiens
                                       Phoenix           Phoenix Coyotes       Nashville    Nashville Predators
                                                                  NBATeams
                                        Detroit           Detroit Pistons       Toronto       Toronto Raptors
                                       Oakland         Golden State Warriors   Memphis       MemphisGrizzlies
                                                                    Airlines
                                       Austria           Austrian Airlines       Spain           Spainair
                                       Belgium           Brussels Airlines      Greece        Aegean Airlines
                                                               Companyexecutives
                                     Steve Ballmer          Microsoft          Larry Page         Google
                                  Samuel J. Palmisano         IBM            WernerVogels        Amazon
                          Table 2: Examplesof the analogical reasoning task for phrases (the full test set has 3218 examples).
                          Thegoalis to computethe fourth phrase using the Ô¨Årst three. Our best model achieved an accuracy
                          of 72%onthisdataset.
                          This way, we can form many reasonable phrases without greatly increasing the size of the vocabu-
                          lary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory
                          intensive. Many techniqueshave been previouslydevelopedto identifyphrases in the text; however,
                          it is out of scope of our work to compare them. We decided to use a simple data-driven approach,
                          wherephrasesare formed based on the unigramand bigram counts, using
                                                                       count(w w )‚àíŒ¥
                                                    score(w ;w ) =             i j        :                        (6)
                                                            i  j    count(w ) √ócount(w )
                                                                            i           j
                          The Œ¥ is used as a discounting coefÔ¨Åcient and prevents too many phrases consisting of very infre-
                          quent words to be formed. The bigrams with score above the chosen threshold are then used as
                          phrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-
                          ing longer phrases that consists of several words to be formed. We evaluate the quality of the phrase
                          representationsusinganewanalogicalreasoningtaskthatinvolvesphrases. Table2showsexamples
                          of the Ô¨Åve categories of analogies used in this task. This dataset is publicly available on the web2.
                          4.1  PhraseSkip-GramResults
                          Starting with the same news data as in the previous experiments, we Ô¨Årst constructed the phrase
                          based training corpus and then we trained several Skip-gram models using different hyper-
                          parameters. As before, we used vector dimensionality 300 and context size 5. This setting already
                          achieves good performance on the phrase dataset, and allowed us to quickly compare the Negative
                          Samplingand the Hierarchical Softmax, both with and without subsampling of the frequent tokens.
                          Theresults are summarized in Table 3.
                          The results show that while Negative Sampling achieves a respectable accuracy even with k = 5,
                          using k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-
                          chical Softmax to achieve lower performancewhen trained without subsampling,it became the best
                          performing method when we downsampled the frequent words. This shows that the subsampling
                          can result in faster training and can also improve accuracy, at least in some cases.
                            2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt
                                                                                         ‚àí5
                                        Method     Dimensionality  Nosubsampling [%]   10  subsampling [%]
                                        NEG-5           300               24                   27
                                       NEG-15           300               27                   42
                                     HS-Huffman         300               19                   47
                          Table 3: Accuracies of the Skip-gram models on the phrase analogy dataset. The models were
                          trained on approximately one billion words from the news dataset.
                                                                       6
                                                                             ‚àí5                            ‚àí5
                                                             NEG-15with10       subsampling     HSwith10      subsampling
                                           Vasco de Gama                Lingsugur                     Italian explorer
                                             Lake Baikal             Great Rift Valley                   Aral Sea
                                             Alan Bean               Rebbeca Naomi                     moonwalker
                                             Ionian Sea                  Ruegen                       Ionian Islands
                                            chess master            chess grandmaster                Garry Kasparov
                                Table 4: Examples of the closest entities to the given short phrases, using two different models.
                               Czech+currency      Vietnam + capital      German+airlines       Russian + river     French + actress
                                    koruna               Hanoi            airline Lufthansa        Moscow           Juliette Binoche
                                 Checkcrown        HoChiMinhCity          carrier Lufthansa      Volga River        Vanessa Paradis
                                  Polish zolty         Viet Nam         Ô¨Çagcarrier Lufthansa        upriver       Charlotte Gainsbourg
                                     CTK              Vietnamese             Lufthansa              Russia             Cecile De
                             Table5: Vectorcompositionalityusingelement-wiseaddition. Fourclosesttokensto thesumoftwo
                             vectors are shown, using the best Skip-gram model.
                             To maximize the accuracy on the phrase analogy task, we increased the amount of the training data
                             by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality
                             of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy
                             of 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B
                             words, which suggests that the large amount of the training data is crucial.
                             Togainfurtherinsightinto howdifferentthe representationslearned by differentmodelsare, we did
                             inspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we
                             show a sample of such comparison. Consistently with the previous results, it seems that the best
                             representations of phrases are learned by a model with the hierarchical softmax and subsampling.
                             5 Additive Compositionality
                             Wedemonstratedthat the word and phrase representations learned by the Skip-gram model exhibit
                             a linear structure that makes it possible to perform precise analogical reasoning using simple vector
                             arithmetics. Interestingly, we foundthat the Skip-gramrepresentationsexhibitanotherkindoflinear
                             structure that makes it possible to meaningfullycombine words by an element-wiseaddition of their
                             vector representations. This phenomenonis illustrated in Table 5.
                             Theadditivepropertyofthevectorscan beexplainedby inspectingthe training objective. The word
                             vectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors
                             are trained to predict the surrounding words in the sentence, the vectors can be seen as representing
                             the distribution of the context in which a word appears. These values are related logarithmically
                             to the probabilities computed by the output layer, so the sum of two word vectors is related to the
                             product of the two context distributions. The product works here as the AND function: words that
                             are assigned high probabilities by both word vectors will have high probability, and the other words
                             will have low probability. Thus, if ‚ÄúVolga River‚Äù appears frequently in the same sentence together
                             with the words ‚ÄúRussian‚Äù and ‚Äúriver‚Äù, the sum of these two word vectorswill result in such a feature
                             vector that is close to the vector of ‚ÄúVolga River‚Äù.
                             6 ComparisontoPublishedWordRepresentations
                             Many authors who previously worked on the neural network based representations of words have
                             published their resulting models for further use and comparison: amongst the most well known au-
                             thors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded
                             their word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-
                             tions on the word analogy task, where the Skip-gram models achieved the best performance with a
                             hugemargin.
                                 3http://metaoptimize.com/projects/wordreprs/
                                                                                  7
                                            Model                 Redmond                       Havel                  ninjutsu           grafÔ¨Åti        capitulate
                                       (training time)
                                       Collobert (50d)             conyers                     plauen                    reiki          cheesecake        abdicate
                                         (2 months)                lubbock                   dzerzhinsky               kohona             gossip          accede
                                                                    keene                     osterreich                karate           dioramas          rearm
                                        Turian (200d)             McCarthy                      Jewell                     -              gunÔ¨Åre              -
                                         (few weeks)                Alston                      Arzu                       -             emotion              -
                                                                   Cousins                      Ovitz                      -             impunity             -
                                        Mnih(100d)                 Podhurst                    Pontiff                     -           anaesthetics      Mavericks
                                           (7 days)                Harlang                    Pinochet                     -             monkeys         planning
                                                                   Agarwal                    Rodionov                     -               Jews          hesitated
                                         Skip-Phrase           RedmondWash.                 Vaclav Havel                 ninja          spray paint     capitulation
                                       (1000d, 1 day)       RedmondWashington          president Vaclav Havel        martial arts         graÔ¨Åtti       capitulated
                                                                  Microsoft               Velvet Revolution        swordsmanship          taggers       capitulating
                                    Table 6: Examplesof the closest tokens given various well known models and the Skip-grammodel
                                    trained on phrases using over 30 billion training words. An empty cell means that the word was not
                                    in the vocabulary.
                                    To give more insight into the difference of the quality of the learned vectors, we provide empirical
                                    comparisonbyshowingthenearestneighboursofinfrequentwordsinTable6. Theseexamplesshow
                                    that the big Skip-gram model trained on a large corpus visibly outperforms all the other models in
                                    the quality of the learned representations. This can be attributed in part to the fact that this model
                                    has been trained on about 30 billion words, which is about two to three orders of magnitude more
                                    data than the typical size used in the prior work. Interestingly, although the training set is much
                                    larger, the training time of the Skip-gram model is just a fraction of the time complexity required by
                                    the previous model architectures.
                                    7 Conclusion
                                    This work has several key contributions. We show how to train distributed representations of words
                                    and phrases with the Skip-gram model and demonstrate that these representations exhibit linear
                                    structure that makes precise analogical reasoning possible. The techniques introduced in this paper
                                    can be used also for training the continuous bag-of-words model introduced in [8].
                                    Wesuccessfully trained models on several orders of magnitude more data than the previously pub-
                                    lished models, thanks to the computationally efÔ¨Åcient model architecture. This results in a great
                                    improvement in the quality of the learned word and phrase representations, especially for the rare
                                    entities. We also found that the subsampling of the frequent words results in both faster training
                                    and signiÔ¨Åcantly better representations of uncommon words. Another contribution of our paper is
                                    the Negative sampling algorithm,which is an extremely simple training method that learns accurate
                                    representations especially for frequent words.
                                    The choice of the training algorithm and the hyper-parameter selection is a task speciÔ¨Åc decision,
                                    as we found that different problems have different optimal hyperparameter conÔ¨Ågurations. In our
                                    experiments, the most crucial decisions that affect the performance are the choice of the model
                                    architecture, the size of the vectors, the subsampling rate, and the size of the training window.
                                    Averyinteresting result of this work is that the word vectors can be somewhat meaningfully com-
                                    bined using just simple vector addition. Another approach for learning representations of phrases
                                    presented in this paper is to simply represent the phrases with a single token. Combination of these
                                    two approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-
                                    ingminimalcomputationalcomplexity. Ourworkcanthusbeseenascomplementarytotheexisting
                                    approachthat attempts to represent phrases using recursive matrix-vector operations [16].
                                    Wemadethecodefortrainingthewordandphrasevectorsbasedonthetechniquesdescribedinthis
                                                                                          4
                                    paper available as an open-source project .
                                        4code.google.com/p/word2vec
                                                                                                     8
             References
             [1] Yoshua Bengio, Re¬¥jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language
               model. The Journal of Machine Learning Research, 3:1137‚Äì1155, 2003.
             [2] Ronan Collobert and Jason Weston. A uniÔ¨Åed architecture for natural language processing: deep neu-
               ral networks with multitask learning. In Proceedings of the 25th international conference on Machine
               learning, pages 160‚Äì167. ACM, 2008.
             [3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-
               Ô¨Åcation: A deep learning approach. In ICML, 513‚Äì520, 2011.
             [4] Michael U Gutmann and Aapo Hyva¬®rinen. Noise-contrastive estimation of unnormalized statistical mod-
               els, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307‚Äì361,
               2012.
             [5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of
               recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011
               IEEEInternational Conference on, pages 5528‚Äì5531. IEEE, 2011.
             [6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training
               LargeScaleNeural NetworkLanguage Models. InProc.AutomaticSpeech Recognition and Understand-
               ing, 2011.
             [7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno
               University of Technology, 2012.
             [8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations
               in vector space. ICLR Workshop, 2013.
             [9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word
               Representations. In Proceedings of NAACL HLT, 2013.
             [10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in
               neural information processing systems, 21:1081‚Äì1088, 2009.
             [11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language
               models. arXiv preprint arXiv:1206.6426, 2012.
             [12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-
               ceedings of the international workshop on artiÔ¨Åcial intelligence and statistics, pages 246‚Äì252, 2005.
             [13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-
               propagating errors. Nature, 323(6088):533‚Äì536, 1986.
             [14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.
             [15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and
               natural language with recursive neural networks. In Proceedings of the 26th International Conference on
               Machine Learning (ICML), volume 2, 2011.
             [16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality
               Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods
               in Natural Language Processing (EMNLP), 2012.
             [17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for
               semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-
               tional Linguistics, pages 384‚Äì394. Association for Computational Linguistics, 2010.
             [18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In
               Journal of ArtiÔ¨Åcial Intelligence Research, 37:141-188, 2010.
             [19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.
               In Transactions of the Association for Computational Linguistics (TACL), 353‚Äì366, 2013.
             [20] Jason Weston, SamyBengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-
               tion. In Proceedings of the Twenty-Second international joint conference on ArtiÔ¨Åcial Intelligence-Volume
               Volume Three, pages 2764‚Äì2770. AAAI Press, 2011.
                                   9
