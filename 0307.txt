                                                     PyramidSceneParsingNetwork
                                                1                    2                    1                        1               1
                         Hengshuang Zhao             Jianping Shi         Xiaojuan Qi          Xiaogang Wang            Jiaya Jia
                                   1The Chinese University of Hong Kong                 2SenseTime Group Limited
                 {hszhao, xjqi, leojia}@cse.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk, shijianping@sensetime.com
                                       Abstract
                 Scene parsing is challenging for unrestricted open vo-
              cabulary and diverse scenes. In this paper, we exploit the
              capability of global context information by different-region-
              based context aggregation through our pyramid pooling
              module together with the proposed pyramid scene parsing
              network (PSPNet). Our global prior representation is ef-
              fective to produce good quality results on the scene parsing
              task, whilePSPNetprovidesasuperiorframeworkforpixel-
              level prediction. The proposed approach achieves state-of-
              the-art performanceonvariousdatasets. ItcameﬁrstinIm-
              ageNet scene parsing challenge 2016, PASCAL VOC 2012
              benchmark and Cityscapes benchmark. A single PSPNet
              yields the new record of mIoU accuracy 85.4% on PASCAL
              VOC2012andaccuracy80.2%onCityscapes.
                                                                                  Figure 1. Illustration of complex scenes in ADE20K dataset.
              1. Introduction
                 Sceneparsing, basedonsemanticsegmentation,isafun-                lenges considering diverse scenes and unrestricted vocabu-
              damental topic in computer vision. The goal is to assign            lary. One example is shown in the ﬁrst row of Fig. 2, where
              each pixel in the image a category label. Scene parsing pro-        a boat is mistaken as a car. These errors are due to similar
              vides complete understanding of the scene. It predicts the          appearance of objects. But when viewing the image regard-
              label, location, as well as shape for each element. This topic      ingthecontextpriorthatthesceneisdescribedasboathouse
              is of broad interest for potential applications of automatic        near a river, correct prediction should be yielded.
              driving, robot sensing, to name a few.                                 Towardsaccuratesceneperception,theknowledgegraph
                 Difﬁculty of scene parsing is closely related to scene and       relies on prior information of scene context.      We found
              label variety. The pioneer scene parsing task [23] is to clas-      that the major issue for current FCN based models is lack
              sify 33 scenes for 2,688 images on LMO dataset [22]. More           of suitable strategy to utilize global scene category clues.
              recent PASCALVOCsemanticsegmentationandPASCAL                       For typical complex scene understanding, previously to get
                                                                                  a global image-level feature, spatial pyramid pooling [
              context datasets [8, 29] include more labels with similar                                                                     18]
              context, such as chair and sofa, horse and cow, etc. The            waswidelyemployedwherespatialstatisticsprovideagood
              newADE20Kdataset[43]isthemostchallengingonewith                     descriptor for overall scene interpretation. Spatial pyramid
              a large and unrestricted open vocabulary and more scene             pooling network [12] further enhances the ability.
              classes. A few representative images are shown in Fig. 1.              Different from these methods, to incorporate suitable
              To develop an effective algorithm for these datasets needs          global features, we propose pyramid scene parsing network
              to conquer a few difﬁculties.                                       (PSPNet). In addition to traditional dilated FCN [3, 40] for
                 State-of-the-art scene parsing frameworks are mostly             pixel prediction, we extend the pixel-level feature to the
              based on the fully convolutional network (FCN) [26]. The            specially designed global pyramid pooling one. The local
              deep convolutional neural network (CNN) based methods               and global clues together make the ﬁnal prediction more
              boost dynamic object understanding, and yet still face chal-        reliable.  We also propose an optimization strategy with
                                                                              1
                                                                               2881
             deeply supervised loss. We give all implementation details,      under object detection frameworks [35]. Liu et al. [24]
             which are key to our decent performance in this paper, and       proved that global average pooling with FCN can improve
             makethecodeandtrained models publicly available 1.               semantic segmentation results. However, our experiments
                Our approach achieves state-of-the-art performance on         show that these global descriptors are not representative
             all available datasets. It is the champion of ImageNet scene     enough for the challenging ADE20K data. Therefore, dif-
             parsing challenge 2016 [43], and arrived the 1st place on        ferent from global pooling in [24], we exploit the capabil-
             PASCALVOC2012semanticsegmentationbenchmark[8],                   ity of global context information by different-region-based
             and the 1st place on urban scene Cityscapes data [6]. They       context aggregation via our pyramid scene parsing network.
             manifest that PSPNet gives a promising direction for pixel-
             level prediction tasks, which may even beneﬁt CNN-based          3. Pyramid Scene Parsing Network
             stereo matching, optical ﬂow, depth estimation, etc.     in
             follow-up work. Our main contributions are threefold.               Westart with our observation and analysis of represen-
                                                                              tative failure cases when applying FCN methods to scene
                • We propose a pyramid scene parsing network to em-           parsing. They motivate proposal of our pyramid pooling
                  bed difﬁcult scenery context features in an FCN based       module as the effective global context prior. Our pyramid
                  pixel prediction framework.                                 scene parsing network (PSPNet) illustrated in Fig. 3 is then
                • Wedevelopaneffectiveoptimization strategy for deep          described to improve performance for open-vocabulary ob-
                  ResNet [13] based on deeply supervised loss.                ject and stuff identiﬁcation in complex scene parsing.
                • We build a practical system for state-of-the-art scene      3.1. Important Observations
                  parsing and semantic segmentation where all crucial
                  implementation details are included.                           ThenewADE20Kdataset[43]contains150stuff/object
                                                                              category labels (e.g., wall, sky, and tree) and 1,038 image-
             2. Related Work                                                  level scene descriptors (e.g., airport terminal, bedroom, and
                                                                              street). So a large amount of labels and vast distributions
                In the following, we review recent advances in scene          of scenes come into existence. Inspecting the prediction
             parsing and semantic segmentation tasks. Driven by pow-          results of the FCN baseline provided in [43], we summarize
             erful deep neural networks [17, 33, 34, 13], pixel-level         several common issues for complex-scene parsing.
             prediction tasks like scene parsing and semantic segmen-
             tation achieve great progress inspired by replacing the          Mismatched Relationship       Context relationship is uni-
             fully-connected layer in classiﬁcation with the convolution      versal and important especially for complex scene under-
             layer [26]. To enlarge the receptive ﬁeld of neural networks,    standing. There exist co-occurrent visual patterns. For ex-
             methodsof[3,40]useddilatedconvolution. Noh et al. [30]           ample, an airplane is likely to be in runway or ﬂy in sky
             proposed a coarse-to-ﬁne structure with deconvolution net-       while not over a road. For the ﬁrst-row example in Fig. 2,
             worktolearnthesegmentationmask. Ourbaselinenetwork               FCNpredictstheboatintheyellowboxasa“car”basedon
             is FCN and dilated network [26, 3].                              its appearance. But the common knowledge is that a car is
                Other work mainly proceeds in two directions.       One       seldomoverariver. Lack of the ability to collect contextual
             line [26, 3, 5, 39, 11] is with multi-scale feature ensembling.  information increases the chance of misclassiﬁcation.
             Since in deep networks, higher-layer feature contains more
             semantic meaning and less location information. Combin-          Confusion Categories     There are many class label pairs
             ing multi-scale features can improve the performance.            in the ADE20K dataset [43] that are confusing in classiﬁ-
                Theother direction is based on structure prediction. The      cation. Examples are ﬁeld and earth; mountain and hill;
             pioneer work [3] used conditional random ﬁeld (CRF) as           wall, house, building and skyscraper. They are with simi-
             post processing to reﬁne the segmentation result. Following      lar appearance. The expert annotator who labeled the entire
             methods[25,41,1]reﬁnednetworksviaend-to-endmodel-                dataset, still makes 17.60% pixel error as described in [43].
             ing. Both of the two directions ameliorate the localization      In the second row of Fig. 2, FCN predicts the object in the
             ability of scene parsing where predicted semantic boundary       box as part of skyscraper and part of building. These re-
             ﬁts objects. Yet there is still much room to exploit necessary   sults should be excluded so that the whole object is either
             information in complex scenes.                                   skyscraper or building, but not both. This problem can be
                To make good use of global image-level priors for di-         remedied by utilizing the relationship between categories.
             verse scene understanding, methods of [18, 27] extracted         Inconspicuous Classes     Scene contains objects/stuff of
             global context information with traditional features not         arbitrary size. Several small-size things, like streetlight and
             fromdeepneuralnetworks. Similarimprovementwasmade                signboard, are hard to ﬁnd while they may be of great im-
                1https://github.com/hszhao/PSPNet                             portance. Contrarily, big objects or stuff may exceed the
                                                                           2882
             Figure 2. Scene parsing issues we observe on ADE20K [43] dataset. The ﬁrst row shows the issue of mismatched relationship – cars are
             seldom over water than boats. The second row shows confusion categories where class “building” is easily confused as “skyscraper”. The
             third row illustrates inconspicuous classes. In this example, the pillow is very similar to the bed sheet in terms of color and texture. These
             inconspicuous objects are easily misclassiﬁed by FCN.
             receptive ﬁeld of FCN and thus cause discontinuous pre-         the momentous global scenery prior. We address this issue
             diction. As shown in the third row of Fig. 2, the pillow        byproposing an effective global prior representation.
             has similar appearance with the sheet.    Overlooking the          Global average pooling is a good baseline model as the
             global scene category may fail to parse the pillow. To im-      global contextual prior, which is commonly used in image
             prove performance for remarkably small or large objects,        classiﬁcation tasks [34, 13]. In [24], it was successfully ap-
             one should pay much attention to different sub-regions that     plied to semantic segmentation. But regarding the complex-
             contain inconspicuous-category stuff.                           sceneimagesinADE20K[43],thisstrategyisnotenoughto
                To summarize these observations, many errors are par-        cover necessary information. Pixels in these scene images
             tially or completely related to contextual relationship and     are annotated regarding many stuff and objects. Directly
             global information for different receptive ﬁelds.  Thus a       fusingthemtoformasinglevectormaylosethespatialrela-
             deep network with a suitable global-scene-level prior can       tion and cause ambiguity. Global context information along
             muchimprovetheperformanceofsceneparsing.                        with sub-region context is helpful in this regard to distin-
                                                                             guishamongvariouscategories. Amorepowerfulrepresen-
             3.2. Pyramid Pooling Module                                     tation couldbefusedinformationfromdifferentsub-regions
                With above analysis, in what follows, we introduce the       withthesereceptiveﬁelds. Similarconclusionwasdrawnin
             pyramidpoolingmodule,whichempiricallyprovestobean               classical work [18, 12] of scene/image classiﬁcation.
             effective global contextual prior.                                 In [12], feature maps in different levels generated by
                In a deep neural network, the size of receptive ﬁeld can     pyramid pooling were ﬁnally ﬂattened and concatenated to
             roughly indicates how much we use context information.          be fed into a fully connected layer for classiﬁcation. This
             Although theoretically the receptive ﬁeld of ResNet [13] is     global prior is designed to remove the ﬁxed-size constraint
             already larger than the input image, it is shown by Zhou et     of CNN for image classiﬁcation. To further reduce context
             al. [42] that the empirical receptive ﬁeld of CNN is much       information loss between different sub-regions, we propose
             smallerthanthetheoreticaloneespeciallyonhigh-levellay-          a hierarchical global prior, containing information with dif-
             ers. This makes many networks not sufﬁciently incorporate       ferent scales and varying among different sub-regions. We
                                                                          2883
              Figure 3. Overview of our proposed PSPNet. Given an input image (a), we ﬁrst use CNN to get the feature map of the last convolutional
              layer (b), then a pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatena-
              tion layers to form the ﬁnal feature representation, which carries both local and global context information in (c). Finally, the representation
              is fed into a convolution layer to get the ﬁnal per-pixel prediction (d).
              call it pyramid pooling module for global scene prior con-
              struction upon the ﬁnal-layer-feature-map of the deep neu-
              ral network, as illustrated in part (c) of Fig. 3.
                 The pyramid pooling module fuses features under four
              different pyramid scales. The coarsest level highlighted in
              red is global pooling to generate a single bin output. The
              following pyramid level separates the feature map into dif-
              ferent sub-regions and forms pooled representation for dif-
              ferent locations. The output of different levels in the pyra-      Figure 4. Illustration of auxiliary loss in ResNet101. Each blue
              mid pooling module contains the feature map with varied            box denotes a residue block. The auxiliary loss is added after the
                                                                                 res4b22 residue block.
              sizes. To maintain the weight of global feature, we use 1×1
              convolution layer after each pyramid level to reduce the di-
              mension of context representation to 1=N of the original           map, we use the pyramid pooling module shown in (c) to
              one if the level size of pyramid is N. Then we directly up-        gather context information. Using our 4-level pyramid, the
              samplethelow-dimensionfeaturemapstogetthesamesize                  pooling kernels cover the whole, half of, and small portions
              feature as the original feature map via bilinear interpolation.    of the image. They are fused as the global prior. Then we
              Finally, different levels of features are concatenated as the      concatenate the prior with the original feature map in the
              ﬁnal pyramid pooling global feature.                               ﬁnal part of (c). It is followed by a convolution layer to
                 Notedthatthenumberofpyramidlevelsandsizeofeach                  generate the ﬁnal prediction map in (d).
              level can be modiﬁed. They are related to the size of feature         To explain our structure, PSPNet provides an effective
              map that is fed into the pyramid pooling layer. The struc-         global contextual prior for pixel-level scene parsing. The
              ture abstracts different sub-regions by adopting varying-size      pyramid pooling module can collect levels of information,
              pooling kernels in a few strides. Thus the multi-stage ker-        more representative than global pooling [24]. In terms of
              nels should maintain a reasonable gap in representation.           computational cost, our PSPNet does not much increase it
              Our pyramid pooling module is a four-level one with bin            compared to the original dilated FCN network. In end-to-
              sizes of 1×1, 2×2, 3×3and6×6respectively. Forthetype               end learning, the global pyramid pooling module and the
              of pooling operation between max and average, we perform           local FCN feature can be optimized simultaneously.
              extensive experiments to show the difference in Section 5.2.
              3.3. Network Architecture                                          4. Deep Supervision for ResNet-Based FCN
                 Withthepyramidpoolingmodule,weproposeourpyra-                      Deep pretrained networks lead to good performance
              midsceneparsingnetwork(PSPNet)asillustratedinFig.3.                [17, 33, 13].   However, increasing depth of the network
              Given an input image in Fig. 3(a), we use a pretrained             may introduce additional optimization difﬁculty as shown
              ResNet[13]modelwiththedilatednetworkstrategy[3,40]                 in [32, 19] for image classiﬁcation. ResNet solves this prob-
              to extract the feature map. The ﬁnal feature map size is 1=8       lem with skip connection in each block. Latter layers of
              of the input image, as shown in Fig. 3(b). On top of the           deep ResNet mainly learn residues based on previous ones.
                                                                              2884
                 Wecontrarilyproposegeneratinginitialresultsbysuper-                Method                      MeanIoU(%)     Pixel Acc.(%)
              vision with an additional loss, and learning the residue af-          ResNet50-Baseline               37.23         78.01
              terwards with the ﬁnal loss. Thus, optimization of the deep           ResNet50+B1+MAX                 39.94         79.46
                                                                                    ResNet50+B1+AVE                 40.07         79.52
              network is decomposed into two, each is simpler to solve.             ResNet50+B1236+MAX              40.18         79.45
                 An example of our deeply supervised ResNet101 [13]                 ResNet50+B1236+AVE              41.07         79.97
              model is illustrated in Fig. 4. Apart from the main branch            ResNet50+B1236+MAX+DR           40.87         79.61
              using softmax loss to train the ﬁnal classiﬁer, another clas-         ResNet50+B1236+AVE+DR           41.68         80.04
              siﬁer is applied after the fourth stage, i.e., the res4b22         Table 1. Investigation of PSPNet with different settings. Baseline
              residue block. Different from relay backpropagation [32]           is ResNet50-based FCN with dilated network. ‘B1’ and ‘B1236’
              that blocks the backward auxiliary loss to several shallow         denote pooled feature maps of bin sizes {1 × 1} and {1 × 1;2 ×
              layers, we let the two loss functions pass through all pre-        2;3 × 3;6 × 6} respectively. ‘MAX’ and ‘AVE’ represent max
              vious layers. The auxiliary loss helps optimize the learning       pooling and average pooling operations individually. ‘DR’ means
              process, while the master branch loss takes the most respon-       that dimension reduction is taken after pooling. The results are
              sibility. We add weight to balance the auxiliary loss.             tested on the validation set with the single-scale input.
                 In the testing phase, we abandon this auxiliary branch
              andonlyusethewelloptimizedmasterbranchforﬁnalpre-                  branch [4] and make it support batch normalization on data
              diction. This kind of deeply supervised training strategy          gathered from multiple GPUs based on OpenMPI. For the
              for ResNet-based FCN is broadly useful under different ex-         auxiliary loss, we set the weight to 0.4 in experiments.
              perimental settings and works with the pre-trained ResNet
              model. This manifests the generality of such a learning            5.2. ImageNet Scene Parsing Challenge 2016
              strategy. More details are provided in Section 5.2.                Dataset and Evaluation Metrics        The ADE20K dataset
              5. Experiments                                                     [43]isusedinImageNetsceneparsingchallenge2016. Dif-
                                                                                 ferent from other datasets, ADE20K is more challenging
                 Our proposed method is successful on scene parsing              for the up to 150 classes and diverse scenes with a total
              and semantic segmentation challenges. We evaluate it in            of 1,038 image-level labels. The challenge data is divided
              this section on three different datasets, including ImageNet       into 20K/2K/3K images for training, validation and testing.
              scene parsing challenge 2016 [43], PASCAL VOC 2012                 Also, it needs to parse both objects and stuff in the scene,
              semantic segmentation [8] and urban scene understanding            which makes it more difﬁcult than other datasets. For eval-
              dataset Cityscapes [6].                                            uation, both pixel-wise accuracy (Pixel Acc.) and mean of
                                                                                 class-wise intersection over union (Mean IoU) are used.
              5.1. Implementation Details
                                                                                 AblationStudyforPSPNet ToevaluatePSPNet,wecon-
                 For a practical deep learning system, devil is always in        duct experiments with several settings, including pooling
              the details. Our implementation is based on the public plat-       types of max and average, pooling with just one global fea-
              formCaffe[15]. Inspired by [4], we use the “poly” learning         ture or four-level features, with and without dimension re-
              rate policy wherecurrentlearningrateequalstothebaseone             duction after the pooling operation and before concatena-
              multiplying (1 −    iter  )power. We set base learning rate
                                maxiter                                          tion. As listed in Table 1, average pooling works better than
              to 0.01 and power to 0.9. The performance can be improved          max pooling in all settings. Pooling with pyramid parsing
              byincreasing the iteration number, which is set to 150K for        outperforms that using global pooling. With dimension re-
              ImageNet experiment, 30K for PASCAL VOC and 90K for                duction, the performance is further enhanced. With our pro-
              Cityscapes. Momentumandweightdecayaresetto0.9and                   posed PSPNet, the best setting yields results 41.68/80.04 in
              0.0001 respectively. For data augmentation, we adopt ran-          terms of Mean IoU and Pixel Acc. (%), exceeding global
              dom mirror and random resize between 0.5 and 2 for all             average pooling of 40.07/79.52 as idea in Liu et al. [
                                                                                                                                       24] by
              datasets, and additionally add random rotation between -           1.61/0.52. And compared to the baseline, PSPNet outper-
              10and10degrees,andrandomGaussianblurforImageNet                    forming it by 4.45/2.03 in terms of absolute improvement
              andPASCALVOC.Thiscomprehensivedataaugmentation                     and 11.95/2.60 in terms of relative difference.
              scheme makes the network resist overﬁtting. Our network
              contains dilated convolution following [4].                        Ablation Study for Auxiliary Loss       The introduced aux-
                 During the course of experiments, we notice that an ap-         iliary loss helps optimize the learning process while not in-
              propriately large “cropsize” can yield good performance            ﬂuencing learning in the master branch. We experiment
              and “batchsize” in the batch normalization [14] layer is           withsetting the auxiliary loss weight α between 0 and 1 and
              of great importance. Due to limited physical memory on             show the results in Table 2. The baseline uses ResNet50-
              GPU cards, we set the “batchsize” to 16 during training.           based FCN with dilated network, with the master branch’s
              To achieve this, we modify Caffe from [37] together with           softmax loss for optimization. Adding the auxiliary loss
                                                                              2885
                    Loss Weight α            MeanIoU(%)      Pixel Acc.(%)               Method                         MeanIoU(%)     Pixel Acc.(%)
                    ResNet50 (without AL)        35.82           77.07                   FCN[26]                            29.39          71.32
                    ResNet50 (with α = 0.3)      37.01           77.87                   SegNet [2]                         21.64          71.00
                    ResNet50 (with α = 0.4)      37.23           78.01                   DilatedNet [40]                    32.31          73.55
                    ResNet50 (with α = 0.6)      37.09           77.84                   CascadeNet [43]                    34.90          74.52
                    ResNet50 (with α = 0.9)      36.99           77.87                   ResNet50-Baseline                  34.28          76.35
                                                                                         ResNet50+DA                        35.82          77.07
              Table 2. Setting an appropriate loss weight α in the auxiliary             ResNet50+DA+AL                     37.23          78.01
              branch is important. ‘AL’ denotes the auxiliary loss. Baseline is          ResNet50+DA+AL+PSP                 41.68          80.04
              ResNet50-based FCN with dilated network. Empirically, α = 0.4              ResNet269+DA+AL+PSP                43.81          80.88
              yields the best performance. The results are tested on the valida-         ResNet269+DA+AL+PSP+MS             44.94          81.69
              tion set with the single-scale input.                                   Table 4. Detailed analysis of our proposed PSPNet with compar-
                                                                                      ison with others. Our results are obtained on the validation set
                                                                                      withthesingle-scale input except for the last row. Results of FCN,
                                                                                      SegNet and DilatedNet are reported in [43]. ‘DA’ refers to data
                                                                                      augmentation we performed, ‘AL’ denotes the auxiliary loss we
                                                                                      added and ‘PSP’ represents the proposed PSPNet. ‘MS’ means
                                                                                      that multi-scale testing is used.
                                                                                               Rank    TeamName                 Final Score (%)
                                                                                                 1     Ours                         57.21
                                                                                                 2     Adelaide                     56.74
                                                                                                 3     360+MCG-ICT-CAS SP            55.56
                                                                                                 -     (our single model)           (55.38)
                                                                                                 4     SegModel                     54.65
              Figure 5. Performance grows with deeper networks. The results                      5     CASIA IVA                    54.33
              are obtained on the validation set with the single-scale input.                    -     DilatedNet [40]               45.67
                                                                                                 -     FCN[26]                       44.80
                                                                                                 -     SegNet [2]                    40.79
                       Method              MeanIoU(%)     Pixel Acc.(%)
                       PSPNet(50)             41.68           80.04                   Table 5. Results of ImageNet scene parsing challenge 2016. The
                       PSPNet(101)            41.96           80.64                   best entry of each team is listed. The ﬁnal score is the mean of
                       PSPNet(152)            42.62           80.80                   MeanIoUandPixelAcc. Results are evaluated on the testing set.
                       PSPNet(269)            43.81           80.88
                       PSPNet(50)+MS          42.78           80.76
                       PSPNet(101)+MS         43.29           81.39                   More Detailed Performance Analysis               We show our
                       PSPNet(152)+MS         43.51           81.38
                       PSPNet(269)+MS         44.94           81.69                   more detailed analysis on the validation set of ADE20K in
                                                                                      Table 4. All our results except the last-row one use single-
              Table 3. Deeper pre-trained model get higher performance. Num-          scale test.   “ResNet269+DA+AL+PSP+MS” uses multi-
              ber in the brackets refers to the depth of ResNet and ‘MS’ denotes      scale testing. Our baseline is adapted from ResNet50 with
              multi-scale testing.                                                    dilated network, which yields MeanIoU 34.28 and Pixel
                                                                                      Acc. 76.35. It already outperforms other prior systems pos-
              branch, α = 0.4 yields the best performance. It outperforms             sibly due to the powerful ResNet [13].
              the baseline with an improvement of 1.41/0.94 in terms of                  Our proposed architecture makes further improvement
              MeanIoUandPixelAcc. (%). Webelievedeepernetworks                        compared to the baseline.          Using data augmentation,
              will beneﬁt more given the new augmented auxiliary loss.                our result exceeds the baseline by 1.54/0.72 and reaches
                                                                                      35.82/77.07. Using the auxiliary loss can further improve
                                                                                      it by 1.41/0.94 and reaches 37.23/78.01. With PSPNet, we
              Ablation Study for Pre-trained Model              Deeper neural         notice relatively more signiﬁcant progress for improvement
              networkshavebeenshowninpreviousworktobebeneﬁcial                        of 4.45/2.03. The result reaches 41.68/80.04. The differ-
              tolargescaledataclassiﬁcation. TofurtheranalyzePSPNet,                  ence from the baseline result is 7.40/3.69 in terms of abso-
              we conduct experiments for different depths of pre-trained              lute improvement and 21.59/4.83 (%) in terms of relativity.
              ResNet. We test four depths of {50, 101, 152, 269}. As                  Adeeper network of ResNet269 yields even higher perfor-
              showninFig. 5, with the same setting, increasing the depth              mance up to 43.81/80.88. Finally, the multi-scale testing
              of ResNet from 50 to 269 can improve the score of (Mean                 schememovesthescoresto44.94/81.69.
              IoU+PixelAcc.) /2(%)from60.86to62.35,with1.49ab-
              solute improvement. Detailed scores of PSPNet pre-trained               Results in Challenge        Using the proposed architecture,
              from different depth ResNet models are listed in Table 3.               our team came in the 1st place in ImageNet scene parsing
                                                                                   2886
                                                                                 Figure7.VisualimprovementsonPASCALVOC2012data. PSP-
                                                                                 Net produces more accurate and detailed results.
              Figure 6. Visual improvements on ADE20K, PSPNet produces              As shown in Table 6, PSPNet outperforms prior meth-
              moreaccurate and detailed results.                                 ods on both settings. Trained with only VOC 2012 data, we
                                                                                                          2
                                                                                 achieve 82.6% accuracy – we get the highest accuracy on
                                                                                 all 20 classes. When PSPNet is pre-trained with MS-COCO
              challenge 2016. Table 5 shows a few results in this com-           dataset, it reaches 85.4% accuracy3 where 19 out of the 20
              petition. Our ensemble submission achieves score 57.21%            classes receive the highest accuracy. Intriguingly, our PSP-
              on the testing set. Our single-model yields score 55.38%,          Net trained with only VOC 2012 data outperforms existing
              which is even higher than a few other multi-model ensem-           methods trained with the MS-COCO pre-trained model.
              ble submissions. This score is lower than that on the valida-         One may argue that our based classiﬁcation model is
              tion set possibly due to the difference of data distributions      more powerful than several prior methods since ResNet
              between validation and testing sets. As shown in column            was recently proposed.     To exhibit our unique contribu-
              (d) of Fig. 2, PSPNet solves the common problems in FCN.           tion, we show that our method also outperforms state-
              Fig. 6 shows another few parsing results on validation set of      of-the-art frameworks that use the same model, including
              ADE20K. Our results contain more accurate and detailed             FCRNs [38], LRR [9], and DeepLab [4]. In this process,
              structures compared to the baseline.                               weeven do not employ time-consuming but effective post-
              5.3. PASCALVOC2012                                                 processing, such as CRF, as that in [4, 9].
                                                                                    Several examples are shown in Fig. 7. For “cows” in row
                 Our PSPNet also works satisfyingly on semantic seg-             one, our baseline model treats it as “horse” and “dog” while
              mentation. We carry out experiments on the PASCAL VOC              PSPNet corrects these errors. For “aeroplane” and “table”
              2012 segmentation dataset [8], which contains 20 object            in the second and third rows, PSPNet ﬁnds missing parts.
              categories and one background class. Following the proce-          For “person”, “bottle” and “plant” in following rows, PSP-
              dureof[26,7,31,3],weuseaugmenteddatawiththeanno-                   Net performs well on these small-size-object classes in the
              tation of [10] resulting 10,582, 1,449 and 1,456 images for        images compared to the baseline model. More visual com-
              training, validation and testing. Results are shown in Ta-         parisons between PSPNet and other methods are included
              ble 6, we compare PSPNet with previous best-performing             in our project website.
              methods on the testing set based on two settings, i.e., with
              or without pre-training on MS-COCO dataset [21]. Meth-             5.4. Cityscapes
              ods pre-trained with MS-COCO are marked by ‘†’. For fair              Cityscapes [6] is a recently released dataset for semantic
              comparison with current ResNet based frameworks [38, 9,            urban scene understanding. It contains 5,000 high quality
              4] in scene parsing/semantic segmentation task, we build           pixel-level ﬁnely annotated images collected from 50 cities
              our architecture based on ResNet101 while without post-
              processing like CRF. We evaluate PSPNet with several-                 2http://host.robots.ox.ac.uk:8080/anonymous/0OOWLP.html
              scale input and use the average results following [3, 24].            3http://host.robots.ox.ac.uk:8080/anonymous/6KIR41.html
                                                                              2887
                 Method             aero bike bird boat bottle bus          car   cat   chair  cow table dog horse mbike person plant sheep sofa train                tv    mIoU
                 FCN[26]            76.8 34.2 68.9 49.4       60.3   75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9              76.5     73.9   45.2    72.4  37.4 70.9 55.1      62.2
                 Zoom-out [28]      85.6 37.3 83.2 62.5       66.0   85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2              81.1     77.1   53.6    74.0  49.2 71.7 63.3      69.6
                 DeepLab[3]         84.4 54.5 81.5 63.6       65.9   85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1              83.2     80.8   59.7    82.2  50.4 73.1 63.7      71.6
                 CRF-RNN[41]        87.5 39.0 79.7 64.2       68.3   87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8              83.1     80.6   59.5    82.8  47.8 78.3 67.1      72.0
                 DeconvNet[30]      89.9 39.3 79.7 63.9       68.2   87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3              83.6     80.2   58.8    83.4  54.3 80.7 65.0      72.5
                 GCRF[36]           85.2 43.9 83.3 65.2       68.3   89.0 82.7 85.3 31.1 79.5 63.3 80.5 79.3              85.5     81.0   60.5    85.5  52.0 77.3 65.1      73.2
                 DPN[25]            87.7 59.4 78.4 64.9       70.3   89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0              83.5     82.3   60.5    83.2  53.4 77.9 65.0      74.1
                 Piecewise [20]     90.6 37.6 80.0 67.8       74.4   92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9              84.3     84.8   62.1    83.2  58.2 80.8 72.3      75.3
                 PSPNet             91.8 71.9 94.7 71.2       75.8   95.2 89.9 95.9 39.3 90.7 71.7 90.5 94.5              88.8     89.6   72.8    89.6  64.0 85.1 76.3      82.6
                 CRF-RNN† [41] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9                           86.4     81.8   58.6    82.4  53.5 77.4 70.1      74.7
                         †
                 BoxSup [7]         89.8 38.0 89.2 68.9       68.0   89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7              85.2     83.5   58.6    84.9  55.8 81.2 70.7      75.2
                           †
                 Dilation8 [40]     91.7 39.6 87.8 63.1       71.8   89.7 82.9 89.8 37.2 84.0 63.0 83.3 89.0              83.8     85.1   56.8    87.6  56.0 80.2 64.7      75.3
                      †
                 DPN [25]           89.0 61.6 87.7 66.8       74.7   91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8              85.6     85.4   63.6    87.3  61.3 79.4 66.4      77.5
                 Piecewise† [20]    94.1 40.7 84.1 67.8       75.9   93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0              85.8     86.0   67.5    90.2  63.8 80.9 73.0      78.0
                         †
                 FCRNs [38]         91.9 48.1 93.4 69.3       75.5   94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2              86.5     87.2   64.6    90.1  59.7 85.5 72.7      79.1
                      †
                 LRR [9]            92.4 45.1 94.6 65.2       75.8   95.1 89.1 92.3 39.0 85.7 70.4 88.6 89.4              88.6     86.6   65.8    86.2  57.4 85.7 77.3      79.3
                 DeepLab† [4]       92.6 60.4 91.6 63.4       76.3   95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1              87.0     87.4   63.3    88.3  60.0 86.8 74.5      79.7
                         †
                 PSPNet             95.8 72.7 95.0 78.9       84.4   94.7 92.0 95.7 43.1 91.0 80.3 91.3 96.3              92.3     90.1   71.5    94.4  66.9 88.8 82.0      85.4
                            Table 6. Per-class results on PASCAL VOC 2012 testing set. Methods pre-trained on MS-COCO are marked with ‘†’.
                     Method              IoUcla.     iIoU cla.   IoUcat.     iIoU cat.
                     CRF-RNN[41]           62.5        34.4        82.7        66.0
                     FCN[26]               65.3        41.7        85.7        70.1
                     SiCNN [16]            66.3        44.9        85.0        71.2
                     DPN[25]               66.8        39.1        86.0        69.1
                     Dilation10 [40]       67.1        42.0        86.5        71.1
                     LRR[9]                69.7        48.0        88.2        74.7
                     DeepLab[4]            70.4        42.6        86.4        67.7
                     Piecewise [20]        71.6        51.7        87.3        74.1
                     PSPNet                78.4        56.7        90.6        78.6
                          ‡
                     LRR [9]               71.8        47.9        88.4        73.9
                             ‡
                     PSPNet                80.2        58.1        90.6        78.2
                Table 7. Results on Cityscapes testing set. Methods trained using
                both ﬁne and coarse data are marked with ‘‡’.
                in different seasons. The images are divided into sets with
                numbers 2,975, 500, and 1,525 for training, validation and
                testing. It deﬁnes 19 categories containing both stuff and
                objects. Also, 20,000 coarsely annotated images are pro-
                videdfortwosettingsincomparison,i.e., training with only                          Figure 8. Examples of PSPNet results on Cityscapes dataset.
                ﬁne data or with both the ﬁne and coarse data. Methods
                trained using both ﬁne and coarse data are marked with ‘‡’.
                Detailed results are listed in Table 7. Our base model is                         midpoolingfeatureprovidesadditionalcontextualinforma-
                ResNet101 as in DeepLab [4] for fair comparison and the                           tion. We have also provided a deeply supervised optimiza-
                testing procedure follows Section 5.3.                                            tion strategy for ResNet-based FCN network. We hope the
                    Statistics in Table 7 show that PSPNet outperforms other                      implementation details publicly available can help the com-
                methodswithnotableadvantage. Usingbothﬁneandcoarse                                munity adopt these useful strategies for scene parsing and
                data for training makes our method yield 80.2 accuracy.                           semantic segmentation and advance related techniques.
                Several examples are shown in Fig. 8. Detailed per-class
                results on testing set are shown in our project website.                          Acknowledgements
                6. Concluding Remarks                                                                 We would like to thank Gang Sun and Tong Xiao for
                                                                                                  their help in training the basic classiﬁcation models. This
                    We have proposed an effective pyramid scene parsing                           work is supported by a grant from the Research Grants
                networkforcomplexsceneunderstanding. Theglobalpyra-                               Council of the Hong Kong SAR (project No. 2150760).
                                                                                              2888
               References                                                                   [18] S. Lazebnik, C. Schmid, and J. Ponce.           Beyond bags of
                                                                                                  features: Spatial pyramid matching for recognizing natural
                 [1] A.Arnab,S.Jayasumana,S.Zheng,andP.H.S.Torr. Higher                           scene categories. In CVPR, 2006. 1, 2, 3
                     order conditional random ﬁelds in deep neural networks. In             [19] C.Lee,S.Xie,P.W.Gallagher,Z.Zhang,andZ.Tu. Deeply-
                     ECCV,2016. 2                                                                 supervised nets. In AISTATS, 2015. 4
                 [2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A               [20] G.Lin, C. Shen, I. D. Reid, and A. van den Hengel. Efﬁcient
                     deep convolutional encoder-decoder architecture for image                    piecewise training of deep structured models for semantic
                     segmentation. arXiv:1511.00561, 2015. 6                                      segmentation. In CVPR, 2016. 8
                 [3] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.              [21] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
                     Yuille.   Semantic image segmentation with deep convolu-                                     ´
                                                                                                  manan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com-
                     tional nets and fully connected crfs. arXiv:1412.7062, 2014.                 monobjects in context. In ECCV, 2014. 7
                     1, 2, 4, 7, 8                                                          [22] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene pars-
                 [4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.                    ing: Label transfer via dense scene alignment. In CVPR,
                     Yuille. Deeplab: Semantic image segmentation with deep                       2009. 1
                     convolutional nets, atrous convolution, and fully connected            [23] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene pars-
                     crfs. arXiv:1606.00915, 2016. 5, 7, 8                                        ing via label transfer. TPAMI, 2011. 1
                 [5] L. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. Atten-             [24] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking
                     tion to scale: Scale-aware semantic image segmentation. In                   wider to see better. arXiv:1506.04579, 2015. 2, 3, 4, 5, 7
                     CVPR,2016. 2                                                           [25] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic im-
                 [6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,                     age segmentation via deep parsing network. In ICCV, 2015.
                     R. Benenson, U. Franke, S. Roth, and B. Schiele.             The             2, 8
                     cityscapes dataset for semantic urban scene understanding.             [26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
                     In CVPR, 2016. 2, 5, 7                                                       networks for semantic segmentation. In CVPR, 2015. 1, 2,
                 [7] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding                       6, 7, 8
                     boxes to supervise convolutional networks for semantic seg-            [27] A.Lucchi,Y.Li,X.B.Bosch,K.Smith,andP.Fua. Arespa-
                     mentation. In ICCV, 2015. 7, 8                                               tial and global constraints really necessary for segmentation?
                                                                                                  In ICCV, 2011. 2
                 [8] M.Everingham,L.J.V.Gool,C.K.I.Williams,J.M.Winn,                       [28] M.Mostajabi,P.Yadollahpour,andG.Shakhnarovich. Feed-
                     and A. Zisserman. The pascal visual object classes VOC                       forward semantic segmentation with zoom-out features. In
                     challenge. IJCV, 2010. 1, 2, 5, 7                                            CVPR,2015. 8
                 [9] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-             [29] R. Mottaghi, X. Chen, X. Liu, N. Cho, S. Lee, S. Fidler,
                     tion and reﬁnement for semantic segmentation. In ECCV,                       R. Urtasun, and A. L. Yuille. The role of context for object
                     2016. 7, 8                                                                   detection and semantic segmentation in the wild. In CVPR,
               [10] B.Hariharan,P.Arbelaez,L.D.Bourdev,S.Maji,andJ.Ma-                            2014. 1
                     lik.  Semantic contours from inverse detectors. In ICCV,               [30] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
                     2011. 7                                                                      workfor semantic segmentation. In ICCV, 2015. 2, 8
                                                 ´                                          [31] G. Papandreou, L. Chen, K. P. Murphy, and A. L. Yuille.
               [11] B. Hariharan, P. A. Arbelaez, R. B. Girshick, and J. Malik.
                     Hypercolumns for object segmentation and ﬁne-grained lo-                     Weakly-and semi-supervised learning of a deep convolu-
                     calization. In CVPR, 2015. 2                                                 tional network for semantic image segmentation. In ICCV,
               [12] K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpooling                             2015. 7
                     in deep convolutional networks for visual recognition. In              [32] L. Shen, Z. Lin, and Q. Huang. Relay backpropagation for
                     ECCV,2014. 1, 3                                                              effective learning of deep convolutional neural networks. In
               [13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning                   ECCV,2016. 4, 5
                     for image recognition. In CVPR, 2016. 2, 3, 4, 5, 6                    [33] K. Simonyan and A. Zisserman.                  Very deep con-
                                                                                                  volutional networks for large-scale image recognition.
               [14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating                    arXiv:1409.1556, 2014. 2, 4
                     deepnetworktrainingbyreducinginternalcovariateshift. In                [34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
                     ICML,2015. 5                                                                 D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
               [15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.                  Goingdeeper with convolutions. In CVPR, 2015. 2, 3
                     Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-               [35] C. Szegedy, S. E. Reed, D. Erhan, and D. Anguelov. Scal-
                     tional architecture for fast feature embedding. In ACM MM,                   able, high-quality object detection. arXiv:1412.1441, 2014.
                     2014. 5                                                                      2
               [16] I. Kreso, D. Causevic, J. Krapac, and S. Segvic. Convolu-               [36] R. Vemulapalli, O. Tuzel, M. Liu, and R. Chellappa. Gaus-
                     tional scale invariance for semantic segmentation. In GCPR,                  sian conditional random ﬁeld network for semantic segmen-
                     2016. 8                                                                      tation. In CVPR, 2016. 8
               [17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet                 [37] L. Wang, Y. Xiong, Z. Wang, and Y. Qiao.                      To-
                     classiﬁcation with deep convolutional neural networks. In                    wards good practices for very deep two-stream convnets.
                     NIPS, 2012. 2, 4                                                             arXiv:1507.02159, 2015. 5
                                                                                         2889
            [38] Z. Wu, C. Shen, and A. van den Hengel.   Bridging
                category-level and instance-level semantic image segmenta-
                tion. arXiv:1605.06885, 2016. 7, 8
            [39] F. Xia, P. Wang, L. Chen, and A. L. Yuille. Zoom better
                to see clearer: Human and object parsing with hierarchical
                auto-zoom net. In ECCV, 2016. 2
            [40] F. Yu and V. Koltun. Multi-scale context aggregation by di-
                lated convolutions. arXiv:1511.07122, 2015. 1, 2, 4, 6, 8
            [41] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
                Z. Su, D. Du, C. Huang, and P. H. S. Torr. Conditional ran-
                domﬁelds as recurrent neural networks. In ICCV, 2015. 2,
                8
                                   `
            [42] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
                ralba.  Object detectors emerge in deep scene cnns.
                arXiv:1412.6856, 2014. 3
            [43] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and
                A. Torralba. Semantic understanding of scenes through the
                ADE20Kdataset. arXiv:1608.05442, 2016. 1, 2, 3, 5, 6
                                                                   2890
