                IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                     1
                              The Graph Neural Network Model
                                 Franco Scarselli, Marco Gori, Ah Chung Tsoi,     Gabriele Monfardini
                                                                Abstract
                        Many underlying relationships among data in several areas of science and engineering, e.g. computer vision,
                     molecular chemistry, molecular biology, pattern recognition, data mining, can be represented in terms of graphs. In this
                     paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural
                     network methods for processing the data represented in the graph domain. This GNN model, which can directly process
                     most of the practically useful types of graphs, e.g. acyclic, cyclic, directed, un-directed, implements a transduction
                     function τ(G;n) ∈ IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A
                     supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. Computational cost
                     of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning
                     algorithm, and demonstrate its generalization capability.
                                                              Index Terms
                        Graph Neural Networks, Graph Processing, Recursive Neural Networks, Graphical Domains.
                                                           I. INTRODUCTION
                  Data can be naturally represented by graph structures in several application areas including proteomics [1], pattern
                recognition and image analysis [2], scene description [3], [4], software engineering [5], [6] and natural language
                processing [7]. The simplest kinds of graph structures include single nodes, and sequences. But in several application
                domains, the information is organized in more complex graph structures such as trees, acyclic graphs, or cyclic
                graphs. Traditionally, the exploitation of data relationships has been the subject of many studies in the community
                of inductive logic programming and, recently, this research theme has been evolving in different directions [8], also
                because of the marriage with statistics and neural networks (see e.g. the recent workshops [9], [10], [11], [12]).
                  In machine learning, the structured data is often associated with the goal of (supervised or unsupervised) learning
                from examples a function τ that maps a graph G and one of its nodes n to a vector of reals1: τ(G;n) ∈ IRm.
                  Applications to a graphical domain can generally be divided into two classes: graph focused and node focused
                applications respectively in this paper.
                  Scarselli, Gori, Monfardini are with the University of Siena, Siena, Italy. Email: {franco,marco,monfardini}@dii.unisi.it.
                  Tsoi is with Hong Kong Baptist University, Kowloon, Hong Kong. Email: act@hkbu.edu.hk
                  Hagenbuchner is with University of Wollongong, Wollongong, Australia. Email: markus@uow.edu.au
                  1Note that in most classiﬁcation problems, the mapping is to a set of integers INm, while in regression problems, the mapping is to a set of
                reals IRm. Here for simplicity of exposition, we will denote only the regression case. The proposed formulation can be trivially re-written for
                the situation of classiﬁcation.
                May 24, 2007                                                                                      DRAFT
