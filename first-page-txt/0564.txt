                                    LanguageModelsareUnsupervisedMultitaskLearners
                        Alec Radford *1 Jeffrey Wu *1 Rewon Child1 David Luan1 Dario Amodei **1 Ilya Sutskever **1
                                          Abstract                                   competentgeneralists. Wewouldliketomovetowardsmore
                     Natural language processing tasks, such as ques-                general systems which can perform many tasks – eventually
                     tion answering, machine translation, reading com-               without the need to manually create and label a training
                     prehension, and summarization, are typically                    dataset for each one.
                     approached with supervised learning on task-                    The dominant approach to creating ML systems is to col-
                     speciﬁc datasets. We demonstrate that language                  lect a dataset of training examples demonstrating correct
                     models begin to learn these tasks without any ex-               behavior for a desired task, train a system to imitate these
                     plicit supervision when trained on a new dataset                behaviors, and then test its performance on independent
                     of millions of webpages called WebText. When                    and identically distributed (IID) held-out examples. This
                     conditioned on a document plus questions, the an-               has served well to make progress on narrow experts. But
                     swers generated by the language model reach 55                  the often erratic behavior of captioning models (Lake et al.,
                     F1ontheCoQAdataset-matchingorexceeding                          2017), reading comprehension systems (Jia & Liang, 2017),
                     the performance of 3 out of 4 baseline systems                  and image classiﬁers (Alcorn et al., 2018) on the diversity
                     without using the 127,000+ training examples.                   and variety of possible inputs highlights some of the short-
                     The capacity of the language model is essential                 comings of this approach.
                     to the success of zero-shot task transfer and in-               Oursuspicion is that the prevalence of single task training
                     creasing it improves performance in a log-linear                on single domain datasets is a major contributor to the lack
                     fashion across tasks. Our largest model, GPT-2,                 of generalization observed in current systems. Progress
                     is a 1.5B parameter Transformer that achieves                   towards robust systems with current architectures is likely
                     state of the art results on 7 out of 8 tested lan-              to require training and measuring performance on a wide
                     guage modeling datasets in a zero-shot setting                  range of domains and tasks. Recently, several benchmarks
                     but still underﬁts WebText. Samples from the                    have been proposed such as GLUE (Wang et al., 2018) and
                     model reﬂect these improvements and contain co-                 decaNLP(McCannetal.,2018)tobeginstudying this.
                     herent paragraphs of text. These ﬁndings suggest
                     a promising path towards building language pro-                 Multitask learning (Caruana, 1997) is a promising frame-
                     cessingsystemswhichlearntoperformtasksfrom                      work for improving general performance. However, mul-
                     their naturally occurring demonstrations.                       titask training in NLP is still nascent. Recent work re-
                                                                                     ports modest performance improvements (Yogatama et al.,
                                                                                     2019) and the two most ambitious efforts to date have
                                                                                     trainedonatotalof10and17(dataset, objective)
                1. Introduction                                                      pairs respectively (McCann et al., 2018) (Bowman et al.,
                                                                                     2018). Fromameta-learningperspective, each (dataset,
                Machine learning systems now excel (in expectation) at               objective) pair is a single training example sampled
                tasks they are trained for by using a combination of large           from the distribution of datasets and objectives. Current
                datasets, high-capacity models, and supervised learning              MLsystems need hundreds to thousands of examples to
                (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei           induce functions which generalize well. This suggests that
                et al., 2016). Yet these systems are brittle and sensitive to        multitask training many need just as many effective training
                slight changes in the data distribution (Recht et al., 2018)         pairs to realize its promise with current approaches. It will
                andtaskspeciﬁcation (Kirkpatrick et al., 2017). Current sys-         be very difﬁcult to continue to scale the creation of datasets
                tems are better characterized as narrow experts rather than          and the design of objectives to the degree that may be re-
                  *, **Equal contribution   1OpenAI, San Francisco, Califor-         quired to brute force our way there with current techniques.
                nia,  United States.     Correspondence to:       Alec Radford       This motivates exploring additional setups for performing
                <alec@openai.com>.                                                   multitask learning.
                                                                                     The current best performing systems on language tasks
