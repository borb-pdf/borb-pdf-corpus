                           Journal of Machine Learning Research 3 (2003) 993-1022                                          Submitted 2/02; Published 1/03
                                                                  Latent Dirichlet Allocation
                           DavidM.Blei                                                                                   BLEI@CS.BERKELEY.EDU
                           Computer Science Division
                           University of California
                           Berkeley, CA 94720, USA
                           AndrewY.Ng                                                                                    ANG@CS.STANFORD.EDU
                           Computer Science Department
                           Stanford University
                           Stanford, CA 94305, USA
                           Michael I. Jordan                                                                         JORDAN@CS.BERKELEY.EDU
                           Computer Science Division and Department of Statistics
                           University of California
                           Berkeley, CA 94720, USA
                           Editor: John Lafferty
                                                                                   Abstract
                                 Wedescribe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of
                                discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each
                                item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in
                                turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of
                                text modeling, the topic probabilities provide an explicit representation of a document. We present
                                efﬁcient approximate inference techniques based on variational methods and an EM algorithm for
                                empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation,
                                and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI
                                model.
                           1. Introduction
                           In this paper we consider the problem of modeling text corpora and other collections of discrete
                           data. The goal is to ﬁnd short descriptions of the members of a collection that enable efﬁcient
                           processing of large collections while preserving the essential statistical relationships that are useful
                           for basic tasks such as classiﬁcation, novelty detection, summarization, and similarity and relevance
                           judgments.
                                Signiﬁcant progress has been made on this problem by researchers in the ﬁeld of informa-
                           tion retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999). The basic methodology proposed by
                           IR researchers for text corpora—a methodology successfully deployed in modern Internet search
                           engines—reduces each document in the corpus to a vector of real numbers, each of which repre-
                           sents ratios of counts. In the popular tf-idf scheme (Salton and McGill, 1983), a basic vocabulary
                           of “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of the
                           number of occurrences of each word. After suitable normalization, this term frequency count is
                           comparedtoaninversedocumentfrequencycount, which measures the number of occurrences of a
                           c
                           2003DavidM.Blei,AndrewY.NgandMichaelI.Jordan.
                          BLEI,NG, ANDJORDAN
          word in the entire corpus (generally on a log scale, and again suitably normalized). The end result
          is a term-by-document matrix X whose columns contain the tf-idf values for each of the documents
          in the corpus. Thus the tf-idf scheme reduces documents of arbitrary length to ﬁxed-length lists of
          numbers.
           Whilethetf-idf reductionhassomeappealingfeatures—notablyinitsbasicidentiﬁcationofsets
          of words that are discriminative for documents in the collection—the approach also provides a rela-
          tively small amount of reduction in description length and reveals little in the way of inter- or intra-
          documentstatistical structure. To address these shortcomings, IR researchers have proposed several
          other dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwester
          et al., 1990). LSI uses a singular value decomposition of the X matrix to identify a linear subspace
          in the space of tf-idf features that captures most of the variance in the collection. This approach can
          achieve signiﬁcant compression in large collections. Furthermore, Deerwester et al. argue that the
          derived features of LSI, which are linear combinations of the original tf-idf features, can capture
          someaspects of basic linguistic notions such as synonymy and polysemy.
           Tosubstantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is
          useful to develop a generative probabilistic model of text corpora and to study the ability of LSI to
          recover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative
          modeloftext,however,itisnotclearwhyoneshouldadopttheLSImethodology—onecanattempt
          to proceed more directly, ﬁtting the model to data using maximum likelihood or Bayesian methods.
           A signiﬁcant step forward in this regard was made by Hofmann (1999), who presented the
          probabilistic LSI (pLSI) model, also known as the aspect model, as an alternative to LSI. The pLSI
          approach, which we describe in detail in Section 4.3, models each word in a document as a sample
          fromamixturemodel,wherethemixturecomponentsaremultinomialrandomvariablesthatcanbe
          viewedasrepresentationsof“topics.” Thuseachwordisgeneratedfromasingletopic,anddifferent
          words in a document may be generated from different topics. Each document is represented as
          a list of mixing proportions for these mixture components and thereby reduced to a probability
          distribution on a ﬁxed set of topics. This distribution is the “reduced description” associated with
          the document.
           While Hofmann’s work is a useful step toward probabilistic modeling of text, it is incomplete
          in that it provides no probabilistic model at the level of documents. In pLSI, each document is
          represented as a list of numbers (the mixing proportions for topics), and there is no generative
          probabilistic model for these numbers. This leads to several problems: (1) the number of parame-
          ters in the model grows linearly with the size of the corpus, which leads to serious problems with
          overﬁtting, and (2) it is not clear how to assign probability to a document outside of the training set.
           To see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions
          underlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these
          methods are based on the “bag-of-words” assumption—that the order of words in a document can
          be neglected. In the language of probability theory, this is an assumption of exchangeability for the
          words in a document (Aldous, 1985). Moreover, although less often stated formally, these methods
          also assume that documents are exchangeable; the speciﬁc ordering of the documents in a corpus
          can also be neglected.
           Aclassic representation theorem due to de Finetti (1990) establishes that any collection of ex-
          changeable random variables has a representation as a mixture distribution—in general an inﬁnite
          mixture. Thus, if we wish to consider exchangeable representations for documents and words, we
          need to consider mixture models that capture the exchangeability of both words and documents.
                               994
                                       LATENTDIRICHLETALLOCATION
               This line of thinking leads to the latent Dirichlet allocation (LDA) model that we present in the
               current paper.
                  It is important to emphasize that an assumption of exchangeability is not equivalent to an as-
               sumption that the random variables are independent and identically distributed. Rather, exchange-
               ability essentially can be interpreted as meaning “conditionally independent and identically dis-
               tributed,” where the conditioning is with respect to an underlying latent parameter of a probability
               distribution. Conditionally, the joint distribution of the random variables is simple and factored
               while marginally over the latent parameter, the joint distribution can be quite complex. Thus, while
               an assumption of exchangeability is clearly a major simplifying assumption in the domain of text
               modeling, and its principal justiﬁcation is that it leads to methods that are computationally efﬁcient,
               the exchangeability assumptions do not necessarily lead to methods that are restricted to simple
               frequency counts or linear operations. We aim to demonstrate in the current paper that, by taking
               the de Finetti theorem seriously, we can capture signiﬁcant intra-document statistical structure via
               the mixing distribution.
                  It is also worth noting that there are a large number of generalizations of the basic notion of
               exchangeability, including various forms of partial exchangeability, and that representation theo-
               remsareavailable for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in
               the current paper focuses on simple “bag-of-words” models, which lead to mixture distributions for
               single words (unigrams), our methods are also applicable to richer models that involve mixtures for
               larger structural units such as n-grams or paragraphs.
                  The paper is organized as follows. In Section 2 we introduce basic notation and terminology.
               The LDA model is presented in Section 3 and is compared to related latent variable models in
               Section 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative
               example of ﬁtting LDA to data is provided in Section 6. Empirical results in text modeling, text
               classiﬁcation and collaborative ﬁltering are presented in Section 7. Finally, Section 8 presents our
               conclusions.
               2. Notation and terminology
               Weusethelanguage of text collections throughout the paper, referring to entities such as “words,”
               “documents,” and “corpora.” This is useful in that it helps to guide intuition, particularly when
               we introduce latent variables which aim to capture abstract notions such as topics. It is important
               to note, however, that the LDA model is not necessarily tied to text, and has applications to other
               problems involving collections of data, including data from domains such as collaborative ﬁltering,
               content-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental
               results in the collaborative ﬁltering domain.
                  Formally, we deﬁne the following terms:
                  • Aword is the basic unit of discrete data, deﬁned to be an item from a vocabulary indexed by
                    {1,...V,}. Werepresentwordsusingunit-basisvectorsthathaveasinglecomponentequalto
                    one and all other components equal to zero. Thus, using superscripts to denote components,
                                                                        v       u
                    the vth word in the vocabulary is represented by aV-vector w such that w =1 and w =0 for
                    u6=v.
                  • Adocument is a sequence of N words denoted by w =(w ,w ,...w, ), where w is the nth
                                                              1 2     N        n
                    wordinthesequence.
                  • Acorpusisacollection of M documents denoted by D ={w ,w ,...w, }.
                                                                1  2    M
                                                  995
                                                                BLEI,NG, ANDJORDAN
                           We wish to ﬁnd a probabilistic model of a corpus that not only assigns high probability to
                       membersofthecorpus, but also assigns high probability to other “similar” documents.
                       3. Latent Dirichlet allocation
                       Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is
                       that documents are represented as random mixtures over latent topics, where each topic is charac-
                       terized by a distribution over words.1
                           LDAassumesthefollowinggenerative process for each document w in a corpus D:
                           1. Choose N ∼Poisson(ξ).
                           2. Choose θ∼Dir(α).
                           3. For each of the N words w :
                                                           n
                                (a) Choose a topic zn ∼ Multinomial(θ).
                               (b) Choose a word w from p(w |z ,β), a multinomial probability conditioned on the topic
                                                       n           n   n
                                    zn.
                       Several simplifying assumptions are made in this basic model, some of which we remove in subse-
                       quent sections. First, the dimensionality k of the Dirichlet distribution (and thus the dimensionality
                       of the topic variable z) is assumed known and ﬁxed. Second, the word probabilities are parameter-
                       ized by a k×V matrix β where β = p(wj =1|zi =1), which for now we treat as a ﬁxed quantity
                                                            ij
                       that is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and
                       more realistic document length distributions can be used as needed. Furthermore, note that N is
                       independent of all the other data generating variables (θ and z). It is thus an ancillary variable and
                       wewill generally ignore its randomness in the subsequent development.
                           Ak-dimensional Dirichlet random variable θ can take values in the (k−1)-simplex (a k-vector
                       θlies in the (k−1)-simplex if θ ≥0,         k  θ =1),andhasthefollowingprobabilitydensityonthis
                                                          i      ∑      i
                       simplex:                                    i=1
                                                                     Γ  k α
                                                                        ∑      i
                                                         p(θ|α)=          i=1    θα1−1···θαk−1,                                  (1)
                                                                       k   Γ(α) 1           k
                                                                     ∏         i
                                                                       i=1
                       wheretheparameterαisak-vectorwithcomponentsα >0,andwhereΓ(x)istheGammafunction.
                                                                                   i
                       TheDirichlet is a convenient distribution on the simplex — it is in the exponential family, has ﬁnite
                       dimensionalsufﬁcientstatistics, and is conjugate to the multinomial distribution. In Section 5, these
                       propertieswillfacilitatethedevelopmentofinferenceandparameterestimationalgorithmsforLDA.
                           Giventheparameters α and β, the joint distribution of a topic mixture θ, a set of N topics z, and
                       a set of N words w is given by:
                                                                               N
                                                  p(θ,z,w|α,β)=p(θ|α)             p(z |θ)p(w |z ,β),                             (2)
                                                                              ∏ n              n   n
                                                                              n=1
                         1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but
                           we make no epistemological claims regarding these latent variables beyond their utility in representing probability
                           distributions on sets of words.
                                                                            996
                                                     LATENTDIRICHLETALLOCATION
                                                                β
                                           α            θ            zw
                                                                                        N     M
                     Figure 1: Graphical model representation of LDA. The boxes are “plates” representing replicates.
                               Theouterplaterepresentsdocuments,whiletheinnerplaterepresentstherepeatedchoice
                               of topics and words within a document.
                     where p(zn|θ) is simply θi for the unique i such that zi = 1. Integrating over θ and summing over
                     z, we obtain the marginal distribution of a document: n
                                                      Z          N                          !
                                         p(w|α,β)= p(θ|α)                p(z |θ)p(w |z ,β) dθ.                     (3)
                                                                  ∏∑ n              n  n
                                                                  n=1 z
                                                                       n
                     Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-
                     bility of a corpus:
                                                  M Z            N                             !
                                    p(D|α,β)=           p(θ |α)     d   p(z   |θ )p(w |z ,β) dθ .
                                                  ∏        d      ∏∑ dn d             dn  dn        d
                                                 d=1              n=1z
                                                                      dn
                        The LDA model is represented as a probabilistic graphical model in Figure 1. As the ﬁgure
                     makes clear, there are three levels to the LDA representation. The parameters α and β are corpus-
                     level parameters, assumed to be sampled once in the process of generating a corpus. The variables
                     θ are document-level variables, sampled once per document. Finally, the variables z  and w   are
                      d                                                                                dn       dn
                     word-level variables and are sampled once for each word in each document.
                        It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model. A
                     classical clustering model would involve a two-level model in which a Dirichlet is sampled once
                     for a corpus, a multinomial clustering variable is selected once for each document in the corpus,
                     and a set of words are selected for the document conditional on the cluster variable. As with many
                     clustering models, such a model restricts a document to being associated with a single topic. LDA,
                     ontheotherhand,involvesthreelevels, and notably the topic node is sampled repeatedly within the
                     document. Under this model, documents can be associated with multiple topics.
                        Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,
                     where they are referred to as hierarchical models (Gelman et al., 1995), or more precisely as con-
                     ditionally independent hierarchical models (Kass and Steffey, 1989). Such models are also often
                     referred to as parametric empirical Bayes models, a term that refers not only to a particular model
                     structure, but also to the methods used for estimating parameters in the model (Morris, 1983). In-
                     deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters
                     suchasαandβinsimpleimplementationsofLDA,butwealsoconsiderfullerBayesianapproaches
                     as well.
                                                                   997
                                                 BLEI,NG, ANDJORDAN
                  3.1 LDAandexchangeability
                  Aﬁnite set of random variables {z ,...z, } is said to be exchangeable if the joint distribution is
                                                1     N
                  invariant to permutation. If π is a permutation of the integers from 1 to N:
                                             p(z ,...z, )=p(z   ,...z,   ).
                                                1     N      π(1)    π(N)
                  An inﬁnite sequence of random variables is inﬁnitely exchangeable if every ﬁnite subsequence is
                  exchangeable.
                     DeFinetti’srepresentationtheoremstatesthatthejointdistributionofaninﬁnitelyexchangeable
                  sequence of random variables is as if a random parameter were drawn from some distribution and
                  then the random variables in question were independent and identically distributed, conditioned on
                  that parameter.
                     In LDA, we assume that words are generated by topics (by ﬁxed conditional distributions) and
                  that those topics are inﬁnitely exchangeable within a document. By de Finetti’s theorem, the prob-
                  ability of a sequence of words and topics must therefore have the form:
                                                 Z      N                 !
                                        p(w,z)= p(θ)        p(z |θ)p(w |z ) dθ,
                                                         ∏ n          n n
                                                         n=1
                  where θ is the random parameter of a multinomial over topics. We obtain the LDA distribution
                  on documents in Eq. (3) by marginalizing out the topic variables and endowing θ with a Dirichlet
                  distribution.
                  3.2 A continuous mixture of unigrams
                  The LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often
                  studied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic
                  variable z, however, we can understand LDA as a two-level model.
                     In particular, let us form the word distribution p(w|θ,β):
                                              p(w|θ,β)= p(w|z,β)p(z|θ).
                                                         ∑
                                                         z
                  Note that this is a random quantity since it depends on θ.
                     Wenowdeﬁnethefollowinggenerative process for a document w:
                    1. Choose θ∼Dir(α).
                    2. For each of the N words w :
                                             n
                        (a) Choose a word w from p(w |θ,β).
                                          n         n
                  This process deﬁnes the marginal distribution of a document as a continuous mixture distribution:
                                                   Z         N            !
                                        p(w|α,β)= p(θ|α)         p(w |θ,β) dθ,
                                                              ∏ n
                                                              n=1
                  where p(w |θ,β) are the mixture components and p(θ|α) are the mixture weights.
                           n
                     Figure 2 illustrates this interpretation of LDA. It depicts the distribution on p(w|θ,β) which is
                  induced from a particular instance of an LDA model. Note that this distribution on the (V −1)-
                  simplexisattained with only k+kV parameters yet exhibits a very interesting multimodal structure.
                                                          998
                                                        LATENTDIRICHLETALLOCATION
                                              
                                              
                                              
                                             
                                                                                                           
                                                                                                       
                                                                                                 
                                                                                           
                                                                                     
                                                                            
                      Figure 2: An example density on unigram distributions p(w|θ,β) under LDA for three words and
                                 four topics. The triangle embedded in the x-y plane is the 2-D simplex representing all
                                 possible multinomial distributions over three words. Each of the vertices of the trian-
                                 gle corresponds to a deterministic distribution that assigns probability one to one of the
                                 words; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid
                                 of the triangle is the uniform distribution over all three words. The four points marked
                                 with an x are the locations of the multinomial distributions p(w|z) for each of the four
                                 topics, and the surface shown on top of the simplex is an example of a density over the
                                 (V −1)-simplex (multinomial distributions of words) given by LDA.
                      4. Relationship with other latent variable models
                      In this section we compare LDA to simpler latent variable models for text—the unigram model, a
                      mixture of unigrams, and the pLSI model. Furthermore, we present a uniﬁed geometric interpreta-
                      tion of these models which highlights their key differences and similarities.
                      4.1 Unigrammodel
                      Under the unigram model, the words of every document are drawn independently from a single
                      multinomial distribution:
                                                                          N
                                                                p(w)= p(w ).
                                                                         ∏ n
                                                                         n=1
                      This is illustrated in the graphical model in Figure 3a.
                                                                        999
                                    BLEI,NG, ANDJORDAN
                                           w  N
                                                 M
                                        (a) unigram
                                      zw
                                                N  M
                                    (b) mixture of unigrams
                                  d       zw
                                                    N  M
                                     (c) pLSI/aspect model
                    Figure 3: Graphical model representation of different models of discrete data.
             4.2 Mixture of unigrams
             If we augment the unigram model with a discrete random topic variable z (Figure 3b), we obtain a
             mixture of unigrams model (Nigam et al., 2000). Under this mixture model, each document is gen-
             erated by ﬁrst choosing a topic z and then generating N words independently from the conditional
             multinomial p(w|z). The probability of a document is:
                                             N
                                   p(w)= p(z)  p(w |z).
                                        ∑ ∏ n
                                        z   n=1
                Whenestimatedfromacorpus,theworddistributionscanbeviewedasrepresentationsoftopics
             under the assumption that each document exhibits exactly one topic. As the empirical results in
             Section 7 illustrate, this assumption is often too limiting to effectively model a large collection of
             documents.
                In contrast, the LDA model allows documents to exhibit multiple topics to different degrees.
             This is achieved at a cost of just one additional parameter: there are k−1 parameters associated
             with p(z) in the mixture of unigrams, versus the k parameters associated with p(θ|α) in LDA.
             4.3 Probabilistic latent semantic indexing
             Probabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann,
             1999). The pLSI model, illustrated in Figure 3c, posits that a document label d and a word w are
                                                                       n
                                          1000
                                       LATENTDIRICHLETALLOCATION
               conditionally independent given an unobserved topic z:
                                      p(d,w )=p(d)  p(w |z)p(z|d).
                                           n      ∑ n
                                                   z
                  ThepLSImodelattempts to relax the simplifying assumption made in the mixture of unigrams
               modelthateachdocumentisgeneratedfromonlyonetopic. Inasense,itdoescapturethepossibility
               that a documentmaycontainmultipletopicssince p(z|d)servesasthemixtureweightsofthetopics
               for a particular document d. However, it is important to note that d is a dummy index into the list
               of documents in the training set. Thus, d is a multinomial random variable with as many possible
               values as there are training documents and the model learns the topic mixtures p(z|d) only for those
               documents on which it is trained. For this reason, pLSI is not a well-deﬁned generative model of
               documents; there is no natural way to use it to assign probability to a previously unseen document.
                  Afurther difﬁculty with pLSI, which also stems from the use of a distribution indexed by train-
               ing documents, is that the number of parameters which must be estimated grows linearly with the
               number of training documents. The parameters for a k-topic pLSI model are k multinomial distri-
               butions of size V and M mixtures over the k hidden topics. This gives kV +kM parameters and
               therefore linear growth in M. The linear growth in parameters suggests that the model is prone
               to overﬁtting and, empirically, overﬁtting is indeed a serious problem (see Section 7.1). In prac-
               tice, a tempering heuristic is used to smooth the parameters of the model for acceptable predic-
               tive performance. It has been shown, however, that overﬁtting can occur even when tempering is
               used (Popescul et al., 2001).
                  LDAovercomesboth of these problems by treating the topic mixture weights as a k-parameter
               hiddenrandomvariableratherthanalargesetofindividualparameterswhichareexplicitlylinkedto
               the training set. As described in Section 3, LDA is a well-deﬁned generative model and generalizes
               easily to new documents. Furthermore, the k+kV parameters in a k-topic LDA model do not grow
               with the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the
               sameoverﬁtting issues as pLSI.
               4.4 A geometric interpretation
               Agood way of illustrating the differences between LDA and the other latent topic models is by
               considering the geometry of the latent space, and seeing how a document is represented in that
               geometry under each model.
                  All four of the models described above—unigram, mixture of unigrams, pLSI, and LDA—
               operate in the space of distributions over words. Each such distribution can be viewed as a point on
               the (V −1)-simplex, which we call the word simplex.
                  The unigram model ﬁnds a single point on the word simplex and posits that all words in the
               corpus come from the corresponding distribution. The latent variable models consider k points on
               the word simplex and form a sub-simplex based on those points, which we call the topic simplex.
               Note that any point on the topic simplex is also a point on the word simplex. The different latent
               variable models use the topic simplex in different ways to generate a document.
                  • Themixtureofunigramsmodelpositsthatforeachdocument,oneofthekpointsontheword
                    simplex(that is, one of the corners of the topic simplex) is chosen randomly and all the words
                    of the document are drawn from the distribution corresponding to that point.
                                                 1001
                                                                                                       BLEI,NG, ANDJORDAN
                                                                                                                                             
                                                                                                                                             
                                                                                                                        
                                                                                                                                            topic 1
                                                                                                                                            
                                                                                                                                            
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             topic simplex
                                                                                                                        
                                                                                                                                             
                                                                                                                      x                     
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                                        
                                                                                                                                             
                                                                                                                                             
                                                                                                                  x     x
                                                                                                                                             x
                                                                                                                                             
                                                                                                                                             
                                                                                                                                             
                                                                                                              x                              
                                                                                                                                             xword simplex
                                                                                                                  x                          
                                                                                                                        
                                                                                                                                             x
                                                                                                                                             
                                                                                                                x                            
                                                                                                                     x                      
                                                                                                                                             
                                                                                                                                             x
                                                                                                                        x
                                                                                                                 x                           
                                                                                                                        
                                                                                                                                             
                                                                                                               x                             
                                                                                                                                             x
                                                                                                                                             
                                                                                                                                             x
                                                                                                                    x                        x
                                                                                                      x                                      
                                                                                                                                             x
                                                                                                                                             
                                                                                                                        
                                                                                                                x                            
                                                                                                                                             
                                                                                                         x                                   x
                                                                                                                                             
                                                                                                                                             x
                                                                                                                       x                        x
                                                                                                  x                                          
                                                                               topic 2                                                       
                                                                                                                        
                                                                                                                                             
                                                                                                                                             
                                                                                                                                           
                                                                                                                        
                                                                                                                      
                                                                                                                       topic 3
                                                                                                                      
                                                                                                                      
                                                                                                                      
                                                                                                                      
                                                                                                                      
                                                                                                                      
                                                                                                                      
                                     Figure 4: The topic simplex for three topics embedded in the word simplex for three words. The
                                                       corners of the word simplex correspond to the three distributions where each word (re-
                                                       spectively) has probability one. The three points of the topic simplex correspond to three
                                                       different distributions over words. The mixture of unigrams places each document at one
                                                       of the corners of the topic simplex. The pLSI model induces an empirical distribution on
                                                       the topic simplex denoted by x. LDA places a smooth distribution on the topic simplex
                                                       denoted by the contour lines.
                                           • ThepLSImodelpositsthateachwordofatrainingdocumentcomesfromarandomlychosen
                                                topic. The topics are themselves drawn from a document-speciﬁc distribution over topics,
                                                i.e., a point on the topic simplex. There is one such distribution for each document; the set of
                                                training documents thus deﬁnes an empirical distribution on the topic simplex.
                                           • LDA posits that each word of both the observed and unseen documents is generated by a
                                                randomlychosentopicwhichisdrawnfromadistributionwitharandomlychosenparameter.
                                                Thisparameterissampledonceperdocumentfromasmoothdistributiononthetopicsimplex.
                                     These differences are highlighted in Figure 4.
                                     5. Inference and Parameter Estimation
                                     Wehavedescribed the motivation behind LDA and illustrated its conceptual advantages over other
                                     latent topic models. In this section, we turn our attention to procedures for inference and parameter
                                     estimation under LDA.
                                                                                                                       1002
                                                     LATENTDIRICHLETALLOCATION
                                           β                                           γ           φ
                        α          θ            zw θ z
                                                                N     M                                      N     M
                     Figure 5: (Left) Graphical model representation of LDA. (Right) Graphical model representation
                               of the variational distribution used to approximate the posterior in LDA.
                     5.1 Inference
                     The key inferential problem that we need to solve in order to use LDA is that of computing the
                     posterior distribution of the hidden variables given a document:
                                                                      p(θ,z,w|α,β)
                                                     p(θ,z|w,α,β)= p(w|α,β) .
                     Unfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distri-
                     bution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:
                                                   Γ( α)Z   k            ! N k V              j !
                                                     ∑ i             α−1
                                                       i              i                      wn
                                      p(w|α,β)=                    θ                  (θ β )     dθ,
                                                   ∏Γ(α)        ∏ i          ∏∑∏ i ij
                                                     i    i     i=1          n=1i=1 j=1
                     a function which is intractable due to the coupling between θ and β in the summation over latent
                     topics (Dickey, 1983). Dickey showsthatthisfunctionisanexpectationunderaparticularextension
                     to the Dirichlet distribution which can be represented with special hypergeometric functions. It has
                     been used in a Bayesian context for censored discrete data to represent the posterior on θ which, in
                     that setting, is a random parameter (Dickey et al., 1987).
                        Although the posterior distribution is intractable for exact inference, a wide variety of approxi-
                     mateinferencealgorithmscanbeconsideredforLDA,includingLaplaceapproximation,variational
                     approximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple
                     convexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in
                     Section 8.
                     5.2 Variational inference
                     Thebasic idea of convexity-based variational inference is to make use of Jensen’s inequality to ob-
                     tain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers
                     a family of lower bounds, indexed by a set of variational parameters. The variational parameters
                     are chosen by an optimization procedure that attempts to ﬁnd the tightest possible lower bound.
                        Asimple way to obtain a tractable family of lower bounds is to consider simple modiﬁcations
                     of the original graphical model in which some of the edges and nodes are removed. Consider in
                     particular the LDA model shown in Figure 5 (left). The problematic coupling between θ and β
                                                                   1003
                                                                BLEI,NG, ANDJORDAN
                       arises due to the edges between θ, z, and w. By dropping these edges and the w nodes, and endow-
                       ing the resulting simpliﬁed graphical model with free variational parameters, we obtain a family
                       of distributions on the latent variables. This family is characterized by the following variational
                       distribution:
                                                                                   N
                                                           q(θ,z|γ,φ)=q(θ|γ)∏q(zn|φn),                                           (4)
                                                                                  n=1
                       wheretheDirichletparameterγandthemultinomialparameters(φ ,...φ, )arethefreevariational
                                                                                                1       N
                       parameters.
                           Having speciﬁed a simpliﬁed family of probability distributions, the next step is to set up an
                       optimization problem that determines the values of the variational parameters γ and φ. As we show
                       in Appendix A, the desideratum of ﬁnding a tight lower bound on the log likelihood translates
                       directly into the following optimization problem:
                                                    ∗  ∗
                                                  (γ ,φ )=argminD(q(θ,z|γ,φ) k p(θ,z|w,α,β)).                                    (5)
                                                                 (γ,φ)
                       Thus the optimizing values of the variational parameters are found by minimizing the Kullback-
                       Leibler (KL) divergence between the variational distribution and the true posterior p(θ,z|w,α,β).
                       This minimization can be achieved via an iterative ﬁxed-point method. In particular, we show in
                       Appendix A.3 that by computing the derivatives of the KL divergence and setting them equal to
                       zero, we obtain the following pair of update equations:
                                                           φ     ∝βexp{E[log(θ)|γ]}                                              (6)
                                                             ni        iwn       q       i
                                                             γ   = α + N φ .                                                     (7)
                                                              i        i   ∑      ni
                                                                             n=1
                       AsweshowinAppendixA.1,theexpectationinthemultinomialupdatecanbecomputedasfollows:
                                                         E [log(θ )|γ]=Ψ(γ )−Ψ  k            γ ,                                (8)
                                                           q      i            i        ∑j=1 j
                       where Ψ is the ﬁrst derivative of the logΓ function which is computable via Taylor approxima-
                       tions (Abramowitz and Stegun, 1970).
                           Eqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a poste-
                       rior Dirichlet given expected observations taken under the variational distribution, E[z |φ ]. The
                                                                                                                        n  n
                       multinomial update is akin to using Bayes’ theorem, p(z |w ) ∝ p(w |z )p(z ), where p(z ) is
                                                                                        n   n         n   n    n               n
                       approximated by the exponential of the expected value of its logarithm under the variational distri-
                       bution.
                           It is important to note that the variational distribution is actually a conditional distribution,
                       varying as a function of w. This occurs because the optimization problem in Eq. (5) is conducted
                                                                                  ∗   ∗
                       for ﬁxed w, and thus yields optimizing parameters (γ ,φ ) that are a function of w. We can write
                                                                           ∗       ∗
                       the resulting variational distribution as q(θ,z|γ (w),φ (w)), where we have made the dependence
                       on w explicit. Thus the variational distribution can be viewed as an approximation to the posterior
                       distribution p(θ,z|w,α,β).                                       ∗      ∗
                           In the language of text, the optimizing parameters (γ (w),φ (w)) are document-speciﬁc. In
                                                                          ∗
                       particular, we view the Dirichlet parameters γ (w) as providing a representation of a document in
                       the topic simplex.
                                                                           1004
                                                    LATENTDIRICHLETALLOCATION
                            (1)     initialize φ0 := 1/k for all i and n
                                              ni
                             (2)    initialize γ := α +N/k for all i
                                              i     i
                            (3)     repeat
                            (4)        for n = 1 to N
                            (5)           for i = 1 to k
                                               t+1                t
                             (6)              φ   :=β exp(Ψ(γ))
                                                       iw
                                               ni       n         i
                                                      t+1
                            (7)           normalize φn   to sum to 1.
                                        t+1          N   t+1
                            (8)        γ    :=α+∑ φn
                                                     n=1
                            (9)     until convergence
                                           Figure 6: A variational inference algorithm for LDA.
                        Wesummarizethe variational inference procedure in Figure 6, with appropriate starting points
                    for γ and φn. From the pseudocode it is clear that each iteration of variational inference for LDA
                    requires O((N+1)k) operations. Empirically, we ﬁnd that the number of iterations required for a
                    single document is on the order of the number of words in the document. This yields a total number
                    of operations roughly on the order of N2k.
                    5.3 Parameter estimation
                    In this section we present an empirical Bayes method for parameter estimation in the LDA model
                    (see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents D =
                     {w ,w ,...,w }, we wish to ﬁnd parameters α and β that maximize the (marginal) log likelihood
                       1   2       M
                    of the data:
                                                                 M
                                                      ℓ(α,β)=      logp(w |α,β).
                                                                ∑          d
                                                                d=1
                        As we have described above, the quantity p(w|α,β) cannot be computed tractably. However,
                    variational inference provides us with a tractable lower bound on the log likelihood, a bound which
                    wecanmaximizewithrespecttoαandβ. WecanthusﬁndapproximateempiricalBayesestimates
                    for the LDA model via an alternating variational EM procedure that maximizes a lower bound with
                    respect to the variational parameters γ and φ, and then, for ﬁxed values of the variational parameters,
                    maximizes the lower bound with respect to the model parameters α and β.
                        We provide a detailed derivation of the variational EM algorithm for LDA in Appendix A.4.
                    Thederivation yields the following iterative algorithm:
                                                                                                               ∗,φ∗ :
                        1. (E-step) For each document, ﬁnd the optimizing values of the variational parameters {γ
                           d ∈D}. This is done as described in the previous section.                           d  d
                        2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model
                           parametersαandβ. Thiscorrespondstoﬁndingmaximumlikelihoodestimateswithexpected
                           sufﬁcient statistics for each document under the approximate posterior which is computed in
                           the E-step.
                                                                  1005
                                                         BLEI,NG, ANDJORDAN
                                                   η                      β k
                                           α            θ            zw
                                                                                        N     M
                                  Figure 7: Graphical model representation of the smoothed LDA model.
                     These two steps are repeated until the lower bound on the log likelihood converges.
                        In Appendix A.4, we show that the M-step update for the conditional multinomial parameter β
                     can be written out analytically:
                                                                 M N
                                                                      d      j
                                                           β ∝         φ∗ w .                                      (9)
                                                            ij   ∑∑ dni dn
                                                                d=1n=1
                     We further show that the M-step update for Dirichlet parameter α can be implemented using an
                     efﬁcient Newton-Raphson method in which the Hessian is inverted in linear time.
                     5.4 Smoothing
                     Thelarge vocabulary size that is characteristic of many document corpora creates serious problems
                     of sparsity. A new document is very likely to contain words that did not appear in any of the
                     documents in a training corpus. Maximum likelihood estimates of the multinomial parameters
                     assign zero probability to such words, and thus zero probability to new documents. The standard
                     approachtocopingwiththisproblemisto“smooth”themultinomialparameters,assigningpositive
                     probability to all vocabulary items whether or not they are observed in the training set (Jelinek,
                     1997). Laplace smoothing is commonly used; this essentially yields the mean of the posterior
                     distribution under a uniform Dirichlet prior on the multinomial parameters.
                        Unfortunately, in the mixture model setting, simple Laplace smoothing is no longer justiﬁed as a
                     maximumaposteriorimethod(althoughitisoftenimplementedinpractice; cf.Nigametal.,1999).
                     In fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior
                     in the mixture model setting, for much the same reason that one obtains an intractable posterior in
                     the basic LDAmodel. Ourproposedsolutiontothisproblemistosimplyapplyvariationalinference
                     methods to the extended model that includes Dirichlet smoothing on the multinomial parameter.
                        In the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat β as
                     a k×V random matrix (one row for each mixture component), where we assume that each row
                                                                                         2 We now extend our infer-
                     is independently drawn from an exchangeable Dirichlet distribution.
                     ence procedures to treat the β as random variables that are endowed with a posterior distribution,
                                                  i
                      2. AnexchangeableDirichlet is simply a Dirichlet distribution with a single scalar parameter η. The density is the same
                        as a Dirichlet (Eq. 1) where αi = η for each component.
                                                                   1006
                                                                                                          LATENTDIRICHLETALLOCATION
                                         conditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and
                                         consider a fuller Bayesian approach to LDA.
                                                 Weconsideravariational approach to Bayesian inference that places a separable distribution on
                                         the random variables β, θ, and z (Attias, 2000):
                                                                                                                                     k                           M
                                                                             q(β ,z                ,θ         |λ,φ,γ)= Dir(β |λ)                                      q (θ ,z |φ ,γ ),
                                                                                    1:k      1:M        1:M                        ∏                 i     i   ∏ d d d d d
                                                                                                                                   i=1                         d=1
                                         where q (θ,z|φ,γ) is the variational distribution deﬁned for LDA in Eq. (4). As is easily veriﬁed,
                                                         d
                                         the resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations
                                         for the variational parameters φ and γ, respectively, as well as an additional update for the new
                                         variational parameter λ:
                                                                                                                                       M N
                                                                                                                                                d              j
                                                                                                                  λ =η+                             φ∗ w .
                                                                                                                     ij               ∑∑ dni dn
                                                                                                                                      d=1n=1
                                         Iterating these equations to convergence yields an approximate posterior distribution on β, θ, and z.
                                                 Weare now left with the hyperparameter η on the exchangeable Dirichlet, as well as the hy-
                                         perparameter α from before. Our approach to setting these hyperparameters is again (approximate)
                                         empirical Bayes—weusevariationalEMtoﬁndmaximumlikelihoodestimatesoftheseparameters
                                         based on the marginal likelihood. These procedures are described in Appendix A.4.
                                         6. Example
                                         In this section, we provide an illustrative example of the use of an LDA model on real data. Our
                                         data are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing
                                         astandardlistofstopwords,weusedtheEMalgorithmdescribedinSection5.3toﬁndtheDirichlet
                                         and conditional multinomial parameters for a 100-topic LDA model. The top words from some of
                                         the resulting multinomial distributions p(w|z) are illustrated in Figure 8 (top). As we have hoped,
                                         these distributions seem to capture some of the underlying topics in the corpus (and we have named
                                         them according to these topics).
                                                 AsweemphasizedinSection4,oneoftheadvantagesofLDAoverrelatedlatentvariablemod-
                                         els is that it provides well-deﬁned inference procedures for previously unseen documents. Indeed,
                                         wecanillustrate how LDA works by performing inference on a held-out document and examining
                                         the resulting variational posterior parameters.
                                                 Figure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter
                                         estimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet
                                         parameters γ for the article and variational posterior multinomial parameters φ for each word in the
                                                                                                                                                                                                  n
                                         article.
                                                 Recall that the ith posterior Dirichlet parameter γ is approximately the ith prior Dirichlet pa-
                                                                                                                                                    i
                                         rameter α plus the expected number of words which were generated by the ith topic (see Eq. 7).
                                                             i
                                         Therefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate
                                         the expected number of words which were allocated to each topic for a particular document. For
                                         the example article in Figure 8 (bottom), most of the γ are close to α . Four topics, however, are
                                                                                                                                                       i                             i
                                         signiﬁcantly larger (by this, we mean γ −α ≥ 1). Looking at the corresponding distributions over
                                                                                                                      i         i
                                         words identiﬁes the topics which mixed to form this document (Figure 8, top).
                                                                                                                                      1007
                                                                BLEI,NG, ANDJORDAN
                           Further insight comes from examining the φn parameters. These distributions approximate
                       p(zn|w) and tend to peak towards one of the k possible topic values. In the article text in Figure 8,
                       the words are color coded according to these values (i.e., the ith color is used if q (zi = 1) > 0.9).
                                                                                                                 n  n
                       With this illustration, one can identify how the different topics mixed in the document text.
                           While demonstrating the power of LDA, the posterior analysis also highlights some of its lim-
                       itations. In particular, the bag-of-words assumption allows words that should be generated by the
                       same topic (e.g., “William Randolph Hearst Foundation”) to be allocated to several different top-
                       ics. Overcoming this limitation would require some form of extension of the basic LDA model;
                       in particular, we might relax the bag-of-words assumption by assuming partial exchangeability or
                       Markovianity of word sequences.
                       7. Applications and Empirical Results
                       In this section, we discuss our empirical evaluation of LDA in several problem domains—document
                       modeling, document classiﬁcation, and collaborative ﬁltering.
                           In all of the mixture models, the expected complete log likelihood of the data has local max-
                       ima at the points where all or some of the mixture components are equal to each other. To avoid
                       these local maxima, it is important to initialize the EM algorithm appropriately. In our experiments,
                       weinitialize EM by seeding each conditional multinomial distribution with ﬁve documents, reduc-
                       ing their effective total length to two words, and smoothing across the whole vocabulary. This is
                       essentially an approximation to the scheme described in Heckerman and Meila (2001).
                       7.1 Documentmodeling
                       Wetrained a number of latent variable models, including LDA, on two text corpora to compare the
                       generalization performance of these models. The documents in the corpora are treated as unlabeled;
                       thus, our goal is density estimation—we wish to achieve high likelihood on a held-out test set. In
                       particular, we computed the perplexity of a held-out test set to evaluate the models. The perplexity,
                       used by convention in language modeling, is monotonically decreasing in the likelihood of the test
                       data, and is algebraicly equivalent to the inverse of the geometric mean per-word likelihood. A
                       lower perplexity score indicates better generalization performance.3 More formally, for a test set of
                       Mdocuments,theperplexity is:
                                                                                   M logp(w )
                                                                                  ∑              d
                                                    perplexity(Dtest)=exp       − d=1                 .
                                                                                        M N
                                                                                      ∑       d
                                                                                        d=1
                           Inourexperiments,weusedacorpusofscientiﬁcabstractsfromtheC.Eleganscommunity(Av-
                       ery, 2002)containing5,225abstractswith28,414uniqueterms,andasubsetoftheTRECAPcorpus
                       containing 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of
                       the data for test purposes and trained the models on the remaining 90%. In preprocessing the data,
                         3. Note that we simply use perplexity as a ﬁgure of merit for comparing models. The models that we compare are all
                           unigram (“bag-of-words”) models, which—as we have discussed in the Introduction—are of interest in the informa-
                           tion retrieval context. We are not attempting to do language modeling in this paper—an enterprise that would require
                           us to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be
                           considered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such
                           extensions to language modeling to future work.
                                                                           1008
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      LATENTDIRICHLETALLOCATION
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \Arts"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \Budgets"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \Children"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \Eduation"
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NEW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            MILLION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             CHILDREN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           SCHOOL
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FILM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           T                                                    AX                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             W                                                                                  OMEN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            STUDENTS
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  SHO                                                                                                                                                                        W                                                                                                                                                                                                                                                                                                                                                   PR                                                                                                                  OGRAM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           PEOPLE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             SCHOOLS
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  MUSIC                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          BUDGET                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              CHILD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              EDUCA                                                                                                                                                                                                                                                                                                    TION
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  MO                                                                                                                                         VIE                                                                                                                                                                                                                                                                                                                                                                                 BILLION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             YEARS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TEA                                                                                                                                                                              CHERS
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  PLA                                                                                                                                                                 Y                                                                                                                                                                                                                                                                                                                                                          FEDERAL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             F                                            AMILIES                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               HIGH
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  MUSICAL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        YEAR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                W                                                                                  ORK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             PUBLIC
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  BEST                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           SPENDING                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            P                                                 ARENTS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           TEA                                                                                                                                                                              CHER
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A                                                           CTOR                                                                                                                                                                                                                                                                                                                                                                                                                                                               NEW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SA                                                                                                   YS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            BENNETT
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FIRST                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ST                                                                                                 A                                                      TE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        F                                            AMIL                                                                                                                                                                                                                 Y                                                                                                                                                                                                                                                                                                                                                                                                MANIGA                                                                                                                                                                                                                                                                                                                                                        T
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  YORK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                WELF                                                                                                                                                                                                                                           ARE                                                                                                                                                                                                                                                                                                                                                                                                                 NAMPHY
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  OPERA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          MONEY                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               MEN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ST                                                                                                 A                                                      TE
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  THEA                                                                                                                                                                                                                                      TER                                                                                                                                                                                                                                                                                  PR                                                                                                                  OGRAMS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          PER                                                                                                                                                                          CENT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  PRESIDENT
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A                                                           CTRESS                                                                                                                                                                                                                                                                                                                                                                                                                                                             GO                                                                                                                              VERNMENT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            CARE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ELEMENT                                                                                                                                                                                                                                                                                                                                                                                                                       AR                                                                                                                  Y
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  LO                                                                                                                 VE                                                                                                                                                                                                                                                                                                                                                                                                          CONGRESS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            LIFE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               HAITI
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               TheWilliam Randolph Hearst Foundation willgive$1.25 million toLincoln Center, Metropoli-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               tan Opera Co., New York Philharmonic and Juilliard School. “Our board felt that we had a
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               real opportunity to make a mark on the future of the performing arts with these grants an act
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               every bit as important as our traditional areas of support in health, medical research, education
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           and the social services,” Hearst Foundation President Randolph A. Hearst said Monday in
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               announcing the grants.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Lincoln Center’s share will be $200,000 for its new building, which
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               will house young artists and provide new public facilities. The Metropolitan Opera Co. and
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               New York Philharmonic will receive $400,000 each. The Juilliard School, where music and
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               donation, too.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Figure 8: An example article from the AP corpus. Each color codes a different factor from which
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    the word is putatively generated.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1009
                                                      BLEI,NG, ANDJORDAN
                                3400                                             Smoothed Unigram
                                3200                                             Smoothed Mixt. Unigrams
                                                                                 LDA
                                3000                                             Fold in pLSI
                                2800
                                2600
                                2400
                              Perplexity2200
                                2000
                                1800
                                1600
                                1400
                                   0     10     20    30     40    50     60    70     80    90    100
                                                          Number of Topics
                               7000                                            Smoothed Unigram
                               6500                                            Smoothed Mixt. Unigrams
                                                                               LDA
                                                                               Fold in pLSI
                               6000
                               5500
                               5000
                              Perplexity4500
                               4000
                               3500
                               3000
                               2500
                                   0     20    40     60    80     100   120    140   160    180    200
                                                         Number of Topics
                   Figure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram
                             model, mixture of unigrams, and pLSI.
                                                               1010
                                                   LATENTDIRICHLETALLOCATION
                                     Num. topics (k)   Perplexity (Mult. Mixt.)  Perplexity (pLSI)
                                     2                 22,266                    7,052
                                     5                 2.20×108                  17,588
                                     10                1.93×1017                 63,800
                                                                22                        5
                                     20                1.20×10                   2.52×10
                                                                106                       6
                                     50                4.19×10                   5.04×10
                                                                150                       7
                                     100               2.39×10                   1.72×10
                                                                264                       7
                                     200               3.51×10                   1.31×10
                    Table 1: Overﬁtting in the mixture of unigrams and pLSI models for the AP corpus. Similar behav-
                             ior is observed in the nematode corpus (not reported).
                    we removed a standard list of 50 stop words from each corpus. From the AP data, we further
                    removed words that occurred only once.
                        WecomparedLDAwiththeunigram,mixtureofunigrams,andpLSImodelsdescribedinSec-
                    tion 4. We trained all the hidden variable models using EM with exactly the same stopping criteria,
                    that the average change in expected log likelihood is less than 0.001%.
                        Both the pLSI model and the mixture of unigrams suffer from serious overﬁtting issues, though
                    for different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,
                    overﬁtting is a result of peaked posteriors in the training set; a phenomenon familiar in the super-
                    vised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a
                    nearly deterministic clustering of the training documents (in the E-step) which is used to determine
                    the word probabilities in each mixture component (in the M-step). A previously unseen document
                    may best ﬁt one of the resulting mixture components, but will probably contain at least one word
                    which did not occur in the training documents that were assigned to that component. Such words
                    will have a very small probability, which causes the perplexity of the new document to explode.
                    As k increases, the documents of the training corpus are partitioned into ﬁner collections and thus
                    induce more words with small probabilities.
                        Inthemixtureofunigrams,wecanalleviateoverﬁttingthroughthevariationalBayesiansmooth-
                    ing scheme presented in Section 5.4. This ensures that all words will have some probability under
                    every mixture component.
                        In the pLSI case, the hard clustering problem is alleviated by the fact that each document is
                    allowed to exhibit a different proportion of topics. However, pLSI only refers to the training doc-
                    uments and a different overﬁtting problem arises that is due to the dimensionality of the p(z|d)
                    parameter. One reasonable approach to assigning probability to a previously unseen document is by
                    marginalizing over d:
                                                             N
                                                 p(w)=            p(w |z)p(z|d)p(d).
                                                         ∑∏∑ n
                                                          d n=1 z
                    Essentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).
                        Thismethodofinference,thoughtheoreticallysound,causesthemodeltooverﬁt. Thedocument-
                    speciﬁc topic distribution has some components which are close to zero for those topics that do not
                    appear in the document. Thus, certain words will have very small probability in the estimates of
                                                                 1011
                                                          BLEI,NG, ANDJORDAN
                     each mixture component. When determining the probability of a new document through marginal-
                     ization, only those training documents which exhibit a similar proportion of topics will contribute
                     to the likelihood. For a given training document’s topic proportions, any word which has small
                     probability in all the constituent topics will cause the perplexity to explode. As k gets larger, the
                     chance that a training document will exhibit topics that cover all the words in the new document
                     decreases and thus the perplexity grows. Note that pLSI does not overﬁt as quickly (with respect to
                     k) as the mixture of unigrams.
                         Thisoverﬁtting problem essentially stems from the restriction that each future document exhibit
                     the same topic proportions as were seen in one or more of the training documents. Given this
                     constraint, we are not free to choose the most likely proportions of topics for the new document. An
                     alternative approach is the “folding-in” heuristic suggested by Hofmann (1999), where one ignores
                     the p(z|d) parameters and reﬁts p(z|d   ). Note that this gives the pLSI model an unfair advantage
                                                          new
                     by allowing it to reﬁt k−1 parameters to the test data.
                         LDAsuffers from neither of these problems. As in pLSI, each document can exhibit a different
                     proportion of underlying topics. However, LDA can easily assign probability to a new document;
                     noheuristics are needed for a new document to be endowed with a different set of topic proportions
                     than were associated with documents in the training corpus.
                         Figure 9 presents the perplexity for each model on both corpora for different values of k. The
                     pLSI model and mixture of unigrams are suitably corrected for overﬁtting. The latent variable
                     models perform better than the simple unigram model. LDA consistently performs better than the
                     other models.
                     7.2 Documentclassiﬁcation
                     In the text classiﬁcation problem, we wish to classify a document into two or more mutually ex-
                     clusive classes. As in any classiﬁcation problem, we may wish to consider generative approaches
                     or discriminative approaches. In particular, by using one LDA module for each class, we obtain a
                     generative modelforclassiﬁcation. It is also of interest to use LDA in the discriminative framework,
                     and this is our focus in this section.
                         Achallenging aspect of the document classiﬁcation problem is the choice of features. Treating
                     individual words as features yields a rich but very large feature set (Joachims, 1999). One way to
                     reduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA
                     reduces any document to a ﬁxed set of real-valued features—the posterior Dirichlet parameters
                      ∗
                     γ (w) associated with the document. It is of interest to see how much discriminatory information
                     weloseinreducing the document description to these parameters.
                         We conducted two binary classiﬁcation experiments using the Reuters-21578 dataset. The
                     dataset contains 8000 documents and 15,818 words.
                         In these experiments, we estimated the parameters of an LDA model on all the documents,
                     without reference to their true class label. We then trained a support vector machine (SVM) on the
                     low-dimensional representations provided by LDA and compared this SVM to an SVM trained on
                     all the word features.
                         Using the SVMLight software package (Joachims, 1999), we compared an SVM trained on all
                     the word features with those trained on features induced by a 50-topic LDA model. Note that we
                     reduce the feature space by 99.6 percent in this case.
                                                                    1012
                                       LATENTDIRICHLETALLOCATION
                  95                                    98
                                                        97
                                                        96
                  90
                 Accuracy                              Accuracy95
                                      LDA Features      94                  LDA Features
                                      Word Features                         Word Features
                  85                                    93
                   0    0.05   0.1   0.15  0.2   0.25    0     0.05  0.1   0.15  0.2   0.25
                          Proportion of data used for training Proportion of data used for training
                                  (a)                                   (b)
               Figure 10: Classiﬁcation results on two binary classiﬁcation problems from the Reuters-21578
                        dataset for different proportions of training data. Graph (a) is EARNvs. NOT EARN.
                        Graph (b) is GRAINvs. NOT GRAIN.
                                   600               LDA
                                   550               Fold in pLSI
                                                     Smoothed Mixt. Unigrams
                                   500
                                   450
                                   400
                                   350
                                  Predictive Perplexity300
                                   250
                                   200
                                     0     10    20    30    40    50
                                                Number of Topics
                          Figure 11: Results for collaborative ﬁltering on the EachMovie data.
                  Figure 10 shows our results. We see that there is little reduction in classiﬁcation performance
               in using the LDA-based features; indeed, in almost all cases the performance is improved with the
               LDAfeatures. Although these results need further substantiation, they suggest that the topic-based
               representation provided by LDA may be useful as a fast ﬁltering algorithm for feature selection in
               text classiﬁcation.
                                                 1013
                                                         BLEI,NG, ANDJORDAN
                     7.3 Collaborative ﬁltering
                     Our ﬁnal experiment uses the EachMovie collaborative ﬁltering data. In this data set, a collection
                     of users indicates their preferred movie choices. A user and the movies chosen are analogous to a
                     document and the words in the document (respectively).
                        The collaborative ﬁltering task is as follows. We train a model on a fully observed set of users.
                     Then, for each unobserved user, we are shown all but one of the movies preferred by that user and
                     are asked to predict what the held-out movie is. The different algorithms are evaluated according to
                     the likelihood they assign to the held-out movie. More precisely, deﬁne the predictive perplexity on
                     Mtestusers to be:
                                                                         M logp(w       |w       ) 
                                                                        ∑            d,N   d,1:N −1
                                                        D )=exp − d=1                  d       d    )  .
                                   predictive-perplexity( test                       M
                        Werestricted the EachMovie dataset to users that positively rated at least 100 movies (a positive
                     rating is at least four out of ﬁve stars). We divided this set of users into 3300 training users and 390
                     testing users.
                        Underthemixtureofunigramsmodel,theprobabilityofamoviegivenasetofobservedmovies
                     is obtained from the posterior distribution over topics:
                                                     p(w|w    )= p(w|z)p(z|w ).
                                                           obs    ∑              obs
                                                                   z
                     In the pLSI model, the probability of a held-out movie is given by the same equation except that
                     p(z|w   ) is computed by folding in the previously seen movies. Finally, in the LDA model, the
                          obs
                     probability of a held-out movie is given by integrating over the posterior Dirichlet:
                                               p(w|w        Z
                                                        )=       p(w|z)p(z|θ)p(θ|w     )dθ,
                                                     obs      ∑                     obs
                                                               z
                     where p(θ|w    ) is given by the variational inference method described in Section 5.2. Note that
                                 obs
                     this quantity is efﬁcient to compute. We can interchange the sum and integral sign, and compute a
                     linear combination of k Dirichlet expectations.
                        With a vocabulary of 1600 movies, we ﬁnd the predictive perplexities illustrated in Figure 11.
                     Again, the mixture of unigrams model and pLSI are corrected for overﬁtting, but the best predictive
                     perplexities are obtained by the LDA model.
                     8. Discussion
                     Wehave described latent Dirichlet allocation, a ﬂexible generative probabilistic model for collec-
                     tions of discrete data. LDA is based on a simple exchangeability assumption for the words and
                     topics in a document; it is therefore realized by a straightforward application of de Finetti’s repre-
                     sentation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI,
                     but with proper underlying generative probabilistic semantics that make sense for the type of data
                     that it models.
                        Exact inference is intractable for LDA, but any of a large suite of approximate inference algo-
                     rithms can be used for inference and parameter estimation within the LDA framework. We have
                     presented a simple convexity-based variational approach for inference, showing that it yields a fast
                                                                   1014
                                       LATENTDIRICHLETALLOCATION
               algorithm resulting in reasonable comparative performance in terms of test set likelihood. Other
               approaches that might be considered include Laplace approximation, higher-order variational tech-
               niques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a
               general methodology for converting low-order variational lower bounds into higher-order varia-
               tional bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of
               maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential
               accuracy can be obtained for the LDA model via a higher-order variational technique known as ex-
               pectation propagation. Finally, Grifﬁths and Steyvers (2002) have presented a Markov chain Monte
               Carlo algorithm for LDA.
                  LDAis a simple model, and although we view it as a competitor to methods such as LSI and
               pLSI in the setting of dimensionality reduction for document collections and other discrete cor-
               pora, it is also intended to be illustrative of the way in which probabilistic models can be scaled
               up to provide useful inferential machinery in domains involving multiple levels of structure. In-
               deed, the principal advantages of generative models such as LDA include their modularity and their
               extensibility. As a probabilistic module, LDA can be readily embedded in a more complex model—
               a property that is not possessed by LSI. In recent work we have used pairs of LDA modules to
               model relationships between images and their corresponding descriptive captions (Blei and Jordan,
               2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily
               extended to continuous data or other non-multinomial data. As is the case for other mixture models,
               including ﬁnite mixture models and hidden Markov models, the “emission” probability p(w |z )
                                                                                   n n
               contributes only a likelihood value to the inference procedures for LDA, and other likelihoods are
               readily substituted in its place. In particular, it is straightforward to develop a continuous variant of
               LDAinwhich Gaussian observables are used in place of multinomials. Another simple extension
               of LDAcomesfromallowingmixtures of Dirichlet distributions in the place of the single Dirichlet
               of LDA. This allows a richer structure in the latent topic space and in particular allows a form of
               document clustering that is different from the clustering that is achieved via shared topics. Finally,
               a variety of extensions of LDA can be considered in which the distributions on the topic variables
               are elaborated. For example, we could arrange the topics in a time series, essentially relaxing the
               full exchangeability assumption to one of partial exchangeability. We could also consider partially
               exchangeable models in which we condition on exogenous variables; thus, for example, the topic
               distribution could be conditioned on features such as “paragraph” or “sentence,” providing a more
               powerful text model that makes use of information obtained from a parser.
               Acknowledgements
               This work was supported by the National Science Foundation (NSF grant IIS-9988642) and the
               Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637).
               Andrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft
               Corporation.
               References
               M. Abramowitz and I. Stegun, editors. Handbook of Mathematical Functions. Dover, New York,
                 1970.
                                                 1015
                                                       BLEI,NG, ANDJORDAN
                                                                   ´
                    D.Aldous. Exchangeability and related topics. In Ecole d’ete de probabilites de Saint-Flour, XIII—
                                                                          ´ ´             ´
                      1983, pages 1–198. Springer, Berlin, 1985.
                    H.Attias. A variational Bayesian framework for graphical models. In Advances in Neural Informa-
                      tion Processing Systems 12, 2000.
                    L.   Avery.       Caenorrhabditis   genetic   center  bibliography.       2002.         URL
                      http://elegans.swmed.edu/wli/cgcbib.
                    R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. ACM Press, New York, 1999.
                    D. Blei and M. Jordan. Modeling annotated data. Technical Report UCB//CSD-02-1202, U.C.
                      Berkeley Computer Science Division, 2002.
                    B. de Finetti. Theory of probability. Vol. 1-2. John Wiley & Sons Ltd., Chichester, 1990. Reprint
                      of the 1975 translation.
                    S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic
                      analysis. Journal of the American Society of Information Science, 41(6):391–407, 1990.
                    P. Diaconis. Recent progress on de Finetti’s notions of exchangeability. In Bayesian statistics, 3
                      (Valencia, 1987), pages 111–125. Oxford Univ. Press, New York, 1988.
                    J. Dickey. Multiple hypergeometric functions: Probabilistic interpretations and statistical uses.
                      Journal of the American Statistical Association, 78:628–637, 1983.
                    J. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. Journal of the
                      American Statistical Association, 82:773–781, 1987.
                    A. Gelman, J. Carlin, H. Stern, and D. Rubin. Bayesian data analysis. Chapman & Hall, London,
                      1995.
                    T. Grifﬁths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings
                      of the 24th Annual Conference of the Cognitive Science Society, 2002.
                    D. Harman. Overview of the ﬁrst text retrieval conference (TREC-1). In Proceedings of the First
                      Text Retrieval Conference (TREC-1), pages 1–20, 1992.
                    D. Heckerman and M. Meila. An experimental comparison of several clustering and initialization
                      methods. Machine Learning, 42:9–29, 2001.
                    T. Hofmann. Probabilistic latent semantic indexing. Proceedings of the Twenty-Second Annual
                      International SIGIR Conference, 1999.
                    F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1997.
                    T. Joachims. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support
                      Vector Learning. M.I.T. Press, 1999.
                    M.Jordan, editor. Learning in Graphical Models. MIT Press, Cambridge, MA, 1999.
                                                                1016
                        LATENTDIRICHLETALLOCATION
          M.Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graph-
           ical models. Machine Learning, 37:183–233, 1999.
          R. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical
           models(parametricempiricalBayesmodels). JournaloftheAmericanStatisticalAssociation,84
           (407):717–726, 1989.
          M. Leisink and H. Kappen. General lower bounds based on computer generated higher order ex-
           pansions. In Uncertainty in Artiﬁcial Intelligence, Proceedings of the Eighteenth Conference,
           2002.
          T. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.
          T. P. MinkaandJ.Lafferty. Expectation-propagationforthegenerativeaspectmodel. InUncertainty
           in Artiﬁcial Intelligence (UAI), 2002.
          C.Morris. ParametricempiricalBayesinference: Theoryandapplications. JournaloftheAmerican
           Statistical Association, 78(381):47–65, 1983. With discussion.
          K.Nigam,J.Lafferty, and A. McCallum. Using maximum entropy for text classiﬁcation. IJCAI-99
           Workshop on Machine Learning for Information Filtering, pages 61–67, 1999.
          K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled
           documents using EM. Machine Learning, 39(2/3):103–134, 2000.
          C. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A proba-
           bilistic analysis. pages 159–168, 1998.
          A.Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for uniﬁed collaborative
           and content-based recommendation in sparse-data environments. In Uncertainty in Artiﬁcial
           Intelligence, Proceedings of the Seventeenth Conference, 2001.
          J. Rennie. Improvingmulti-classtextclassiﬁcationwithnaiveBayes. TechnicalReportAITR-2001-
           004, M.I.T., 2001.
          G. Ronning. Maximum likelihood estimation of Dirichlet distributions. Journal of Statistcal Com-
           putation and Simulation, 34(4):215–221, 1989.
          G. Salton and M. McGill, editors. Introduction to Modern Information Retrieval. McGraw-Hill,
           1983.
          AppendixA. Inference and parameter estimation
          In this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter
          maximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by
          deriving a useful property of the Dirichlet distribution.
                              1017
                                                              BLEI,NG, ANDJORDAN
                      A.1 ComputingE[log(θi|α)]
                      The need to compute the expected value of the log of a single probability component under the
                      Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA.
                      This value can be easily computed from the natural parameterization of the exponential family
                      representation of the Dirichlet distribution.
                           Recall that a distribution is in the exponential family if it can be written in the form:
                                                                            T              	
                                                       p(x|η)=h(x)exp η T(x)−A(η) ,
                      where η is the natural parameter, T(x) is the sufﬁcient statistic, and A(η) is the log of the normal-
                      ization factor.
                           WecanwritetheDirichlet in this form by exponentiating the log of Eq. (1):
                                                     k                             k           k            	
                                    p(θ|α)=exp        ∑ (α −1)logθ +logΓ ∑                α −∑ logΓ(α) .
                                                        i=1   i          i            i=1   i     i=1         i
                      From this form, we immediately see that the natural parameter of the Dirichlet is η = α −1 and
                                                                                                                i     i
                      the sufﬁcient statistic is T(θi)=logθi. Furthermore, using the general fact that the derivative of
                      the log normalization factor with respect to the natural parameter is equal to the expectation of the
                      sufﬁcient statistic, we obtain:
                                                        E[logθ |α]=Ψ(α)−Ψ  k α 
                                                                i           i       ∑j=1 j
                      where Ψ is the digamma function, the ﬁrst derivative of the log Gamma function.
                      A.2 Newton-RaphsonmethodsforaHessianwithspecialstructure
                      In this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization
                      method. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ron-
                      ning, 1989, Minka, 2000).
                           TheNewton-Raphsonoptimization technique ﬁnds a stationary point of a function by iterating:
                                                          α                       −1
                                                                =α −H(α ) g(α )
                                                            new     old       old       old
                      where H(α) and g(α) are the Hessian matrix and gradient respectively at the point α. In general,
                      this algorithm scales as O(N3) due to the matrix inversion.
                           If the Hessian matrix is of the form:
                                                                                    T
                                                                H=diag(h)+1z1 ,                                            (10)
                      wherediag(h)isdeﬁnedtobeadiagonalmatrixwiththeelementsofthevectorhalongthediagonal,
                      then we can apply the matrix inversion lemma and obtain:
                                                                                 −1   T         −1
                                                      −1           −1    diag(h)    11 diag(h)
                                                    H =diag(h) −              z−1+ k h−1
                                                                                    ∑j=1 j
                      Multiplying by the gradient, we obtain the ith component:
                                                                              g −c
                                                                  (H−1g) = i
                                                                          i     h
                                                                                 i
                                                                         1018
                                       LATENTDIRICHLETALLOCATION
               where
                                                  k  g /h
                                            c= ∑j=1 j j .
                                                −1   k  −1
                                               z  +∑j=1hj
                Observe that this expression depends only on the 2k values h and g and thus yields a Newton-
                                                             i     i
                Raphson algorithm that has linear time complexity.
                A.3 Variational inference
                In this section we derive the variational inference algorithm described in Section 5.1. Recall that
                this involves using the following variational distribution:
                                                       N
                                       q(θ,z|γ,φ)=q(θ|γ)∏q(zn|φn)                   (11)
                                                       n=1
               as a surrogate for the posterior distribution p(θ,z,w|α,β), where the variational parameters γ and
               φare set via an optimization procedure that we now describe.
                  Following Jordan et al. (1999), we begin by bounding the log likelihood of a document using
               Jensen’s inequality. Omitting the parameters γ and φ for simplicity, we have:
                      logp(w|α,β)=logZ ∑p(θ,z,w|α,β)dθ
                                          z
                                  = logZ ∑p(θ,z,w|α,β)q(θ,z)dθ
                                          z      q(θ,z)
                                  ≥ Z ∑q(θ,z)logp(θ,z,w|α,β)dθ−Z ∑q(θ,z)logq(θ,z)dθ
                                       z                         z
                                  = E [logp(θ,z,w|α,β)]−E [logq(θ,z)].              (12)
                                      q                 q
               Thus we see that Jensen’s inequality provides us with a lower bound on the log likelihood for an
               arbitrary variational distribution q(θ,z|γ,φ).
                  It can be easily veriﬁed that the difference between the left-hand side and the right-hand side
                of the Eq. (12) is the KL divergence between the variational posterior probability and the true
                posterior probability. That is, letting L(γ,φ;α,β) denote the right-hand side of Eq. (12) (where we
               have restored the dependence on the variational parameters γ and φ in our notation), we have:
                            logp(w|α,β)=L(γ,φ;α,β)+D(q(θ,z|γ,φ)k p(θ,z|w,α,β)).     (13)
               This shows that maximizing the lower bound L(γ,φ;α,β) with respect to γ and φ is equivalent to
               minimizing the KL divergence between the variational posterior probability and the true posterior
               probability, the optimization problem presented earlier in Eq. (5).
                  Wenowexpandthelowerboundbyusingthefactorizations of p and q:
                           L(γ,φ;α,β)=E [logp(θ|α)]+E [logp(z|θ)]+E [logp(w|z,β)]
                                       q            q           q                   (14)
                                    −E [logq(θ)]−E [logq(z)].
                                       q         q
                                                  1019
                                                             BLEI,NG, ANDJORDAN
                      Finally, we expand Eq. (14) in terms of the model parameters (α,β) and the variational parameters
                      (γ,φ). Each of the ﬁve lines below expands one of the ﬁve terms in the bound:
                                                                  k              k
                                                      k                                                  k      
                               L(γ,φ;α,β)=logΓ ∑          α − logΓ(α)+ (α −1) Ψ(γ)−Ψ ∑                         γ
                                                       j=1  j    ∑          i   ∑ i              i         j=1 j
                                                                 i=1            i=1
                                               N k
                                                                      k      
                                           +         φ   Ψ(γ)−Ψ ∑          γ
                                              ∑∑ ni          i         j=1 j
                                              n=1i=1
                                               N k V         j
                                           +            φ w logβ                                                        (15)
                                              ∑∑∑ ni n             ij
                                              n=1i=1 j=1
                                                                 k              k
                                                     k                                                 k     
                                           −logΓ ∑        γ   + logΓ(γ)− (γ −1) Ψ(γ)−Ψ ∑                    γ
                                                       j=1 j    ∑         i    ∑ i             i         j=1 j
                                                                i=1            i=1
                                               N k
                                           −∑∑φnilogφni,
                                              n=1i=1
                      where we have made use of Eq. (8).
                          In the following two sections, we show how to maximize this lower bound with respect to the
                      variational parameters φ and γ.
                      A.3.1 V
                               ARIATIONALMULTINOMIAL
                      We ﬁrst maximize Eq. (15) with respect to φni, the probability that the nth word is generated by
                      latent topic i. Observe that this is a constrained maximization since   k  φ =1.
                                                                                            ∑     ni
                                                                                              i=1
                          We form the Lagrangian by isolating the terms which contain φ         and adding the appropriate
                                                                                             ni
                                                             v       i
                      Lagrange multipliers. Let β     be p(w =1|z =1) for the appropriate v. (Recall that each w is
                                                   iv        n                                                           n
                      a vector of size V with exactly one component equal to one; we can select the unique v such that
                        v
                      w =1):
                        n
                                                          k                                      k           
                                L     =φ Ψ(γ)−Ψ ∑ γ +φ logβ −φ logφ +λ ∑                                φ −1 ,
                                  [φ ]    ni     i         j=1 j       ni     iv    ni    ni    n    j=1 ni
                                    ni
                      where we have dropped the arguments of L for simplicity, and where the subscript φ        denotes that
                                                                                                             ni
                      wehaveretained only those terms in L that are a function of φ . Taking derivatives with respect to
                                                                                      ni
                      φni, we obtain:
                                              ∂L =Ψ(γ)−Ψ  k γ +logβ −logφ −1+λ.
                                             ∂φ          i       ∑j=1 j          iv        ni
                                                ni
                      Settingthisderivativetozeroyieldsthemaximizingvalueofthevariationalparameterφ (cf.Eq.6):
                                                                                                               ni
                                                                                  k     
                                                      φ ∝βexp Ψ(γ)−Ψ ∑                γ    .                            (16)
                                                        ni    iv         i         j=1 j
                                                                       1020
                                                                                           LATENTDIRICHLETALLOCATION
                                    A.3.2 VARIATIONALDIRICHLET
                                    Next, we maximize Eq. (15) with respect to γ , the ith component of the posterior Dirichlet param-
                                                                                                                 i
                                    eter. The terms containing γ are:
                                                                                   i
                                                                        k                                                            N
                                                                                                               k                                                   k           
                                                           L = (α−1) Ψ(γ)−Ψ ∑ γ +                                                        φ       Ψ(γ)−Ψ ∑                     γ
                                                             [γ]      ∑ i                         i               j=1 j             ∑ ni               i                j=1 j
                                                                      i=1                                                          n=1
                                                                                                                             k
                                                                                      k                                                                           k            
                                                                       −logΓ ∑                 γ     +logΓ(γ)−                  (γ −1) Ψ(γ)−Ψ ∑                             γ       .
                                                                                         j=1 j                      i       ∑ i                       i                j=1 j
                                                                                                                            i=1
                                    This simpliﬁes to:
                                                                k
                                                   L =  Ψ(γ)−Ψ  k γ  α + N φ −γ−logΓ  k γ +logΓ(γ).
                                                     [γ]      ∑             i            ∑j=1 j               i     ∑          ni       i                  ∑j=1 j                        i
                                                                                                                       n=1
                                                              i=1
                                    Wetakethederivative with respect to γ :
                                                                                                      i
                                                                                                                                              k
                                                          ∂L            ′                    N                          ′      k                             N                   
                                                                 =Ψ(γ) α +∑                        φ −γ −Ψ ∑ γ                                      α +∑              φ −γ .
                                                          ∂γ                 i       i       n=1 ni            i                j=1 j ∑                j       n=1 nj             j
                                                              i                                                                             j=1
                                    Setting this equation to zero yields a maximum at:
                                                                                                        γ =α + N φ .                                                                                 (17)
                                                                                                          i        i     ∑          ni
                                                                                                                            n=1
                                          Since Eq. (17) depends on the variational multinomial φ, full variational inference requires
                                    alternating between Eqs. (16) and (17) until the bound converges.
                                    A.4 Parameterestimation
                                    In this ﬁnal section, we consider the problem of obtaining empirical Bayes estimates of the model
                                    parameters α and β. We solve this problem by using the variational lower bound as a surrogate
                                    for the (intractable) marginal log likelihood, with the variational parameters φ and γ ﬁxed to the
                                    values found by variational inference. We then obtain (approximate) empirical Bayes estimates by
                                    maximizing this lower bound with respect to the model parameters.
                                          Wehave thus far considered the log likelihood for a single document. Given our assumption
                                    of exchangeability for the documents, the overall log likelihood of a corpus D = {w ,w ,...w, }
                                                                                                                                                                                   1      2            M
                                    is the sum of the log likelihoods for individual documents; moreover, the overall variational lower
                                    bound is the sum of the individual variational bounds. In the remainder of this section, we abuse
                                    notation by using L for the total variational bound, indexing the document-speciﬁc terms in the
                                    individual bounds by d, and summing over all the documents.
                                          Recall from Section 5.3 that our overall approach to ﬁnding empirical Bayes estimates is based
                                    on a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize
                                    the bound L(γ,φ;α,β) with respect to the variational parameters γ and φ. In the M-step, which we
                                    describe in this section, we maximize the bound with respect to the model parameters α and β. The
                                    overall procedure can thus be viewed as coordinate ascent in L.
                                                                                                                    1021
                                                         BLEI,NG, ANDJORDAN
                     A.4.1 CONDITIONALMULTINOMIALS
                     Tomaximizewithrespect to β, we isolate terms and add Lagrange multipliers:
                                                M N k V                       k               
                                        L =         d       φ wj logβ +         λ    V   β −1 .
                                          [β]  ∑∑∑∑ dni dn               ij  ∑ i ∑j=1 ij
                                               d=1n=1i=1j=1                  i=1
                     Wetakethederivative with respect to β , set it to zero, and ﬁnd:
                                                           ij
                                                                 M N
                                                           β ∝        d φ  wj .
                                                            ij   ∑∑ dni dn
                                                                d=1n=1
                     A.4.2 DIRICHLET
                     Thetermswhichcontain α are:
                                 M          k          k             k                         k      !
                         L =         logΓ ∑      α − logΓ(α)+             (α −1) Ψ(γ )−Ψ ∑          γ
                           [α]  ∑             j=1  j   ∑         i    ∑ i              di        j=1 dj
                                d=1                    i=1           i=1
                     Taking the derivative with respect to α gives:
                                                          i
                                                                         M
                                      ∂L           k                                    k       
                                          =M Ψ ∑ αj −Ψ(αi) +                 Ψ(γ )−Ψ ∑         γ
                                      ∂α             j=1                 ∑       di         j=1 dj
                                         i                               d=1
                     This derivative depends on αj, where j 6= i, and we therefore must use an iterative method to ﬁnd
                     the maximal α. In particular, the Hessian is in the form found in Eq. (10):
                                                  ∂L =δ(i,j)MΨ′(α)−Ψ′  k α ,
                                                 ∂αα                   i       ∑j=1 j
                                                    i j
                     and thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.
                        Finally, note that we can use the same algorithm to ﬁnd an empirical Bayes point estimate of η,
                     the scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4.
                                                                   1022
