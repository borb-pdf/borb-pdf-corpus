                IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                     1
                              The Graph Neural Network Model
                                 Franco Scarselli, Marco Gori, Ah Chung Tsoi,     Gabriele Monfardini
                                                                Abstract
                        Many underlying relationships among data in several areas of science and engineering, e.g. computer vision,
                     molecular chemistry, molecular biology, pattern recognition, data mining, can be represented in terms of graphs. In this
                     paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural
                     network methods for processing the data represented in the graph domain. This GNN model, which can directly process
                     most of the practically useful types of graphs, e.g. acyclic, cyclic, directed, un-directed, implements a transduction
                     function ¿(G,n) ∈ IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A
                     supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. Computational cost
                     of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning
                     algorithm, and demonstrate its generalization capability.
                                                              Index Terms
                        Graph Neural Networks, Graph Processing, Recursive Neural Networks, Graphical Domains.
                                                           I. INTRODUCTION
                  Data can be naturally represented by graph structures in several application areas including proteomics [1], pattern
                recognition and image analysis [2], scene description [3], [4], software engineering [5], [6] and natural language
                processing [7]. The simplest kinds of graph structures include single nodes, and sequences. But in several application
                domains, the information is organized in more complex graph structures such as trees, acyclic graphs, or cyclic
                graphs. Traditionally, the exploitation of data relationships has been the subject of many studies in the community
                of inductive logic programming and, recently, this research theme has been evolving in different directions [8], also
                because of the marriage with statistics and neural networks (see e.g. the recent workshops [9], [10], [11], [12]).
                  In machine learning, the structured data is often associated with the goal of (supervised or unsupervised) learning
                from examples a function ¿ that maps a graph G and one of its nodes n to a vector of reals1: ¿(G,n) ∈ IRm.
                  Applications to a graphical domain can generally be divided into two classes: graph focused and node focused
                applications respectively in this paper.
                  Scarselli, Gori, Monfardini are with the University of Siena, Siena, Italy. Email: {franco,marco,monfardini}@dii.unisi.it.
                  Tsoi is with Hong Kong Baptist University, Kowloon, Hong Kong. Email: act@hkbu.edu.hk
                  Hagenbuchner is with University of Wollongong, Wollongong, Australia. Email: markus@uow.edu.au
                  1Note that in most classiﬁcation problems, the mapping is to a set of integers INm, while in regression problems, the mapping is to a set of
                reals IRm. Here for simplicity of exposition, we will denote only the regression case. The proposed formulation can be trivially re-written for
                the situation of classiﬁcation.
                May 24, 2007                                                                                      DRAFT
                         IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                             2
                            In graph focused applications, the function ¿ is independent of the node n and implements a classiﬁer or a
                         regressor on a graph structured dataset. For example, a chemical compound can be modeled by a graph G, the
                         nodes of which stand for atoms and the edges of which represent chemical bonds (see Figure 1-A) linking some of
                         the atoms together. The mapping ¿(G) may be used to estimate the probability that the chemical compound is active
                         against a certain disease. In Figure 1-B, an image can be represented by a region adjacency graph (RAG) where
                         nodes denote homogeneous regions of intensity of the image and arcs represent their adjacency relationship [13].
                         In this case, ¿(G) may be used to classify the image into different classes according to its contents, e.g. castles,
                         cars, people, and so on.
                            In node focused applications, ¿ depends on the node n, so that the classiﬁcation (or the regression) depends
                         on the properties of each node. Object detection is an example of this class of applications. It consists of ﬁnding
                         whether an image contains a given object or not, and, if so, localizing its position. This problem can be solved by
                         a function ¿ which classiﬁes the nodes of the RAG according to whether the corresponding region belongs to the
                         object or not. For example, the output of ¿ for Figure 1-B might be 1 for the black nodes, which corresponds to
                         the castle, and 0 otherwise. Another example comes from web page classiﬁcations. The web can be represented by
                         a graph where nodes stand for pages and edges represent the hyperlinks between the web pages (Figure 1-C). The
                         web connectivity can be exploited, along with page contents, for several purposes, e.g. classifying the pages into a
                         set of topics.
                                                                       OH
                                                                                      H
                                                                                      N
                                     HO
                                     HO
                                                                    A                                                                    B
                                                                                                  www.dii.unisi.it/~franco           www.uow.edu.au/~markus
                                               www.dii.unisi.it       www.dii.unisi.it/people
                                                                                                    www.dii.unisi.it/~marco              www.uow.edu.au/~act
                                                                                                       C
                         Fig. 1.   Some applications where the information is represented by graphs: A is a chemical compound (adrenaline); B an image; and C a subset
                         of the web.
                         May 24, 2007                                                                                                                                           DRAFT
                 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                            3
                    Traditional machine learning applications cope with graph structured data by using a preprocessing phase which
                 maps the graph structured information to a simpler representation, e.g. vectors of reals. In other words, the
                 preprocessing step ﬁrst “squashes” the graph structured data into a vector of reals and then deal with the preprocessed
                 data using a list based data processing technique. However, important information, e.g., the topological dependency
                 of information on node n may be lost during the preprocessing stage and the ﬁnal result may depend, in an
                 unpredictable manner, on the details of the preprocessing algorithm. More recently, there are various approaches
                 [14], [15] attempting to preserve the graph structured nature of the data for as long as required before processing
                 the data. In these recent approaches the idea is to encode the underlying graph structured data using the topological
                 relationships among the nodes of the graph to incorporate the graph structured information in the data processing
                 step. Recursive neural networks [14], [16], [17] and Markov chains [15], [18], [19] belong to this set of techniques
                 and are commonly applied both to graph and node focused problems. Our method extends these two approaches
                 in that it can deal directly with graph structured information.
                    Existing recursive neural networks are neural network models whose input domain consists of directed acyclic
                 graphs [14], [16], [17]. The method estimates the parameters ϕ   of a function which maps a graph to a vector
                                                                               w
                 of reals. The approach can also be used for node focused applications, but in this case, the graph must undergo
                 a preprocessing phase [20]. Similarly, using a preprocessing phase, it is possible to handle certain types of cyclic
                 graphs [21]. Recursive neural networks have been applied to several problems including logical term classiﬁcation [22],
                 chemical compound classiﬁcation [23], logo recognition [2], [24], web Page scoring [25], and face localization [26].
                    Recursive neural networks are also related to support vector machines  [27], [28], [29], which adopt special
                 kernels to operate on graph structured data. For example, the diffusion kernel [30] is based on a heat equation; the
                 kernels proposed in [31], [32] exploit the vectors produced by a graph random walker and those designed in [33],
                 [34], [35] use a method of counting the number of common substructures of two trees. In fact, recursive neural
                 networks, as support vector machine methods, automatically encode the input graph into an internal representation.
                 However, in recursive neural networks, the internal encoding is learned, while in support vector machine it is
                 designed by the user.
                    On the other hand, Markov chain models can emulate processes where the causal connections among events are
                 represented by graphs. Recently, random walk theory, which addresses a particular class of Markov chain models,
                 has been applied with some success to the realization of web page ranking algorithms [15], [18]. Internet search
                 engines use ranking algorithms to measure the relative “importance” of web pages. Such measurements are generally
                 exploited, along other page features, by “horizontal” search engines, e.g., Google [15], or by personalized search
                 engines (“vertical” search engines, see, e.g., [19]) to sort the universal resource locators (URLs) returned on user
                        2
                 queries . Some attempts have been made to extend these models with learning capabilities such that a parametric
                 model representing the behavior of the system can be estimated from training examples [19], [37], [38]. Such
                 models are able to generalize the results to score all the web pages in the collection.
                   2The relative importance measure of a web page is also used to serve other goals, e.g. to improve the efﬁciency of crawlers [36].
                 May 24, 2007                                                                                            DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                           4
                      In this paper, we present a new neural network model which is suitable for both graph and node focused
                    applications. This new model uniﬁes these two existing models into a common framework. We will call this new
                    neural network model a graph neural network (GNN). It will be shown that the GNN is an extension of both
                    recursive neural networks and random walk models and that it retains their characteristics.
                      The model extends recursive neural networks since it can process a more general class of graphs including cyclic,
                    directed and undirected graphs, and to deal with node focused applications without any preprocessing steps. The
                    approach extends random walk theory by the introduction of a learning algorithm and by enlarging the class of
                    processes that can be modeled.
                      In this paper a learning algorithm will be introduced which estimates the parameters of the GNN model on a
                    set of given training examples. In addition, the computational cost of the parameter estimation algorithm will be
                    considered. It is also worth to mention that elsewhere it is proved that GNNs show a sort of universal approximation
                                                                                                                                          3
                    property and, under mild conditions, they can approximate most of the practically useful functions ϕ on graphs [39].
                      The structure of this paper is as follows: after a brief description of the notation used in this paper as well
                    as some preliminary deﬁnitions, Section II presents the concept of a graph neural network model, together with
                    a learning algorithm for the parameter estimation. Moreover, Section III discusses the computational cost of the
                    learning algorithm. Some experimental results are presented in Section IV. Conclusions are drawn in Section V.
                                                         II. THE GRAPH NEURAL NETWORK MODEL
                      We begin by introducing some notations that will be used throughout the paper. A graph G is a pair (N,E),
                    where N is the set of nodes and E is the set of edges. The set ne[n] stands for the neighbours of n, i.e. the
                    nodes connected to n by an arc, while co[n] denotes the set of arcs having n as a vertex. Nodes and edges may
                    have labels represented by real vectors. The labels attached to node n and edge (n ,n ) will be represented by
                                                                                                                 1   2
                    l  ∈IRlN and l           ∈IRlE respectively. Let l denote the vector obtained by stacking together all the labels of
                     n               (n ,n )
                                        1  2
                    the graph. The notation adopted for labels follows a more general scheme: if y is a vector that contains data from
                    a graph and S is subset of the nodes (the edges), then yS denotes the vector obtained by selecting from y the
                    components related to the node (the edges) in S. For example, lne[n] stands for the vector containing the labels of
                    all the neighbours of n. Labels usually include features of objects related to nodes and features of the relationships
                    between the objects. For example, in the case of Figure 1-b, in an image, node labels might represent properties of
                    the regions (e.g., area, perimeter, average color intensity), while edge labels might represent the relative position
                    of the regions (e.g. the distance between their barycenters and the angle between the momentums). No assumption
                    is made on the arcs, directed and undirected edges are both permitted. However, when different kinds of edges
                    co-exist in the same dataset, it is necessary to distinguish them. This can be easily achieved by attaching a proper
                    label to each edge. In this case, different kinds of arcs turn out to be just arcs with different labels.
                      The considered graphs may be either positional or non–positional. Non–positional graphs are those described so
                    far, positional graphs differs since for each node n, there exists an injective function ºn : ne[n] → {1,...,|N|}
                      3Due to the length of proof, such a result cannot be proved here and is included [39].
                    May 24, 2007                                                                                                           DRAFT
                          IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                    5
                          which assigns to each neighbour of n a different position. Notice that, the position of the neighbour can be implicitly
                          used for storing useful information. For instance, let us consider the example of the Region Adjacency Graph (RAG),
                          (see Figure 1-b). ºn can be used to represent the relative spatial position of the regions; e.g., ºn might enumerate
                          the neighbours of a node n, which represents the adjacent regions, following a clockwise ordering.
                              The domain considered in this paper is the set D of pairs graph–node, i.e. D = G £ N where G is a set of
                          the graphs and N is a subset of their nodes. We assume a supervised learning framework with the learning set
                          L = {(G ,n ,t )|,G = (N ,E ) ∈ G;n                                       ∈ N ;t          ∈ IRm,1 · i · p,1 · j · q }, where n                                ∈ N
                                        i    i,j    i,j       i           i     i             i,j         i   i,j                                                i                i,j          i
                          denotes the j-the node in the set N ∈ N; (G ,n                                , t   ) indicates that t           is the target for node n               (of graph
                                                                               i               i    i,j    i,j                         i,j                                    i,j
                          G), p · |G| and q · |N |. Interestingly, all the graphs of the learning set can be combined into a unique
                             i                          i           i
                          disconnected graph, and, therefore, one might think of the learning set as the pair L = (G,T ) where G = (N,E)
                          is a graph and T a is set of pairs {(n ,t )|n ∈ N, t ∈ IRm,1 · i · q}. It is worth mentioning that this compact
                                                                                i    i     i           i
                          deﬁnition is not only useful for its simplicity, but that it also captures directly the very nature of some problems
                          where the domain consists of only one graph, for instance, a large portion of the Internet (see Figure 1-c).
                          A. The model
                              The intuitive idea underlining the proposed approach is that nodes in a graph represent objects or concepts, and
                          edges represent their relationships. Each concept is naturally deﬁned by its features and the related concepts. Thus,
                          we can attach a state x               ∈ IRs to each node n, that is based on the information contained in the neighborhood
                                                             n
                          of n (see Figure 2). The variable x                     contains a representation of the concept denoted by n and can be used to
                                                                               n
                          produce an output o , i.e. a decision about the concept.
                                                        n
                                                                                                                    x
                                                                                                                     5
                                                                                                        l(4,5)       l            l(5,7)          x
                                                                                                                      5                             7
                                                                                                                                                l
                                                                                               l                                                 7
                                                                                                4 x
                                                                                                   4                    l(5,6)
                                                                                                  l(1,4)                              l(6,7)
                                                                          x                                          x
                                                                           3        l               x       l         6 l
                                                                         l           (3,1)           1       (6,1)       6
                                                                          3                       l
                                                                                                   1
                                                                                                   l                              l(6,8)
                                                                                                    (1,2)
                                                                                                                                          x
                                                                                                                                           8
                                                                                              x
                                                                                               2                                         l
                                                                                                 l                                        8
                                                                                                  2
                                                                        x = f (l ,l        , l    , l    , l   , x , x , x , x , l , l , l , l )
                                                                         1    w 1 (1,2) (3,1)       (1,4)  (6,1)  2   3   4   6   2  3   4   6
                                                                                               l co[1]                x ne[1]       l ne[n]
                          Fig. 2.    The variable x depends on the information in the neighborhood of node 1.
                                                      1
                          May 24, 2007                                                                                                                                                  DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                          6
                      Let fw be a parametric function, called local transition function, that expresses the dependence of a node n on
                    its neighborhood and let g     be the local output function that describes how the output is produced. Then, x          and
                                                w                                                                                         n
                    o are deﬁned as follows
                     n
                                                              xn = fw(ln,l            , x     , l    )
                                                                                  co[n]  ne[n]  ne[n]
                                                              on = gw(xn,ln),                                                                (1)
                    where ln, lco[n], xne[n], lne[n] are respectively the label of n, the labels of its edges, the states and the labels of the
                    nodes in the neighborhood of n.
                      Remark 1: Different notions of neighborhood can be adopted. For example, one may wish to remove the labels
                    l    , since they include information that is implicitly contained also in x         . Moreover, the neighborhood could
                     ne[n]                                                                           ne[n]
                    contain nodes that are two or more links from n. In general, Eq. (1) could be simpliﬁed in several different ways
                                                                      4
                    and we could obtain a set of minimal models . In the following, the discussion will mainly be based on the form
                    deﬁned by Eq. (1), which is not minimal, but it is the one that more closely represents our intuitive notion of
                    neighborhood.
                      Remark 2: Equation (1) is customized for undirected graphs. When dealing with directed graphs, the function
                    fw can also accept as input a representation of the direction of the arcs. For example, fw may take as input a
                    variable d for each arc ℓ ∈ co[n] such that d = 1, if ℓ is directed towards n and d = 0, if ℓ comes from
                               ℓ                                        ℓ                                            ℓ
                    n. In the following, in order to keep the notations compact, we maintain the customization of Eq. (1). However,
                    unless explicitly stated, all the results stated in this paper hold also for directed graphs and for mixed directed and
                    undirected links.
                      Remark 3: In general the transition and the output functions and their parameters may depend on the node n.
                    In fact, it is plausible that different mechanisms (implementations) are used to represent different kinds of objects.
                                                                                               k                     k
                    In this case, each kind of nodes k     has its own transition function f n, output function g n and set of parameters
                                                         n
                    w . Thus, Eqs. (1) becomes x = (fkn)               (l ,l     , x     , l    ) and o = (gkn)         (x ,l ). However, for
                      k                                n          w      n co[n]    ne[n]  ne[n]        n          w      n n
                       n                                            k                                                k
                                                                     n                                                n
                    the sake of simplicity our analysis will consider Eq. (1) that describes a particular model where all the nodes share
                    the same implementation.
                      Let x, o, l and l     be the vectors constructed by stacking all the variable x , all the outputs, all the labels, and
                                         N                                                               n
                    all the node labels, respectively. Eq. (1) can be re-written in a compact form as:
                                                                       x = Fw(x,l)                                                           (2)
                                                                        o = Gw(x,lN)
                    where F , the global transition function and G , the global output function are stacked versions of |N| instances
                             w                                         w
                    of fw and gw, respectively.
                      We are interested in the case when x,o are uniquely deﬁned and Eq. (2) deﬁnes a map ϕ                 : D → IRm which
                                                                                                                          w
                    takes a graph input and returns an output on for each node. The Banach Fixed Point theorem [40] provides a
                      4By minimal, we address a model that has the smallest number of variables while retaining the computational power.
                    May 24, 2007                                                                                                          DRAFT
                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                7
                  sufﬁcient condition for the existence and uniqueness of the solution of a system of equations. According to the
                  Banach theorem, Eq. (2) has a unique solution provided that Fw is a contraction map w.r.t. the state, i.e. there
                  exists ¹, 0 < ¹ < 1, such that kFw(x,l) ¡ Fw(y,l)k · ¹kx ¡ yk holds for any x,y, where k ¢ k denotes a
                  vectorial norm. Thus, for the moment, let us assume that Fw is a contraction mapping. Later, we will show that,
                  in GNNs, this property is enforced by an appropriate implementation of the transition function.
                    Note that Eq. (1) makes it possible to process both positional and non–positional graphs. For positional graphs,
                  fw must receive the positions of the neighbors as additional inputs. In practice, this can be easily achieved provided
                  that information contained in x    , l    , l    are sorted according to neighbor positions and is properly padded
                                                 ne[n]  co[n] ne[n]
                  with special null values in positions corresponding to non–existing neighbors. For example, xne[n] = [y1,...,yM],
                  where M = maxn,u ºn(u) is the maximal number of neighbors of the node n; yi = xu holds, if u is the i–the
                  neighbor of n (º (u) = i); and y = x , for some predeﬁned null state x , if there is no i–the neighbor.
                                   n                i    0                                  0
                    However, for non-positional graphs it is useful to replace function fw of Eq. (1) with
                                                     x = X h (l ,l             , x ,l ), n ∈ N ,                                 (3)
                                                      n            w n (n,u)     u u
                                                           u∈ne[n]
                  where hw is a parametric function. This transition function, which has been successfully used in recursive neural
                  networks [41], is not affected by the positions and the number of the children. In the following, Eq. (3) is referred
                  to as the Non–positional Form, while Eq. (1) is called the Positional Form.
                    In order to implement the GNN model, the following items must be provided:
                    (1)    Amethod to solve Eq. (1);
                    (2)    Alearning algorithm to adapt f     and g   using examples from the training data set5;
                                                           w       w
                    (3)    An implementation of fw and gw.
                    These aspects will be considered in turn in the following subsections.
                  B. Computation of the state
                    Banach’s Fixed Point theorem suggests the following classic iterative scheme for computing the state:
                                                               x(t+1)=F (x(t),l)                                                 (4)
                                                                             w
                  where x(t) denotes the t-the iteration of x. The dynamic system (4) converges exponentially fast to the solution
                  of Eq. (2) for any initial value x(0). We can therefore think of x(t) as the state that is updated by the transition
                  function F . In fact, Eq. (4) implements the Jacobi iterative method for solving non–linear equations [42]. Thus,
                             w
                  the outputs and the states can be computed by iterating
                                                x (t+1) = f (l ,l            , x    (t),l    ),
                                                  n               w n co[n]     ne[n]    ne[n]                                   (5)
                                                     o (t)  = g (x (t),l ),                   n∈N.
                                                      n           w n       n
                    5In other words, the parameters w are estimated using examples contained in the training data set.
                  May 24, 2007                                                                                                DRAFT
                                   IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                                                                               8
                                        Note that the computation described in Eq. (5) can be interpreted as the representation of a network consisting
                                   of units which compute f                                  and g . Such a network will be called an encoding network, following an analog
                                                                                        w                w
                                   terminology used for the recursive neural network model [14]. In order to build the encoding network, each node
                                   of the graph can be replaced by a unit computing the function fw (see Figure 3). Each unit stores the current state
                                   x (t) of node n, and, when activated, it calculates the state x (t + 1) using the node label and the information
                                      n                                                                                                                          n
                                   stored in the neighborhood. The simultaneous and repeated activation of the units produce the behavior described
                                   by Eq. (5). The output for node n is produced by another unit which implements gw.
                                                                                                                                            o2(t)
                                                                                                                                 l 2         gw
                                                                 l                                                    l 1              x (t)                                           l 3
                                                      l           2                                                                     2              fw          l 2 ,l (2,3) ,..
                                                        (1,2)                                                                  x (t)
                                                                                                       o (t)         g          1                                                      g            o (t)
                                                                   l (2,3)                               1              w                         x (t)              x (t)               w           3
                                                      l1                     l3                                                                    2                  3
                                                                                                          l 1 ,l (1,2) ,..       fw       x (t)              x (t)        fw            l 3 ,l (4,3) ,..
                                                      l (1,4)             l (4,3)                                                           1                  3
                                                                 l4                                                                               x                                                                                    ....
                                                                                                                                                   4(t)
                                                                                                                                           x (t)                                                                                        ....
                                                                                                                                             4         fw          l 4 , l (1,4) ,..           gw
                                                                                                                                  gw          l 4
                                                                                                                                                                                                                                       ....
                                                                                                                                o (t)
                                                                                                                                  4
                                                                                                                                                                                               fw                                       ....
                                   Fig. 3.       Agraph and its corresponding encoding network. When f                                           and g        are implemented by static neural networks, the encoding network
                                                                                                                                            w             w
                                   is a recurrent neural network.
                                        When f               and g           are implemented by static neural networks, the encoding network turns out to be a recurrent
                                                        w               w
                                   neural network where the connections between the neurons can be divided into internal and external connections.
                                   The internal connectivity is determined by the neural network architecture used to implement the unit. The external
                                   connectivity is dependent on the edges of the processed graph.
                                   C. The learning algorithm
                                        Learning in GNNs consists of estimating the parameter w such that ϕ                                                                                approximates the data in the learning
                                                                                                                                                                                     w
                                   data set L = {(G ,n                           , t      )|, G = (N ,E ) ∈ G;n                                   ∈N;t ∈IRm,1·i·p,1·j·q},whereq is the
                                                                      i      i,j      i,j           i              i       i                i,j             i     i,j                                                           i                      i
                                   number of supervised nodes in G . For graph-focused tasks one special node is used for the target (q = 1 holds),
                                                                                                      i                                                                                                                                     i
                                   whereas for node-focused tasks, in principle, the supervision can be performed on every node. The learning task
                                   can be posed as the minimization of a quadratic cost function:
                                                                                                                              p      q
                                                                                                                                       i
                                                                                                                            XX                                                      2
                                                                                                               ew =                       (ti,j ¡ ϕw(Gi,ni,j)) .                                                                                               (6)
                                                                                                                            i=1 j=1
                                   May 24, 2007                                                                                                                                                                                                           DRAFT
                                 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                                                                9
                                     Remark 4: As common in neural network applications, the cost function may include a penalty term to control
                                 other properties of the model. For example, the cost function may contain a smoothing factor to penalize any abrupt
                                 changes of the outputs and to improve the generalization performance.
                                     The learning algorithm is based on a gradient descendant strategy and is composed of the following steps:
                                  (a) The states xn(t) are iteratively updated by Eq. (5) until at time T they approach the ﬁxed point solution of
                                          Eq. (2): x(T) ¼ x;
                                  (b) The gradient ∂ew(T) is computed;
                                                                       ∂w
                                  (c) The weights w are updated according to the gradient.
                                     Concerning step (a), note that the hypothesis that Fw is a contraction map ensures the convergence to the ﬁxed
                                 point. Step (c) is carried out within the traditional framework of gradient descent. As shown in the following,
                                 step (b) can be carried out in a very efﬁcient way by exploiting the diffusion process that takes place in GNNs.
                                 Interestingly, this diffusion process is very much related to the one which takes place in recurrent neural networks,
                                 for which the gradient computation is based on backpropagation through time algorithm [43], [14], [44]. In this
                                 case, the encoding network is unfolded from time T back to an initial time t0. The unfolding produces the layered
                                 network as shown in Figure 4. Each layer corresponds to a time instance and contains a copy of all the units f                                                                                                 of
                                                                                                                                                                                                                                            w
                                 the encoding network. The units of two consecutive layers are connected following the graph connectivity. The last
                                 layer corresponding to time T includes also the unit g                                                  and computes the output of the network. Backpropagation
                                                                                                                                     w
                                 through time consists of carrying out the traditional backpropagation step on the unfolded network to compute the
                                 gradient of the cost function at time T with respect to all the instances of f                                                                      and g . Then, ∂ew(T) is obtained
                                                                                                                                                                                w              w                     ∂w
                                 by summing the gradients of all instances.
                                                                 o (t)
                                                                   2                                                                                                            l ,l       ,... l ,l      ,... l ,l      ,...          l ,l      ,...
                                                                                                                                                                    l1           1 (1,2)        1 (1,2)        1 (1,2)                  1 (1,2)
                                                       l          gw                                                                              o (t)
                                                         2                                                                                          1      gw              fw             fw             fw                      fw
                                                                                                                                                               2               1              1              1                        1
                                            l 1             x (t)                                        l 3                                                        l2                                          l ,l       ,...          l ,l      ,...
                                                              2            fw          l 2,l(2,3),...                                                                                                            2 (2,3)                  2 (2,3)
                                                    x (t)                                                                                         o (t)
                              o (t)         g         1                                                   g           o (t)                         2      gw              fw             fw             fw                      fw
                                1             w                                         x (t)               w          3                                       2               1              1              1                        1
                                                                      x (t)               3
                                                                        2                                                                                           l3                                          l ,l       ,...          l ,l      ,...
                                l   ,l       ,...     f                                       f           l   ,l       ,..                                                                                       3 (4,3)                  3 (4,3)
                                  1    (1,2)            w x (t)                                w            3   (4,3)                             o (t)
                                                                 1                x (t)                                                             3      gw              f              f              f                       f
                                                                                    3                                                                          2            w1             w1             w1                       w1
                                                                      x (t)                                                                                         l                                           l ,l       ,...          l ,l      ,...
                                                                        4                                                                                            4                                           4 (1,4)                  4 (1,4)
                                                                                                                                                  o (t)
                                                                x (t)                                                                               4      g               f              f              f                       f
                                                                  4        f          l   , l      ,...                                                      w              w              w              w                        w
                                                                             w          4    (1,4)                                                             2               1              1              1                        1
                                                       gw          l 4                                                                           time                        T            T−1            T−2                      t0
                                                      o (t)
                                                       4
                                 Fig. 4.      The encoding network and the unfolding through time concept.
                                 May 24, 2007                                                                                                                                                                                             DRAFT
                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                              10
                    However, backpropagation through time requires to store the states of every instance of the units. When the
                  graphs and T ¡ t are large, the memory required may be considerable6. On the other hand, in our case, a more
                                   0
                  efﬁcient approach is possible based on the Almeida–Pineda algorithm [45], [46]. Since Eq. (5) has reached a stable
                  point x before the gradient computation, we can assume that x(t) = x holds for any t ¸ t0. Thus, backpropagation
                  through time can be carried out by storing only x.
                    The folllowing two theorems show that such an intuitive approach has a formal justiﬁcation. The former theorem
                  proves that function ϕ   is differentiable.
                                        w
                    Theorem 1: DIFFERENTIABILITY
                  Let F   and G    be respectively the global transition and the global output functions of a GNN. If F (x,l) and
                        w        w                                                                                      w
                  G (x,l ) are continuously differentiable w.r.t. x and w, then ϕ     is continuously differentiable w.r.t. w.
                    w     N                                                        w
                       Proof: Let a function £ be deﬁned as: £(x,w) = x¡Fw(x,l), Such a function is continuously differentiable
                  w.r.t. x and w, since it is the difference of two continuously differentiable functions. Note that ∂£(x,w) =
                                                                                                                       ∂x
                  I ¡ ∂Fw(x,l), where I denotes the a-dimension identity matrix and a = s|N|, s is the dimension of the
                   a     ∂x                 a
                  state. Since F   is a contraction function, there exists ¹,0 · ¹ < 1 such that k∂Fw(x,l)k · ¹, which implies
                                w                                                                   ∂x
                  k∂£(x,w)k > (1 ¡ ¹). Thus, the determinant of ∂£(x,w) is not null and we can apply the implicit function
                    ∂x                                                ∂x
                  theorem (see [47]) to £ and point w. As a consequence, there exists a function ª, which is deﬁned and continuously
                  differentiable in a neighborhood of w such that £(ª(w),w) = 0, and, ª(w) = Fw(ª(w),l). Since this result
                  holds for any w, it is demonstrated that ª is continuously differentiable on the whole domain.
                    Finally, note that ϕ (G,n) = [G (ª(w),l )] , where [¢]        denotes the operator that returns the components
                                        w             w          N n            n
                  corresponding to node n. Thus, ϕw is the composition of differentiable functions and hence is itself differentiable.
                    It is worth mentioning that this property does not hold for general dynamical systems for which a slight change
                  in the parameters can force the transition from one ﬁxed point to another. The fact that ϕ     is differentiable in
                                                                                                              w
                  GNNs is due to the assumption that Fw is a contraction.
                    The next theorem provides a method for an efﬁcient computation of the gradient.
                    Theorem 2: BACKPROPAGATION
                  Let F   and G    be respectively the transition and the output functions of a GNN. If F (x,l) and G (x,l    ) are
                       w        w                                                                       w             w     N
                  continuously differentiable w.r.t. x and w then
                                                    ∂e      ∂e     ∂G                 ∂F
                                                      w =      w ¢    w(x,l )+z¢         w(x,l).                                (7)
                                                    ∂w       ∂o    ∂w       N         ∂w
                  holds, where x is the stable state of the GNN and z = limt→¡∞ z(t) is the limit of the sequence z(T),z(T¡1),...
                  deﬁned by
                                                 z(t) = z(t +1)¢ ∂Fw(x,l)+ ∂ew ¢ ∂Gw(x,l ).                                     (8)
                                                                   ∂x            ∂o     ∂x      N
                    6Internet applications where the graph may represent a portion of the web, are a straightforward example of cases when the amount of
                  required storage may have a very important role.
                  May 24, 2007                                                                                                DRAFT
                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                  11
                  Moreover, the convergence to z is exponentially fast and is independent of the initial state z(T).
                       Proof: Since F    is a contraction mapping, there exists ¹,0 · ¹ < 1 such that k∂Fw(x,w)k · ¹ holds. Thus,
                                      w                                                                  ∂x
                  Eq. (8) converges to a stable ﬁxed point for each initial state. The stable ﬁxed point z is the solution of Eq. (8)
                  and satisﬁes                                                µ                 ¶
                                                        ∂e     ∂G                    ∂F           ¡1
                                                   z =     w ¢    w(x,l )¢ I ¡           w(x,l)      ,                              (9)
                                                         ∂o     ∂x       N       a    ∂x
                  where a = s|N| holds. Moreover, let us consider again the function ª deﬁned in the proof of Theorem 1. By the
                  implicit function theorem,                   µ                 ¶
                                                       ∂ª = I ¡∂Fw(x,l) ¡1 ∂Fw(x,l),                                              (10)
                                                       ∂w         a    ∂x              ∂w
                  holds. On the other hand, since the error e      depends on the output of the network o = G (ª(w),l )), the
                                                                w                                                  w          N
                  gradient ∂ew can be computed using the chain rule for differentiation,
                            ∂w
                                                       ∂ew    = ∂ew ¢ ∂Gw(x,l )
                                                        ∂w          ∂o     ∂w       N
                                                              + ∂ew ¢ ∂Gw(x,l )¢ ∂ª(w).                                           (11)
                                                                    ∂o     ∂x       N ∂w
                  The theorem follows by putting together Eqs. (9), (10), and (11),
                                                ∂ew    = ∂ew ¢ ∂Gw(x,l )+ ∂ew ¢ ∂Gw(x,l )¢
                                                 ∂w          ∂o     ∂w       N      ∂o     ∂x       N
                                                            µ                 ¶
                                                              I ¡∂Fw(x,l) ¡1 ∂Fw(x,l)
                                                               a    ∂x              ∂w
                                                       = ∂ew ¢ ∂Gw(x,l )+z¢ ∂Fw(x,l).
                                                             ∂o     ∂w       N         ∂w
                     The relationship between the gradient deﬁned by Eq. (7) and the gradient computed by the Almeida–Pineda
                  algorithm can be easily recognized. The ﬁrst term on the left hand side of Eq. (7) represents the contribution to
                  the gradient due to the output function G . Backpropagation calculates the ﬁrst term while it is propagating the
                                                             w
                  derivatives through the layer of the functions g   (see Fig. 4). The second term represents the contribution due to
                                                                  w
                  the transition function Fw. In fact, from Eq. (8),
                                                z(t)   = z(t+1)¢ ∂Fw(x,l)+ ∂ew ¢ ∂Gw(x,l )
                                                                       ∂x            ∂o     ∂x       N
                                                                  µ            ¶
                                                                    ∂F           T¡t
                                                       = z(T)¢         w(x,l)
                                                                     ∂x
                                                           T¡t¡1                        µ            ¶i
                                                             X ∂e        ∂G               ∂F
                                                       +             w ¢    w(x,l )¢         w(x,l)     .
                                                                   ∂o     ∂x       N       ∂x
                                                             i=0
                  If we assume z(T) = ∂ew(T) ¢ ∂Gw (x(T),l ) and x(t) = x, for t · t · T, it follows
                                          ∂o(T)    ∂x(T)         N                       0
                  May 24, 2007                                                                                                   DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                         12
                                                                     T¡t
                                                         z(t)   = X∂ew(T)¢ ∂Gw (x(T),lN)¢
                                                                           ∂o(T)     ∂x(T)
                                                                     i=0
                                                                       i µ                            ¶
                                                                     Y ∂F
                                                                                  w     (x(T ¡j),l)
                                                                     j=1   ∂x(T ¡j)
                                                                     T¡t                   t
                                                                = X ∂ew(T) =X∂ew(T).
                                                                     i=0 ∂x(T ¡i)        i=T ∂x(i)
                      Thus, Eq (8) accumulates the ∂ew(T) into the variable z. This mechanism corresponds to back propagate the
                                                         ∂x(i)
                    gradients through the layers containing the fw units.
                      The learning algorithm is detailed in Table I. It consists of a main procedure and of the two functions FORWARD
                    and BACKWARD. Function FORWARD takes as input the current set of parameters w and iterates to ﬁnd the ﬁxed
                    point. The iteration is stopped when kx(t)¡x(t¡1)k is less than a given threshold ε according to a given norm
                                                                                                                  f
                    k¢k. Function BACKWARD computes the gradient: system (8) is iterated until kz(t ¡ 1) ¡ z(t)k is smaller than a
                    threshold εb; then the gradient is calculated by Eq. (7). The main procedure updates the weights until the output
                    reaches a desired accuracy or some other stopping criterion is achieved. More implementation details along with a
                    computational cost analysis are included in Section III.
                                                                                                             BACKWARD(x,w)
                                                                                                                 o=Gw(x,lN);
                                                                                                                      ∂F
                          MAIN                                                                                   A= w(x,l);
                                                                                                                       ∂x
                                                                                                                      ∂e    ∂G
                              initialize w;                                                                      b =    w ¢   w(x,lN);
                                                                   FORWARD(w)                                         ∂o     ∂x
                              x=FORWARD(w);                            initialize x(0), t = 0;                   initialize z(0), t=0;
                              repeat                                   repeat                                    repeat
                                   ∂ew=BACKWARD(x,w);                                                                z(t) = z(t +1)¢A+b;
                                   ∂w                                      x(t+1)=Fw(x(t),l);
                                  w=w¡¸¢∂ew;                                                                         t=t ¡1;
                                               ∂w                          t=t +1;
                                  x=FORWARD(w);                                                                  until kz(t ¡ 1) ¡ z(t)k · ε ;
                                                                       until kx(t)¡x(t¡1)k · ε                                               b
                                                                                                   f
                                                                                                                     ∂e     ∂G
                              until (a stopping criterion);                                                      c =    w ¢   w(x,lN);
                                                                       return x(t);                                   ∂o    ∂w
                                                                                                                            ∂F
                              return w;                                                                          d=z(t)¢      w(x,l);
                                                                   end                                                      ∂w
                          end                                                                                    ∂ew = c+d;
                                                                                                                 ∂w
                                                                                                                        ∂e
                                                                                                                 return   w;
                                                                                                                         ∂w
                                                                                                             end
                                                                              TABLE I
                     THE LEARNING ALGORITHM. THE FUNCTION FORWARD COMPUTES THE STATES, WHILE BACKWARD CALCULATES THE GRADIENT. THE
                                  PROCEDURE FORWARD MINIMIZE THE ERROR THE BY CALLING ITERATIVELY FORWARD AND BACKWARD/.
                    D. GNN implementations
                      The implementation of the local output function gw does not need to fulﬁll any particular constraint. In GNNs,
                    g   is a multi-layered feedforward neural network. On the other hand, the local transition function f        plays a crucial
                     w                                                                                                        w
                    May 24, 2007                                                                                                          DRAFT
                            IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                              13
                            role in the proposed model, since its implementation determines the number and the existence of the solutions of
                            Eqs. (1). The assumption behind GNN is that the design of f                                           is such that the global transition function F                             is
                                                                                                                              w                                                                        w
                            a contraction w.r.t. the state x. In the followings, we describe two neural network models that fulﬁll this purpose
                            using different strategies. These models are based on the non–positional form described by Eq. (3). It can be easily
                            observed that there exist two corresponding models based on the positional form.
                                1) LINEAR (NON–POSITIONAL) GNN
                                      Eq. (3) can naturally be implemented by
                                                                                           h (l ,l             , x ,l ) = A              x +b .                                                         (12)
                                                                                             w n (n,u)             u u              n,u u           n
                                     where the vector b ∈ IRs and the matrix A                                  ∈IRs£s are deﬁned by the output of two feedforward neural
                                                                  n                                       n,u
                                     networks (FNN), whose parameters correspond to the parameters of the GNN. More precisely, let us call
                                     transition network a FNN that has to generate An,u forcing network another FNN that has to generate bn
                                                                 2l +l                s2                      l             s
                                     and. Let Áw : IR N                  E → IR            and ½w : IR N → IR be respectively the functions implemented by the
                                     transition and the forcing networks. Then, we deﬁne
                                                                                                       A           =           ¹        ¢ ¥                                                             (13)
                                                                                                           n,u            s|ne[u]|
                                                                                                           b       = ½w(l ),                                                                            (14)
                                                                                                             n                    n
                                                                            ¢            ¡                            ¢
                                     where ¹ ∈ (0,1) and ¥ = resize Á (l ,l                                    , l  ) , where resize(¢) denotes the operator that allocates the
                                                                                             w n (n,u) u
                                     elements of a s2-dimensional vector into a s£s matrix. Here, it is further assumed that kÁ (l ,l                                                           , l  )k ·
                                                                                                                                                                              w n (n,u) u 1
                                                 7
                                     s holds ; this can be straightforwardly veriﬁed if the output neurons of the transition network uses an
                                     appropriately bounded activation function, e.g. a hyperbolic tangent. Note that in this case Fw(x,l) = Ax+b,
                                                                                                                                                                      ¹                   ¹
                                     where b is the vector constructed by stacking all the b , and A is a block matrix {A                                                    }, with A            =A
                                                                                                                         n                                              n,u                 n,u          n,u
                                                                                 ¹
                                     if u is a neighbor of n and A                        =0otherwise. Moreover, vectors b and matrices A                                          do not depend on
                                                                                    n,u                                                          n                           n,u
                                     the state x, but only on the node and edge labels. Thus, ∂Fw = A, and, by simple algebra,
                                                                                                                                ∂x
                                                                                °         °                                    X                         
                                                                                °∂Fw°
                                                                                °         °       = kAk ·max                                kA k 
                                                                                ° ∂x °                          1      u∈N                        n,u 1
                                                                                             1                                    n∈ne[u]
                                                                                                                 ¹                   X                
                                                                                                  · max                          ¢           k¥k1·¹,
                                                                                                         u∈N        s|ne[u]|       n∈ne[u]
                                     which implies that Fw is a contraction function (w.r.t. k ¢ k1) for any set of parameters w.
                                2) NON–LINEAR (NON–POSITIONAL) GNN
                                       In this case, hw is realized by a multilayered FNN. Since three layered neural networks are universal
                                     approximators, hw can approximate any desired function. However, not all the parameters w can be used,
                                     since it must be ensured that the corresponding transition function F                                          is a contraction. This can be achieved
                                                                                                                                                w
                                     by adding a penalty term to Eq. (6), i.e.
                               7The norm one of a matrix M = {m                  } is deﬁned as kMk1 = max P |m                       |
                                                                              i,j                                      j     i    i,j
                            May 24, 2007                                                                                                                                                             DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                         14
                                                                                                       °      °
                                                               p   qi                                µ          ¶
                                                              XX                             2         °∂Fw°
                                                       ew =           (ti,j ¡ ϕw(Gi,ni,j)) +¯L °              ° ,
                                                              i=1 j=12                                 ° ∂x °
                          where the penalty term, L(y) is (y ¡¹) if y > ¹ and 0 otherwise, and the parameter ¹ ∈ (0,1) deﬁnes the
                          desired contraction constant of F . More generally, the penalty term can be any expression, differentiable
                                                               w
                          w.r.t. w which has a small Jacobian function. For example, in our experiments, we use the penalty term:
                                 P        ¡       ¢
                                    s          i               i                         ∂F
                          p   =         L kAk ,whereA isthei–thecolumnof                   w.Infact, such an expression is an approximation
                           w        i=1          1                                        ∂x
                                ¡°∂Fw° ¢         ¡          i  ¢
                          of L °       °    =L max kAk .
                                   ∂x 1                i      1
                    E. A comparison with random walks and recursive neural networks
                      GNNs turn out to be an extension of other models already proposed in the literature. In particular, recursive
                    neural networks [14] are a special case of GNNs, where
                      1) The input graph is a Directed Acyclic Graph (DAG);
                      2) The inputs of f      are limited to l   and x      , where ch[n] is the set of children of n 8;
                                           w                  n        ch[n]
                      3) There is a supersource s from which all the other nodes can be reached. This node s is typically used for
                          output o (graph-focused tasks).
                                   s
                    The neural architectures, which have been suggested for realizing fw and gw, include multilayered FNNs [14],
                    [16], cascade correlation [48], and self organizing maps [17], [49]. Note that the above constraints on the processed
                    graphs and on the inputs of f      exclude any sort of cyclic dependence of a state on itself. Thus, in the recursive
                                                     w
                    neural network model, the encoding networks are FNNs. This assumption simpliﬁes the computation of the states.
                    In fact, the states can be computed following a predeﬁned ordering that is induced by the partial ordering of the
                    DAG.
                      Interestingly, the GNN model captures also the random walks on graphs when choosing f                as a linear function.
                                                                                                                        w
                    Random walks and, more generally, Markov chain models are useful in several application areas and have been
                    recently used to develop ranking algorithms for Web search engines [15], [18]. In random walks on graphs, the
                    state xn associated with a node is a real value and is described by:
                                                                       x = X a x                                                            (15)
                                                                         n             n,i  u
                                                                              u∈pa[n]
                    where pa[n] is the set of parents of n, and a       ∈IR, a     ¸0holds for each n,i. The a          are normalized so that
                                                                    n,i         n,i                                 n,i
                    P         a    =1. In fact, Eq. (15) can represent a random walker who is traveling on the graph. The value a
                      i∈pa[n] i,n                                                                                                            n,i
                    represents the probability that the walker, when visiting node n, decides to go to node i. The state x            stands for
                                                                                                                                   n
                    the probability that the walker is on node n in the steady state. When all xn are stacked into a vector x, Eq. (15)
                    becomes x = Ax where A = {a            } and a      is deﬁned as in Eq. (15) if i ∈ pa[n] and a        =0otherwise. It is
                                                         n,i        n,i                                                n,i
                    easily veriﬁed that kAk = 1, where the 1–norm k¢k is deﬁned as kAk = max P |a                       |. Markov chain theory
                                             1                                1                    1         i   n   n,i
                      8A node u is child of n if there exists an arc from n to u. Obviously, ch[n] µ ne[n] holds.
                    May 24, 2007                                                                                                          DRAFT
                   IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                    15
                                                                          TABLE II
                     THE TIME COMPLEXITY OF THE MOST EXPENSIVE INSTRUCTIONS OF THE LEARNING ALGORITHM. FOR EACH INSTRUCTION AND EACH
                    GNNMODELABOUNDONTHEORDEROFTHEFLOATINGPOINTOPERATIONSISGIVEN.ITALSODISPLAYSTHENUMBEROFTIMESPER
                                                          EPOCH THAT EACH INSTRUCTION IS EXECUTED.
                                   instruction              positional             non¡linear             linear        execs.
                               z(t+1)=z(t)¢A+b                 s|E|                    s|E|                 s|E|          it
                                                                                                                           b
                                                                 ¡→                      ¡→                   ¡→
                                 o=Gw(x(t),lw)                |N|Cg                  |N|Cg                |N|Cg            1
                                                                 ¡→                      ¡→
                               x(t+1)=F (x(t),l)              |N|C                    |E|C                  s|E|          it
                                           w                        f                      h                               f
                                                                                                         ¡→       ¡→
                                                                  Ã¡                     Ã¡           |N|C½+|E|CÁ          1
                                       ∂F
                                  A= w(x,l)                  s|N|Cf                  s|E|Ch                  ¡             1
                                        ∂x
                                        ∂ew                     |N|                    |N|                  |N|            1
                                        ∂o
                                        ∂pw                      2      Ã¡              2      Ã¡
                                                       tR¢max(s ¢hi , C )     tR ¢ max(s ¢hi , C )           ¡             1
                                        ∂w                           f    f                  h   h
                                    ∂e  ∂G                       Ã¡                      Ã¡                   Ã¡
                               b =    w    w(x,lN)            |N|Cg                  |N|Cg                |N|Cg            1
                                    ∂o   ∂x
                                    ∂e  ∂G                       Ã¡                      Ã¡                   Ã¡
                               c =    w    w(x,lN)            |N|Cg                  |N|Cg                |N|Cg            1
                                    ∂o ∂w
                                         ∂F                      Ã¡                      Ã¡              Ã¡       Ã¡
                                 d=z(t) w(x,l)                |N|Cf                   |E|Ch           |N|C½+|E|CÁ          1
                                          ∂w
                                                                                            t
                   suggests that if there exists t such that all the elements of the matrix A are non–null, then Eq. (15) is a contraction
                   map [50]. Thus, provided that the above condition on A holds, random walks on graphs are an instance of GNNs,
                   where A is a constant stochastic matrix instead of being generated by neural networks.
                                                           III. COMPUTATIONAL COST ISSUES
                     In this section, an accurate analysis of the computational cost will be derived. The analysis will focus on three
                   different GNN models: positional GNNs, where the functions f          and g    of Eq. (1) are implemented by FNNs;
                                                                                      w        w
                   linear (non–positional) GNNs; non–linear (non–positional) GNNs.
                     First we will describe with more details the most complex instructions involved in the learning procedure (see
                   Table II). Then, the complexity of the learning algorithm will be deﬁned. For the sake of simplicity, the cost is
                   derived assuming that the training set contains just one graph G. Such an assumption does not cause any loss
                   of generality, since the graphs of the training set can always be merged into a single graph. The complexity is
                                                                           9
                   measured by the order of the ﬂoating point operations
                     In the following, it , it , it   denote the number of epochs, the mean number of forward iterations (of the
                                          l    f   b
                   repeat cycle in function FORWARD) and the mean number of backward iterations (of the repeat cycle in function
                   BACKWARD), respectively. Moreover, we will assume that there exist two procedures FP and BP which implement
                   the forward phase and the backward phase of the backpropagation procedure [51], respectively. Formally, given a
                     9According to the common deﬁnition of time complexity, an algorithm requires O(l(a)) operations, if there exist ® > 0, a¹ ¸ 0, such that
                   c(a) · ®l(a) holds for each a ¸ a¹, where c(a) is the maximal number of operations executed by the algorithm when the length of input is a.
                   May 24, 2007                                                                                                     DRAFT
                     IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                    16
                     function l     : IRa → IRb implemented by a FNN, we have
                                 w
                                                                                  l  (y)    = FP(l ,y)
                                                                ·                  w ¸                 w
                                                                 ±∂lw(y),±∂lw(y)            = BP(l ,y,±).
                                                                   ∂w          ∂y                      w
                     Here, y ∈ IRa is the input vector and the row vector ± ∈ IRb is a signal which suggests how the network output
                                                                                                                                                     2
                     must be adjusted to improve the cost function. In most applications, the cost function is ew(y) = (t ¡ y) and
                     ± = ∂ew(y) = 2(t¡o), where o = l (y) and t is the vector of the desired output corresponding to input x. On
                            ∂o                                   w
                     the other hand, ±∂lw(y) is the gradient of e             w.r.t. the network input and is easily computed as a side product
                                           ∂y                              w
                                            10           ¡→        Ã¡
                     of backpropagation . Finally, Cl and Cl denote the computational complexity required by the application of FP
                     and BP on l . For example, if l           is implemented by a multilayered FNN with a inputs, b hidden neurons, and c
                                    w                       w
                                     ¡→      Ã¡
                     outputs, then Cl = Cl = O(ab+ac) holds.
                     Complexity of instructions
                        Instructions z(t + 1) = z(t) ¢ A + b, o = G (x,l ), and x(t + 1) = F (x(t),l): Since A is a matrix
                                                                                 w      N                            w
                     having at most 2s|E| non–null elements, the multiplication of z(t) by A, and as a consequence, the instruction
                     z(t+1) = z(t)¢A+b, costs O(s|E|) ﬂoating points operations. Moreover, the state x(t+1) and the output vector
                     o are calculated by applying the local transition function and the local output function to each node n. Thus, in
                     positional GNNs and in non–linear GNNs, where fw, hw, gw are directly implemented by FNNs, x(t+1) and o
                     are computed by running the forward phase of backpropagation once for each node or edge (see Table II).
                        Onthe other hand, in linear GNNs, xn(t) is calculated in two steps: the matrices An of Eq. (13) and the vectors
                                                                                                                                         ¡→           →¡
                     bn Eq. (14) are evaluated; then, xn(t) is computed. The former phase, the cost of which is O(|E|CÁ +|N|C½),
                     is executed once for each epoch, whereas the latter phase, the cost of which is O(s|E|), is executed at every step
                     of the cycle in the function Forward.
                                             ∂F
                        Instruction A =         w(x,l): This instruction requires the computation of the Jacobian of F . Note that A =
                                              ∂x                                                                                      w
                     {A }isablock matrix where the block A                     measures the effect of node u on node n, if there is an arc (n,u)
                         n,u                                              n,u
                     from u to n, and zero otherwise. In the linear model, the matrices An,u correspond to those displayed Eq. (13)
                     and used to calculate x(t). Thus, such an instruction has no cost in the linear GNNs.
                        In non–linear GNNs, A           = ∂hw(l ,l         , x ,l ), is computed by appropriately exploiting the backpropagation
                                                   n,u     ∂x     n (n,u)     u u
                                                              n
                     procedure. More precisely, let q ∈ IRs be a vector where all the components are zero except for the i–the one,
                                                           i
                     which equals one, i.e. q = [1,0,...,0], q = [0,1,0,...,0], and so on. Note that BP, when it is applied to l
                                                  1                     2                                                                                 w
                                                i         ∂lw                                                   ∂lw
                     with ± = bi, returns A         =qi       (y), i.e. the i-the column of the Jacobian            (y). Thus, An,u can be computed
                                                n,u        ∂y                                                   ∂y
                     by applying BP on all the qi, i.e.
                                                                      1           s          i
                                                         A =[A ,...,A ], A                        =BP (h ,y,q ),                                       (16)
                                                           n,u        n,u         n,u        n,u         2   w       i
                       10                                                            ∂e          ∂l
                         Backpropagation computes for each neuron v the delta value    w(y) = ± w(y), where ew is the cost function and av the activation
                                                                                     ∂a          ∂a
                                                                                       v            v
                                              ∂l
                     level of neuron v. Thus, ± w(y) is just a vector staking all the delta values of the input neurons.
                                               ∂y
                     May 24, 2007                                                                                                                    DRAFT
                            IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                              17
                            where BP indicates that we are considering only the ﬁrst component of the output of BP. A similar reasoning
                                            2
                            can also be used with positional GNNs. The complexity of these procedures is easily derived and is displayed in
                            the third row of Table II.
                                                                                                                                                       P
                                                          ∂ew            ∂pw                                                                               q                                2
                                Computation of                   and           : In linear GNNs, the cost function is e                            =            (t ¡ϕ (G,n )) , and, as a
                                                           ∂o             ∂w                                                                   w           i=1 i            w           i
                            consequence, ∂ew = 2(t ¡ o ), if n is a node belonging to the training set, and 0 otherwise. Thus, ∂ew is
                                                                   k        n             k
                                                  ∂o                          k                                                                                                                      ∂o
                                                      k
                            easily calculated by O(|N|) operations.
                                In positional and non–linear GNNs, a penalty term p                                 is added to the cost function to force the transition function
                                                                                                                w
                            to be a contraction mapping. In this case, it is necessary to compute ∂pw, because such a vector must be added to
                                                                                                                                        ∂w
                            the gradient. Let Ai,j denote the element in position i,j of the block A                                                 . According to the deﬁnition of p ,
                                                           n,u                                                                                   n,u                                                      w
                            we have
                                                                                           s                     s                                    s
                                                                       p     = XXL( X X|Ai,j|¡¹)= XX® .
                                                                         w                                               n,u                                 u,j
                                                                                 u∈Nj=1            (n,u)∈E i=1                               u∈Nj=1
                                                  P                Ps            i,j
                            where ®           =                             |A       | ¡ ¹, if such a sum is > 0, and it is 0 otherwise. It follows:
                                         u,j          (n,u)∈E          i=1       n,u
                                                                                                    s                         s                            i,j
                                                                      ∂pw                  XX X X i,j ∂An,u
                                                                                 = 2                    ®                         sgn(A          ) ¢
                                                                       ∂w                                  u,j                              n,u         ∂w
                                                                                           u∈Nj=1               (n,u)∈E i=1
                                                                                                                s      s                                     i,j
                                                                                           X X XX                                                       ∂A
                                                                                 = 2                                       ®      ¢ sgn(Ai,j ) ¢             n,u ,
                                                                                                                             u,j              n,u         ∂w
                                                                                           u∈N(n,u)∈Ej=1 i=1
                                                                                                                                                                                                        i,j
                            where sgn is the sign function. Moreover, let Rn,u be a matrix whose element in position i,j is ®u,j ¢ sgn(A                                                                     )
                                                                                                                                                                                                        n,u
                            and let vec be the operator that takes a matrix and produces a column vector by stacking all its columns one on
                            top of the other. Then
                                                                              ∂p              X X                                        ∂vec(A           )
                                                                                  w =2                           (vec(R           ))′ ¢               n,u                                               (17)
                                                                               ∂w                                            n,u                ∂w
                                                                                             u∈N(n,u)∈E
                            holds. The vector ∂vec(An,u) depends on selected implementation of h                                                   or f . For sake of simplicity, let us
                                                                ∂w                                                                            w           w
                            restrict our attention to non–linear GNNs and assume that the transition network is a three layered FNN. ¾ , a ,
                                                                                                                                                                                                     j      j
                            V , and t are the activation function11, the vector of the activation levels, the matrix of the weights, and the
                               j            j
                            thresholds of the j-the layer, respectively. The following reasoning can also be extended to positional GNNs and
                            networks with a different number of layers. The function h                                           is formally deﬁned in terms of ¾ , a , V , t :
                                                                                                                            w                                                          j      j       j    j
                            a =[l ,x ,l                   , l  ],    a =V a +t , a =V ¾ (a )+t , and h (l ,l                                                        , x ,l ) = ¾ (a ). By the
                              1        n      u (n,u) u                 2         1    1      1         3         2 2       2         2           w n (n,u)             u u             3     3
                            chain rule, it follows
                                                                                                     µ∂h                                   ¶
                                                                    vec(A          )    = vec               w(l ,l            , x ,l )
                                                                              n,u                       ∂x        n (n,u)          u u
                                                                                                     ¡       u                                                     ¢
                                                                                        = vec diag(¾′(a ))¢V ¢diag(¾′(a ))¢V                                         ,
                                                                                                                 3     3         2               2    2          1
                            where ¾′ is the derivate of ¾ , diag is an operator that transforms a vector into a diagonal matrix having such a vector
                                        j                              j
                            as diagonal, and V 1 is the submatrix of V 1 that contains only the weights that connect the inputs corresponding
                               11¾ is a vectorial function that takes in input the vector of the activations of a layer and returns the vector of the outputs for the same layer.
                                   j
                            May 24, 2007                                                                                                                                                             DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                         18
                    to x to the hidden layer. The parameters w affects four components of vec(A              ), i.e. a , V , a , V . Thus, by
                        u                                                                                 n,u        3     2   2    1
                    properties of derivatives for matrix products and the chain rule,
                                                 ∂vec(A       )        ∂vec(A      )  ∂¾′(a )       ∂vec(A      )
                                                          n,u     =             n,u ¢     3  3 +             n,u ¢
                                                      ∂w                 ∂¾′(a )         ∂w          ∂vec(V )
                                                                            3   3                             2
                                                                       ∂vec(V )       ∂vec(A      )   ∂¾′(a )
                                                                                2 +            n,u ¢     2   2
                                                                          ∂w            ∂¾′(a )         ∂w
                                                                                           2   2
                                                                  + ∂vec(An,u) ¢ ∂vec(V1)
                                                                        ∂vec(V1)          ∂w
                    holds.
                      Moreover, let Ia denote the a £ a identity matrix. Let ­ be the Kronecker product and suppose that Pa
                    is a a2 £ a matrix such that vec(diag(v)) = P v for any vector v ∈ IRa. By the Kronecker product property
                                                                         a
                    vec(AB)=(B′­Ia)¢vec(A)holdsformatricesA,B,Ia havingcompatibledimensions[],wehavevec(An,u) =
                    ¡                            ′     ¢                               ∂vec(A    )    ¡                            ′     ¢
                     (V ¢diag(¾′(a ))¢V ) ­I ¢P ¢¾′(a )whichimplies                           n,u  = (V ¢diag(¾′(a ))¢V ) ­I ¢P .
                        2          2   2       1      s     s   3   3                   ∂¾′(a3)           2          2   2      1       s     s
                                                                                           3
                    Similarly, using the properties vec(ABC) = (C′ ­A)¢vec(B) and vec(AB) = (Ia ­A)¢vec(B), it follows
                                            ∂vec(An,u)                    ′             ′          ′
                                                             = (diag(¾ (a2))¢V1) ­diag(¾ (a3))
                                             ∂vec(V )                     2                        3
                                                      2
                                            ∂vec(An,u)                         ′  ¡  ′            ′             ¢
                                                             = (vec(R )) ¢ V ­(diag(¾ (a ))¢V ) ¢P
                                              ∂¾′(a )                     u,v        1            3   3       2       dh
                                                  2   2
                                            ∂vec(An,u)       = (I ­(diag(¾′(a ))¢V ¢diag(¾′(a )))) ,
                                             ∂vec(V )               s            3   3      2          2   2
                                                      1
                    where d is the number of hidden neurons. When the above values are multiplied by (vec(R                  ))′, we have
                            h                                                                                             u,v
                                                          ∂vec(A      )   ∂¾′(a )
                                                 ′                 n,u       3   3
                                   (vec(Ru,v))       ¢      ∂¾′(a3)     ¢   ∂w      =
                                                               3
                                                         ³     ³          ′                       ´´′        ∂¾′(a )
                                                     =     vec R       ¢ V  ¢ diag(¾′(a )) ¢ V ′      ¢ P ¢      3   3                      (18)
                                                                   u,v    1          2   1       2        s     ∂w
                                                          ∂vec(A      )   ∂vec(A      )
                                                 ′                 n,u             n,u
                                   (vec(Ru,v))       ¢     ∂vec(V 2) ¢        ∂w        =
                                                         ³     ³        ′                  ′          ′      ´´′ ∂vec(An,u)
                                                     =     vec diag(¾ (a ))¢R          ¢ V   ¢ diag(¾ (a ))      ¢                          (19)
                                                                        3   3      u,v     1          2  2              ∂w
                                                          ∂vec(A      )   ∂¾′(a )
                                                 ′                 n,u       2   2
                                   (vec(Ru,v))       ¢     ∂¾′(a2)      ¢   ∂w      =
                                                               2
                                                         ¡     ¡                                 ¢¢′         ∂¾′(a )
                                                     = vec V′ ¢diag(¾′(a ))¢R               ¢ V ′    ¢ P    ¢    2   2                      (20)
                                                                  2          3   3      u,v     1       dh      ∂w
                                                          ∂vec(A      )   ∂vec(V )
                                                 ′                 n,u             1
                                   (vec(Ru,v))       ¢     ∂vec(V 1) ¢       ∂w       =
                                                     = ¡vec¡diag(¾′(a2))¢V′ ¢diag(¾′(a3))¢Ru,v¢¢′ ¢ ∂vec(V 1) ,                             (21)
                                                                       2           2         3                       ∂w
                    where the mentioned Kronecker product properties have been used.
                                           ′  ∂vec(A    )
                      Thus, (vec(R       )) ¢        n,u   is the sum of four contributions represented by (18), (19), (20) and (21). The
                                      u,v         ∂w
                    second and the fourth term (Eqs. (19) and (21)) can be computed directly using the corresponding formulas. The
                    May 24, 2007                                                                                                          DRAFT
                            IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                              19
                            ﬁrst one can be calculated by observing that ¾′(a3) is in a three layered FNN form which is the same as hw except
                                                                                                   3
                                                                                                                                       ¹
                            for the activation function of the last layer. In fact, if we denote by hw such a network, then
                                                                                            ′    ∂vec(An,u)∂¾′(a3)
                                                                                                                         3                      ¹
                                                                         (vec(R          )) ¢                                      =BP (h ,a ,±),                                                       (22)
                                                                                     u,v            ∂¾′(a )             ∂w                   1    w 1
                                                                                                        3     3
                                                                           ′   ∂vec(An,u)
                            holds, where ± = (vec(Ru,v)) ¢                        ∂¾′(a ) . A similar reasoning can be applied also to the fourth contribution.
                                                                                     3    3
                                The above described method includes two tasks: the matrix multiplications of Eqs. (18)-(21) and the back-
                            propagation as deﬁned by Eq. (22). The former task consists of several matrix multiplications. By inspection of
                            Eqs. (18)-(21), the number of ﬂoating point operations is approximately estimated as 2s2+12s¢hi +10s2¢hi 12.
                                                                                                                                                                                     h                 h
                            The second task has approximately the same cost as a backpropagation phase through the original function hw.
                                                                                       ∂p                              2         Ã¡
                                Thus, the complexity of computing                         w is O(|E|max(s ¢hi , C )). Note, however, even if the sums in Eq. (17)
                                                                                        ∂w                                   h       h
                            ranges over all the arcs of the graph, only those arcs (n,u) such that Rn,u 6= 0 need to be considered. In practice,
                            Rn,u 6= 0 is a rare event, since it happens only when the columns of the Jacobian are larger than ¹ and a penalty
                            function was used to limit the occurrence of these cases. As a consequence a better estimate of the complexity of
                                              ∂pw                               2          Ã¡
                            computing                 is O(t        ¢ max(s ¢ hi , C )), where t                       is the average number of nodes u such that R                                     6=0
                                               ∂w               R                       h      h                   R                                                                             n,u
                            holds for some n.
                                Instructions b = ∂ew ∂Gw(x,l ) and c = ∂ew ∂Gw(x,l ): The terms b and c can be calculated by the back
                                                            ∂o ∂x              N                     ∂o ∂w               N
                                                    ∂e
                            propagation of              w through the network that implements g . Since, such an operation must be repeated for each
                                                     ∂o                                                                  w
                                                                                                       ∂ew ∂Gw                                  ∂ew ∂Gw                                     Ã¡
                            node, the time complexity of instructions b = ∂o                                  ∂x (x,lN) and c = ∂o ∂w (x,lN) are O(|N|Cg) for all
                            the GNN models.
                                                                ∂F
                                Instruction d = z(t)                w(x,l): By deﬁnition of F , f , and BP, we have
                                                                 ∂w                                            w w
                                                                     z(t) ¢ ∂Fw(x,l)               = Xzn(t)¢∂fw(ln,lco[n],xu,lne[n])
                                                                                ∂w                        n∈N                 ∂w
                                                                                                   = XBP(f ,y,z (t)),                                                                                   (23)
                                                                                                                       1    w          n
                                                                                                          n∈N
                            where y = [l ,x ,l                      , l  ] and BP indicates that we are considering only the ﬁrst part of the output of BP.
                                                 n     u (n,u) u                       1
                            Similarly,
                                                                     z(t) ¢ ∂Fw(x,l) =X X z (t)¢ ∂hw(l ,l                                               , x ,l )
                                                                                ∂w                                      n         ∂w n (n,u)                u u
                                                                                                   n∈Nu∈ne[n]
                                                                                                 =X X BP(h ,y,z (t)),                                                                                   (24)
                                                                                                                           1    w          n
                                                                                                   n∈Nu∈ne[n]
                            where y = [l ,x ,l                      , l  ]. Thus, Eqs. (23) and (24) provide a direct method to compute d in positional and
                                                 n      u (n,u) u
                            non–linear GNNs, respectively.
                               12Such a value is obtained by considering the following observations: for an a £ b matrix C and b £ c matrix D, the multiplication CD
                            requires approximately 2abc operations; more precisely abc multiplications and ac(b ¡ 1) sums. If D is a diagonal b £ b matrix, then CD
                                                                                                                                                           2
                            requires 2ab operations. Moreover, if C is an a £ b matrix, D is a b £ a matrix, and Pa is the a £ a matrix above deﬁned and used in
                            Eqs. (18)-(21), then computing vec(CD)Pc costs only 2ab operations. Finally, a ,a ,a                                   are already available, since they are computed
                                                                                                                                      1    2    3
                            during the forward phase of the learning algorithm.
                            May 24, 2007                                                                                                                                                             DRAFT
                          IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                  20
                                                                                                                                                                       P
                                                             i                                         i                        i                                  i       s        i,j   i
                              For linear GNNs, let h             denote the i-the output of h              andnotethat h (l ,l                  , x ,l ) = b +                   A x =
                                                             w                                         w                        w n (n,u)           u u            u       j=1      n,u u
                                                   P
                            i              ¹           s       j     i,j                                            i,j           i,j
                          ½ (ln)+                 ¢          x ¢Á (ln,l(n,u),lu) holds, where A                          and Á        are the element in position i,j of matrix
                            u           s|ne[u]|       j=1 u         w                                              n,u           w
                          A and the corresponding output of the transition network (see Eq. (13)), respectively, while bi is the i–the
                             n,u                                                                                                                                            u
                                                       i     i                                                                                                              i
                          element of vector b , ½               is the corresponding output of the forcing network (see Eq. (14)), and x                                       is the i–the
                                                       u     u                                                                                                              u
                          element of x . Then,
                                             u
                                                                ∂F                        X X                         ∂h
                                                       z(t) ¢        w(x,l) =                              z (t)¢         w(l ,l           , x ,l )
                                                                 ∂w                                          n         ∂w n (n,u)              u u
                                                                                          n∈Nu∈ne[n]
                                                                                                             s                  i
                                                                                   = X X Xzi(t)¢∂hw(ln,l(n,u),xu,lu)
                                                                                                                   n        ∂w
                                                                                          n∈Nu∈ne[n] i=1
                                                                                          X                                X X                            ¹ ¹
                                                                                   =             BP (½ ,y,±) +                              BP (Á ,y,±),
                                                                                                      1   w                                      1    w
                                                                                          n∈N                              n∈Nu∈ne[n]
                                                  ¹                                                   ′             ¹                                       i            ¹          j
                          where y = ln, y = [ln,l(n,u),lu], ± = |ne[n]| ¢ z (t), and ± is a vector that stores z (t) ¢                                                          ¢ x    in the
                                                                                                                                                            n        s|ne[u]|       u
                                                                            ¹          ¹                       ′
                          position corresponding to i,j, i.e. ± = s|ne[u]|vec(zn(t)¢xu). Thus, in linear GNNs, d is computed by calling the
                          backpropagation procedure on each arc and node.
                          Time complexity of the GNN model
                              According to our experiments, the application of a trained GNN on a graph (test phase) is relatively fast even
                                                                                                                                                              →¡                    ¡→
                          for large graphs. Formally, the complexity is easily derived from Table II and it is O(|N|C                                               +it ¢|N|C ) for
                                                                                                                                                                  g       f            f
                                                              ¡→                  ¡→                                                       →¡                              ¡→            ¡→
                          positional GNNs, O(|N|C +it ¢|E|C ) for non–linear GNNs, and O(|N|C +it ¢|E|s+|N|C +|E|C )
                                                                  g       f           h                                                        g       f                      ½              Á
                          for linear GNNs. In practice, the cost of the test phase is mainly due to the repeated computation of the state
                          x(t). The cost of each iteration is linear both w.r.t. the dimension of the input graph (the number of edges) and
                          the dimension of the employed FNNs. The number of iterations required for the convergence of the state depends
                          on the problem at hand, but Banach’s Theorem ensures that the convergence is exponentially fast and experiments
                          have shown that 5-15 iterations are sufﬁcient to approximate the ﬁxed point.
                              In positional and non–linear GNNs, the transition functions must be activated it ¢ |N| and it ¢ |E| times,
                                                                                                                                                       f                    f
                          respectively. Even if such a difference may appear signiﬁcant, in practice the complexity of the two models is
                          similar, because the network that implements the f                             is larger than the one that implements h . In fact, f                             has
                                                                                                     w                                                               w                 w
                          M(s+l )input neurons, where M is the maximum number of neighbours for a node, whereas h                                                         has only s+l
                                      E                                                                                                                               w                       E
                          input neurons. An appreciable difference can be noticed only for graphs where the number neighbors of nodes is
                          highly variable, since the inputs of fw must be sufﬁcient to accommodate the maximal number of neighbors and
                          many inputs may remain unused when f                            is applied. On the other hand, it is observed that in the linear model
                                                                                      w
                          the FNNs are used only once for each iteration, so that the complexity of each iteration is O(s|E|) instead of
                                   ¡→                      →¡
                          O(|E|C ). Note that C                   = O((s + l +2l )¢hi ) holds, when h                                is implemented by a three layered FNN
                                       h                       h                   E         N         h                         w
                          with hi       hidden neurons. Thus, the linear model is always faster than the non–linear model. As conﬁrmed by the
                                     h
                          experiments, such an advantage is mitigated by the smaller accuracy which the model usually achieves.
                          May 24, 2007                                                                                                                                                  DRAFT
                    IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                         21
                      In GNNs, the learning phase requires much more time than the test phase, mainly due to the repetition of the
                    forward and the backward phases for several epochs. The experiments have shown that the time spent in the forward
                    and the backward phases are not very different 13. Similar to the forward phase, the cost of function Backward
                    is mainly due to the repetition of the instruction that computes z(t). Theorem 2 ensures that z(t) converges
                    exponentially fast and the experiments conﬁrmed that it        is usually a small number.
                                                                                b
                      Formally, the cost of each learning epoch is given by the sum of all the instructions times the iterations in
                    Table II. An inspection of Table II shows that the cost of all instructions involved in the learning phase are linear
                    both with respect to the dimension of the input graph and of the FNNs. The only exception is due to the possible
                    computation of ∂pw which depends quadratically on s.
                                      w
                                                                                        ∂p                                 2       Ã¡
                      In fact, the non–linear GNNs needs to calculate the term            w, which costs O(t       ¢ max(s ¢ hi , C )). The
                                                                                         w                      R                h    h
                    experiments have shown that usually t         is a small number. In most epochs, t         is 0, since the Jacobian does
                                                               R                                            R
                    not violate the imposed constraint, and in the other cases tR is usually in the range 1-5. Thus, for a small state
                    dimension s, the computation of ∂pw requires few applications of backpropagation on h and has a small impact
                                                          w
                    on the global complexity of the learning. On the other hand, in theory, if s is very large, it might happen that
                     2           Ã¡
                    s ¢hi >> C ¼(s+l +2l )¢hi and at the same time t >> 0 rendering the computation of the gradient
                          h         h          E      N      h                          R
                    very slow. However, it is worth mentioning that this case was never observed in our experiments.
                                                                 IV. EXPERIMENTAL RESULTS
                      GNNs have been experimentally evaluated to assess their performance. In this paper, we present the results
                    obtained on a set of small problems. Other examples of applications based on GNNs can be found in [52], [53],
                    [54], [55], [56].
                      In particular the experiments have been carried out with both linear and non–linear GNNs. Since according to
                    existing results on recursive neural networks, the non–positional transition function slightly outperforms positional
                    one, currently only non–positional GNNs have been implemented and tested.
                      The following facts hold for each experiment, unless otherwise speciﬁed. Both the (non–positional) linear and
                    the non–linear model were tested. All the functions involved in the two models, i.e. g , Á , and ½               for linear
                                                                                                                   w    w         w
                    GNNs, and g , h       for GNNs were implemented by three layered FNNs with sigmoidal activation functions.
                                  w w
                      The presented results were averaged over ﬁve different runs. In each run, the dataset was a collection of random
                    graphs constructed by the following procedure: each pair of nodes was connected with a certain probability ±; the
                    resulting graph was checked to verify whether it was connected or not and if it was not, random edges were inserted
                    until the condition was satisﬁed.
                      The dataset was split into a training set, a validation set and a test set and the validation set is used to avoid
                    possible issues with overﬁtting. For the problems where the original data is only one single big graph G, the training
                    set, the validation set and the test set include different supervised nodes of G. Otherwise, when several graphs
                      13More precisely, the time spent in function Backward is usually 1.5-2 times larger than the time spent in function Forward.
                    May 24, 2007                                                                                                         DRAFT
                                                                                                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                22
                                                                                                  were available, all the patterns of a graph Gi were assigned to only one set. In every trial, the training procedure
                                                                                                  performed at most 5,000 epochs and every 20 epochs the GNN was evaluated on the validation set. The GNN that
                                                                                                  achieved the lowest cost on the validation set was considered the best model and was applied to the test set.
                                                                                                                The performance of the model is measured by the accuracy in classiﬁcation problems (when t                                                                                                                                                                                                                                                                                                                                                                                                                                                                        can take only
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                i,j
                                                                                                  the values ¡1 or 1) and by the relative error in regression problems (when t                                                                                                                                                                                                                                                                                                                                                                                          may be any real number). More
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      i,j
                                                                                                  precisely, in classiﬁcation problem, a pattern is considered correctly classiﬁed if ϕw(Gi,ni,j) > 0 and ti,j = 1
                                                                                                  or if ϕ (G ,n ) < 0 and t                                                                                                                                                          =¡1. Thus, accuracy is deﬁned as the percentage of patterns correctly classiﬁed
                                                                                                                                        w                           i                 i,j                                                                           i,j
                                                                                                  by the GNN on the test set. On the other hand, in regression problems, the relative error on a pattern is given by
                                                                                                  |(ti,j ¡ ϕw(Gi,ni,j))/ti,j|.
                                                                                                                                                                                                                                                                                                                                                                  R                  14
                                                                                                                The algorithm was implemented in Matlab° 7                                                                                                                                                                                                                                          and the software can be freely downloaded, together with the
                                                                                                  source and some examples [57]. The experiments were carried out on a Power Mac G5 with a 2 GHz PowerPC
                                                                                                  processor.
                                                                                                  A. The subgraph matching problem
                                                                                                                The subgraph matching problem consists of ﬁnding the nodes of a given subgraph S in a larger graph G. More
                                                                                                  precisely, the function ¿ that has to be learned is such that ¿(G ,n                                                                                                                                                                                                                                                                                                                   ) = 1 if n                                                            belongs to a subgraph of G which
                                                                                                                                                                                                                                                                                                                                                                                                                                                         i                  i,j                                                               i,j                                                                                                                                                                   i
                                                                                                  is isomorphic to S, and ¿(Gi,ni,j) = ¡1, otherwise (see Fig. 5). Subgraph matching has a number of practical
                                                                                                  applications, such as object localization and detection of active parts in chemical compounds [58], [59], [60]. Very
                                                                                                  often the subgraph is not exactly known in advance and is available only from a set of examples corrupted by noise.
                                                                                                                                                                                                                                                                                                                                                                                5                                                                                                                                                                                        1                                                          5
                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                             8                                                                                                                                                                                                          
                                                                                                                                                                                                                                                      8                                                                                                                                                                                                                                                                                                                8                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                   1                                                                           5                                                                    1                                                                      5                            
                                                                                                                                                                                                                  1                                                                       5                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                            5                                                                                                                                          5                                                             
                                                                                                                                                                                                                                                      5                                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                 1                                                                                                                                       5                                            2                                                  8
                                                                                                                                                                                                                                               S                                                                                                                                                            G1                                                                                                                                                                               G2
                                                                                                  Fig. 5.                                  Two graphs G , G that contain a subgraph S. The numbers inside the nodes represent the labels. The function ¿ to be learned is
                                                                                                                                                                                                             1                      2
                                                                                                  ¿(G ,n )=1, if n                                                                                                   is a dark gray node, and ¿(G ,n                                                                                                                                  ) = ¡1, if n                                                              is a light gray node.
                                                                                                                       i               i,j                                                           i,j                                                                                                                                                  i               i,j                                                                    i,j
                                                                                                                In our experiments, the dataset L consisted of 600 connected random graphs (constructed using ± = 0.2), equally
                                                                                                  divided into a training, a validation and a test set. A smaller subgraph S, which was randomly generated in each
                                                                                                  trial, was inserted into every graph of the dataset. Thus, each graph G contained at least a copy of S, even if
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     i
                                                                                                             14                                                          c
                                                                                                                      Copyright ° 1994-2006 by The MathWorks, Inc.
                                                                                                  May 24, 2007                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                DRAFT
                   IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                   23
                   more copies might have been included by the random construction procedure. All the nodes had integer labels in
                   the range [0,10] and, in order to deﬁne the correct targets t   =¿(G,n ),abrute force algorithm located all the
                                                                                i,j       i   i,j
                   copies of S in G . Finally, a small Gaussian noise, with zero mean and a standard deviation of 0.25, was added
                                     i
                   to all the labels. As a consequence, all the copies of S in our dataset were different due to the noise introduced.
                     In all the experiments, the state dimension was s = 5 and all the neural networks involved in the GNNs had 5
                   hidden neurons. More network architectures have been tested with similar results.
                     In order to evaluate the relative importance of the labels and the connectivity in the subgraph localization, also
                   a feedforward neural network (FNN) was applied to this test. The FNN had one output, 20 hidden and one input
                   units. The FNN predicted t     using only the label l     of node n    . Thus, the FNN did not use the connectivity
                                               i,j                       n              i,j
                                                                          i,j
                   and exploited only the relative distribution of the labels in S w.r.t. the labels in graphs G.
                     Table III presents the accuracies achieved by the non–linear GNN model (non–linear), the linear GNN model
                   (linear) and the FNN with several dimensions for S and G. The results allow to single out some of the factors that
                   have inﬂuence on the complexity of the problem and on the performance of the models. Obviously, the proportion
                   of positive and negative patterns affects the performance of all the methods. The results improve when |S| is close
                   to |G|, whereas when |S| is about a half of |G|, the performance is lower. In fact, in the latter case the dataset is
                   perfectly balanced and it is more difﬁcult to guess the right response. Moreover, the dimension |S|, by itself, has
                   inﬂuence on the performance, because the labels can assume only 11 different values and when |S| is small most
                   of the nodes of the subgraph can be identiﬁed by their labels. In fact, the performances are better for smaller |S|,
                   even if we restrict our attention to the cases when |G| = 2|S| holds
                     The results show that GNNs always outperform the FNNs, conﬁrming that the GNNs can exploit the label
                   contents and the graph topology at the same time. Moreover, the non–linear GNN model achieved a slightly better
                   performance than the linear one, probably because non–linear GNNs implement a more general model that can
                   approximate a larger class of functions. Finally, it can be observed that the total average error for FNNs is about
                   ﬁfty per cent larger than the GNN error (13.7 for non–linear GNNs, 14.6 for linear GNNs and 22.8 for FNNs).
                   Actually, the relative difference between the GNN and the FNN errors, which measures the advantage provided by
                   the topology, tend to become smaller for larger values of |S| (see last column of Table III). In fact, GNNs use an
                   information diffusion mechanism to decide whether a node belongs or not to the subgraph. When S is larger, more
                   information has to be diffused and, as a consequence, the function to be learned is more complex.
                   B. The Mutagenesis problem
                     The Mutagenesis dataset [61] is a small dataset, which is available online and is often used as a benchmark in the
                   Relational Learning and Inductive Logic Programming literature. It contains the descriptions of 230 nitroaromatic
                   compounds that are common intermediate subproducts of many industrial chemical reactions [62]. The goal of the
                   benchmark consists of learning to recognize the mutagenic compounds. The log mutagenicity was thresholded at
                   zero, so the prediction is a binary classiﬁcation problem.
                     In [62] it is shown that 188 molecules out of 230 are amenable to a linear regression analysis. This subset was
                   May 24, 2007                                                                                                    DRAFT
                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                              24
                                                                            No.of nodesinG
                                                                        6    10    14    18    Avg.
                                                               NL      92.4  90.0  90.0  84.3   89.1
                                                        3      L       93.3  84.5  86.7  84.7   87.3
                                                               FNN 81.4 78.2 79.6 82.2          80.3
                                                               NL      91.3  87.7  84.9  83.3   86.8
                                                        5      L       90.4  85.8  85.3  80.6   85.5
                                                No.            FNN 85.2 73.2 65.2 75.5          74.8
                                                of             NL            89.8  84.6  79.9   84.8
                                                nodes   7      L             91.3  84.4  79.2   85.0
                                                in S           FNN           84.2  66.9  64.6   71.9
                                                               NL            93.3  84.0  77.8   85.0
                                                        9      L             92.2  84.0  77.7   84.7
                                                               FNN           91.6  73.7  67.0   77.4
                                                               NL      91.8  90.2  85.9  81.3
                                                        Avg.   L       91.9  88.5  85.1  80.6
                                                               FNN 83.3 81.8 71.3 72.3
                                                       Total   NL                  87.3
                                                       avg.    L                   86.5
                                                               FNN                 77.2
                                                                      TABLE III
                  THEACCURACIESACHIEVEDBYNON–LINEARMODEL(NL),THELINEARMODEL(L)ANDAFEEDFORWARDNEURALNETWORK(FNN)ON
                                                            SUBGRAPH MATCHING PROBLEM.
                  called “regression friendly”, while the remaining 42 compounds were termed “regression unfriendly”. Many different
                  features have been used in the prediction. Apart from the atom-bond structure (AB), each compound is provided with
                  four global features [62]. The ﬁrst two features are chemical measurements (C): the lowest unoccupied molecule
                  orbital and the water/octanol partition coefﬁcient, while the remaining two are pre-coded structural attributes (PS).
                  Finally the atom-bond description can be used to deﬁne functional groups (FG), e.g. methyl groups and many
                  different rings, that can be used as higher level features. In our experiments, the best results were achieved using
                  AB, C, and PS, without the functional groups. Probably, the reason is that GNNs can recover the substructures that
                  are relevant to the classiﬁcation, exploiting the graphical structure contained in the atom–bond description.
                    In our experiments, each molecule of the dataset was transformed into a graph where nodes represent atoms
                  and edges stand for atom–bonds. The average number of nodes in a molecule is around 26. Node labels contain
                                                                                               15
                  the type of the atom, its energy state and the global properties AB, C and PS   . In each graph there is only one
                  supervised node, the ﬁrst atom in the atom-bond description (Fig. 6). The desired output is positive, if the molecule
                    15Our best results were achieved without the functional groups. Probably, the reason is that GNNs can recover the substructures that are
                  relevant to the classiﬁcation, exploiting the graphical information contained in the atom–bond description.
                  May 24, 2007                                                                                               DRAFT
                           IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                                                          25
                                                                                                                       }            }
                                                                                                                [O, 0.2, 5, −0.2, 0.3, −0.8 ... ]   label
                                                                       Supervised node
                                                                                                                       Oxygen       Atom           Global
                                                                                                                                    properties     properties
                                                                                                                          [C, 0.15, −2, 0.8, 0.3, −0.8 ... ]
                                                                   Unupervised nodes
                                                                                                                               [H, 0.1, 2, 0.4, 0.3, −0.8 ... ]
                                                                                                                    [H, 0.1, 1.5, −0.4, 0.3, −0.8 ... ]
                           Fig. 6.    The atom-bond structure of a molecule represented by a graph with labelled nodes.
                           is mutagenic, and negative, otherwise.
                                                                                                                                   16
                               In Tables IV,V and VI, the results obtained by non–linear GNNs                                          are compared with those achieved by other
                           methods. The presented results were evaluated using a 10-fold cross-validation procedure, i.e. the dataset was
                           randomly split into 10 parts and the experiments were repeated 10 times, each time using a different part as test set
                           and the remaining patterns as training set. The results were averaged on 5 runs of the cross-validation procedure.
                               GNNsachievedthebest accuracy on the regression–unfriendly part (Table V) and on the whole dataset (Table VI),
                           whereas the results are close to the state of the art on the regression–friendly part (Table IV). Surprisingly, the
                           accuracy is better on the unfriendly part, instead of the friendly subset. This fact may be due to small number of
                           patterns contained in the friendly part (42 patterns), which does not allow a good generalization. GNNs suffers
                           from this problem more than the other techniques, because other methods are mainly based on inductive logic
                           programming instead of neural networks.
                               Moreover, whereas most of the approaches showed a higher level of accuracy when applied to the whole dataset
                           with respect to the unfriendly part, the converse holds for GNNs. This suggests GNNs that can capture characteristics
                           of the patterns that are useful to solve the problem but are not homogeneously distributed in the two parts.
                           C. Web page ranking
                               In this experiment, the goal is to learn a web page ranking algorithm, inspired by Google’s PageRank [15].
                           According to PageRank a page is considered authoritative if it is referred by many other pages and if the referring
                                                                                                                                                P              p
                           pages are authoritative. Formally, the PageRank pn of a page n is pn = d                                                              u +(1¡d), where on is
                                                                                                                                                    u∈pa[n] o
                                                                                                                                                                 n
                           the outdegree of n, and d ∈ [0,1] is the damping factor [15]. In this experiments, it is shown that a GNN can
                           learn a modiﬁed version of PageRank, which adapts the “authority” measure according to the page content. For
                           this purpose, a random web graph G containing 5000 nodes was generated, with ± = 0.2. Training, validation and
                              16Some results were already presented in [63].
                           May 24, 2007                                                                                                                                                         DRAFT
                IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                      26
                                                                TABLE IV
                                     THE RESULTS ON THE REGRESSION-FRIENDLY PART OF THE MUTAGENESIS DATASET
                                                 Method       Knowledge   Reference Accuracy
                                              non–linear GNN  AB+C+PS                 94.3
                                              Neural Networks   C+PS        [61]     89.0%
                                                 P-Progol       AB+C        [61]     82.0%
                                                 P-Progol     AB+C+FG       [61]     88.0%
                                                 MFLOG          AB+C        [64]     95.7%
                                                  FOIL           AB         [65]      76%
                                               boosted-FOIL  not available  [66]     88.3%
                                                1nn(dm)          AB         [67]       83
                                                1nn(dm)         AB+C        [67]      91%
                                                 RDBC            AB         [68]      84%
                                                 RDBC           AB+C        [68]      83%
                                                  RSD         AB+C+FG       [69]     92.6%
                                                 SINUS        AB+C+FG       [69]     84.5%
                                                RELAGGS       AB+C+FG       [69]     88.0%
                                                   RS            AB         [70]     88.9%
                                                   RS          AB+FG        [70]     89.9%
                                                   RS        AB+C+PS+FG     [70]     95.8%
                                                 SVM         not available  [71]      91.5
                                                     P
                                                                 TABLE V
                                    THE RESULTS ON THE REGRESSION-UNFRIENDLY PART OF THE MUTAGENESIS DATASET
                                                  Method      Knowledge  Reference Accuracy
                                               non–linear GNN AB+C+PS               96.0%
                                                 1nn(dm)         AB        [67]      72%
                                                 1nn(dm)        AB+C       [67]      72%
                                                  TILDE          AB        [72]      85%
                                                  TILDE         AB+C       [72]      79%
                                                   RDBC          AB        [68]      79%
                                                   RDBC         AB+C       [68]      79%
                test set consisted of different nodes of this graph. More precisely, only 50 nodes were supervised in the training
                set, other 50 nodes belonged to the validation set, and the remaining nodes was in the test set.
                   To each node n, a bidimensional boolean label [an,bn] is attached, that represents whether the page belongs to
                two given topics. If the page n belongs to both topics, then, [an,bn] = [1,1], while if it belongs to only one topic,
                then [a ,b ] = [1,0], or [a ,b ] = [0,1] and if it does not belong to either topics then [a ,b ] = [0,0]. The GNN
                       n n               n n                                                      n n
                May 24, 2007                                                                                       DRAFT
                  IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                            27
                                                                     TABLE VI
                                                   THE RESULTS ON THE WHOLE MUTAGENESIS DATASET
                                                     Method       Knowledge  Reference  Accuracy
                                                  non–linear GNN  AB+C+PS                 90.5%
                                                     1nn(dm)         AB         [67]      81%
                                                     1nn(dm)        AB+C        [67]      88%
                                                      TILDE          AB         [72]      77%
                                                      TILDE         AB+C        [72]      82%
                                                      RDBC           AB         [68]      83%
                                                      RDBC          AB+C        [68]      82%
                                                                                  p
                                                                                  2 n      if (a XORb ) = 1
                  was trained in order to produce the following output: ¿(G,n) =     kpk1        n       n        where p stands
                                                                                  pn       otherwise
                  for the Google’s PageRank.                                        kpk1
                    The design of ranking algorithms capable of mixing together the information provided by web connectivity and
                  page content has been a matter of recent research [73], [74], [75], [76].
                    For this example, only the linear model has been used, because it is naturally suited to approximate the linear
                  dynamics of the PageRank. Moreover, the transition and the forcing networks (see Section 1) were implemented
                  by three layered neural networks with 5 hidden neurons, and the dimension of the state was s = 1. For the output
                  function, g is implemented as g (x ,l ) = x′ ¢¼ (x ,l ), where ¼       is the function realized by a three layered
                            w                    w n n          n  w n n               w
                  neural networks with 5 hidden neurons.
                    Figure 7 shows the output of the GNN ϕ and the target function ¿ on test set. Plot A displays the result for the
                  pages that belong to only one topic and Plot B the result for the other pages. Pages are displayed on horizontal axes
                  and are sorted according to the desired output ¿(G,n). The vertical axes denote the value of function ¿ (continuous
                  lines) and the value of the function implemented by the GNN (the dotted lines). The two plots show clearly that
                  GNNperforms very well.
                    Finally, Fig. 8 displays the error functions during the learning process. The continuous line is the error on the
                  training set, whereas the dotted line is the error on the validation set. It is worth noting that the two curves are
                  always very close and that the error on the validation set is still decreasing after 2400 epochs. This suggests that
                  the GNN does not experiment overﬁtting problems, despite the fact that the learning set consists of only 50 pages
                  from a graph containing 5000 nodes.
                  D. The parity problem
                    Whereas in the web page ranking experiments, the network output mainly depended on the graph connectivity,
                  in this problem the converse holds and the output depends only on the node labels. In fact, the purpose of this
                  experiment is to verify whether the GNN model is capable of discarding information contained in the topology of
                  May 24, 2007                                                                                             DRAFT
                       IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                               28
                                                                                                          1
                                                                                                        0.95
                                                                                                         0.9
                                                                                                        0.85
                                                                                                         0.8
                                                                                                        0.75
                                                                                                         0.7
                                                                                                        0.65
                                                                                                         0.6
                                                                                                        0.550        500        1000       1500       2000       2500
                                                          A                                                                          B
                       Fig. 7.  The desired function ¿ (the continuous lines) and the output of the GNN (the dotted lines) on the pages that belong to only one topic
                       (Plot A) and on the other pages (Plot B). Horizontal axis stands for pages, vertical axis for scores. Pages have been sorted according to the
                       desired value ¿(G,n).
                                                                 2
                                                              10
                                                                 1
                                                              10
                                                                 0
                                                              10
                                                                 −1
                                                              10
                                                                 −2
                                                              10
                                                                 −3
                                                              10 0           500         1000         1500        2000        2500
                       Fig. 8.  The plots of the error function on the training set (continuous line) and on the validation (dashed line) set during learning phase.
                       the graphs, when such information is not needed to solve the problem.
                          Train and validation sets contained 500 graphs, while the test set consisted of 2,000 graphs. Each node was
                       labelled with a random vector of eight binary integers, i.e. lni,j = [¸1,...,¸8], ¸k ∈ {0,1}. The function to be
                       learned is: ¿(G,ni,j) = 1, if lni,j contains an even number of ones, and ¿(G,ni,j) = ¡1, otherwise.
                          The results are shown in Table VII. Note that the output network g , which has to approximate the parity
                                                                                                                     w
                       function, must contain a sufﬁcient number of hidden neurons. It is observed that in this case, it requires 10 hidden
                       neurons in each output network to achieve an accuracy of larger than 97%.
                       May 24, 2007                                                                                                                               DRAFT
                      IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                      29
                         For comparison purposes, a three layer FNN with 20 hidden neurons was applied on this task (see Table VII).
                      The difference between the accuracy achieved by the FNN and the GNNs provides an insight of the impact due to
                      the noise introduced in GNNs by the topology of the graphs. This impact is larger for the non–linear model that is
                      more general and can realize a larger set of functions. However, the results conﬁrms that GNNs are able to learn
                      a function that depends only on node labels.
                                                                                     TABLE VII
                                                                               THE PARITY PROBLEM
                                                            Model    Hidden         Accuracy                 Time
                                                                                 Test      Train      Test       Train
                                                                                                          s       m s
                                                                        2      53.90%     55.41%     10.8       29 22
                                                                                                          s       m s
                                                                        5      92.77%     93.41%     14.5       34 20
                                                                                                          s       m s
                                                          non–linear    10     97.08%     97.48%     21.3       46 03
                                                                                                          s     h   m s
                                                                        20     89.64%     90.05%     34.5      1 07 36
                                                                                                          s     h   m s
                                                                        30     92.04%     92.85%     47.8      1 30 05
                                                                                                         s        m s
                                                                        2      63.51%     64.48%      1.9       34 39
                                                                                                         s        m s
                                                                        5      96.08%     96.55%      2.1       40 02
                                                                                                         s        m s
                                                             linear     10     98.21%     98.50%      2.3       44 25
                                                                                                         s        m s
                                                                        20     99.36%     99.52%      3.0       51 25
                                                                                                         s        m s
                                                                        30     99.40%     99.64%      3.5       59 47
                                                                                                         s      h   m s
                                                             FNN        20     99.46%     99.45%      0.3      1 09 04
                                                                               V. CONCLUSIONS
                         In this paper, we have introduced a new neural network model which can handle graph inputs: the graphs can
                      be acyclic, cyclic, directed, un-directed. A learning algorithm is furnished to estimate the parameters of the neural
                      networks based on the back propagation techniques. The computational complexity of the learning algorithm is
                      considered. Moreover, some promising experimental results have been provided to asses the model.
                                                                          VI. ACKNOWLEDGEMENT
                         The authors acknowledge ﬁnancial support from the Australian Research Council in the form of an International
                      Research Exchange scheme which facilitated the visit by the ﬁrst author to University of Wollongong when the
                      initial work on this paper was performed.
                                                                                  REFERENCES
                       [1] P. Baldi and G. Pollastri, “The principled design of large-scale recursive neural network architectures-dag-rnns and the protein structure
                           prediction problem,” Journal of Machine Learning Research, vol. 4, pp. 575–602, 2003.
                       [2] E. Francesconi, P. Frasconi, M. Gori, S. Marinai, J.Q. Sheng, G. Soda, and A. Sperduti, “Logo recognition by recursive neural networks,”
                           in Lecture Notes in Computer Science — Graphics Recognition, Karl Tombre and Atul K. Chhabra, Eds. Springer, 1997, GREC’97
                           Proceedings.
                      May 24, 2007                                                                                                                       DRAFT
                      IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                           30
                       [3] E. Krahmer, S. Erk, and A. Verleg, “Graph-based generation of referring expressions,” Computational Linguistics, vol. 29, no. 1, 2003.
                       [4] A. Mason and E. Blake, “A graphical representation of the state spaces of hierarchical level-of-detail scene descriptions,” IEEE Trans.
                            Visualization and Computer Graphics, vol. 7, no. 1, pp. 70–75, 2001.
                       [5] L. Baresi and R. Heckel, “Tutorial introduction to graph transformation: A software engineering perspective,” in Lecture Notes in Computer
                            Science (ICGT 2002), 2002, vol. 2505, pp. 402–429.
                       [6] C. Collberg, S. Kobourov, J. Nagra, J. Pitts, and K. Wampler, “A system for graph-based visualization of the evolution of software,” in
                            Proc. of the 2003 ACM symposium on Software visualization SoftVis 2003. 2003, ACM Press.
                       [7] A. Bua, M. Gori, and F. Santini, “Recursive neural networks applied to discourse representation theory,” in Lecture Notes in Computer
                            Science (ICANN’02), 2002, vol. 2415.
                       [8] L. De Raedt, Logical and Relational Learning: From Inductive Logic Programming to Multi-Relational Data Mining, Springer, 2006, in
                            press.
                       [9] “International workshop on statistical relational learning and its connections to other ﬁelds (srl2004),”   in ICML-2004, T Dietterich,
                            L. Getoor, and K. Murphy, Eds.
                      [10] “International workshop on sub-symbolic paradigms in structured domains (rml2005),” in ECML-PKDD 2005, P. Avesani and M. Gori,
                            Eds.
                      [11] “Third international workshop on mining graphs, trees, and sequences (mgts2005),” in ECML-PKDD 2005, S. Nijseen, Ed.
                      [12] “Fourth international workshop on mining graphs, trees, and sequences (mgts2006),” in ECML-PKDD 2006, T. Gaertner, G. Garriga, and
                            T. Meini, Eds.
                      [13] T. Pavlidis, Structural pattern recognition, Springer, Series in Electrophysics, 1977.
                      [14] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adaptive processing of data structures,” IEEE Transactions on Neural
                            Networks, vol. 9, no. 5, pp. 768–786, September 1998.
                      [15] S. Brin and L. Page, “The anatomy of a large–scale hypertextual Web search engine,” in Proceedings of the 7th World Wide Web
                            Conference, Apr. 1998.
                      [16] A. Sperduti and A. Starita, “Supervised neural networks for the classiﬁcation of structures,” IEEE Transactions on Neural Networks, vol.
                            8, pp. 429–459, 1997.
                      [17] M. Hagenbuchner, A. Sperduti, and A. C. Tsoi, “A self-organizing map for adaptive processing of structured data,” IEEE Transactions
                            on Neural Networks, 2003.
                      [18] J. Kleinberg, “Authoritative sources in a hyperlinked environment,” Journal of the ACM, vol. 46, no. 5, pp. 604–632, 1999.
                      [19] A. C. Tsoi, G. Morini, F. Scarselli, M. Hagenbuchner, and M. Maggini, “Adaptive ranking of web pages,” in Proceedings of the 12th
                            WWWConference, Budapest, Hungary, May 2003.
                      [20] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in color images using recursive neural networks,” in Proceedings of
                            the 1st ANNPR Workshop, Florence (Italy), Sept. 2003.
                      [21] M. Bianchini, M. Gori, and F. Scarselli, “Processing directed acyclic graphs with recursive neural networks,” IEEE Transactions on Neural
                            Networks, vol. 12, no. 6, pp. 1464–1470, 2001.
                                 ¨
                      [22] A. Kuchler and C. Goller, “Inductive learning in symbolic domains using structure-driven recurrent neural networks,” in 20th Annual
                                                                              ¨            ¨
                            German Conference on Artiﬁcial Intelligence, G. Gorz and S. Holldobler, Eds., Dresden, Germany, Sept. 1996, vol. 1137 of Lecture Notes
                            in Computer Science, Springer.
                      [23] T. Schmitt and C. Goller, “Relating chemical structure to activity: An application of the neural folding architecture,” in Workshop on
                            Fuzzy–Neuro Systems ’98 and Conference on Egineering Applications of Neural Networks, EANN ’98, 1998.
                      [24] M. Hagenbuchner and A. C. Tsoi, “Recursive cascade correlation and recursive multilayer perceptron, a comparison,” IEEE Transactions
                            on Neural Networks, 2002 (Submitted).
                      [25] M. Gori, M. Maggini, E. Martinelli, and F. Scarselli, “Learning user proﬁles in NAUTILUS,” in International Conference on Adaptive
                            Hypermedia and Adaptive Web–based Systems, Trento (Italy), August 2000.
                      [26] M. Bianchini, P. Mazzoni, L. Sarti, and F. Scarselli, “Face spotting in color images using recursive neural networks,” in Proceedings of
                            WIRN03, Vietri sul Mare (Italy), July 2003.
                      [27] B. Hammer and J. Jain, “Neural methods for non-standard data,” in Proceedings of the 12th European Symposium on Artiﬁcial Neural
                            Networks, M.Verleysen, Ed., 2004, pp. 281–292.
                      May 24, 2007                                                                                                                           DRAFT
           IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR 31
                ¨
           [28] T. Gartner, “Kernel-based learning in multi-relational data mining,” ACM SIGKDD Explorations, vol. 5, no. 1, pp. 49–58, 2003.
                ¨
           [29] T. Gartner, J.W. Lloyd, and P.A. Flach, “Kernels and distances for structured data,” Machine Learning, vol. 57, no. 3, pp. 205–232, 2004.
           [30] R.I. Kondor and J. Lafferty, “Diffusion kernels on graphs and other discrete structures,” in Proc. 19th International Conference on Machine
             Learning (ICML2002), C. Sammut and A.G. (eds) Hoffmann, Eds. 2002, pp. 315–322, Morgan Kaufmann Publishers Inc.
           [31] H. Kashima, K. Tsuda, and A. Inokuchi, “Marginalized kernels between labeled graphs,” in Proc. 20th International Conference on
             Machine Learning (, T. Fawcett and N. (eds) Mishra, Eds. 2003, pp. 321–328, AAAI Press.
                 ´
           [32] P. Mahe, N. Ueda, T. Akutsu, Perret J.-L., and J.-P. Vert, “Extensions of marginalized graph kernels,” in Proc. 21th International Conference
             on Machine Learning (ICML2004). 2004, ACM Press.
           [33] M. Collins and N. Duffy, “Convolution kernels for natural language,” in Advances in Neural Information Processing Systems 14, Proc. of
             NIPS 2001, T. G. Dietterich, S. Becker, and Z. Ghahramani, Eds. 2002, pp. 625–632, MIT Press.
           [34] J. Suzuki, Y. Sasaki, and E. Maeda, “Kernels for structured natural language data.,” in NIPS, 2003.
           [35] J. Suzuki, H. Isozaki, and E. Maeda, “Convolution kernels with feature selection for natural language processing tasks.,” in ACL, 2004,
             pp. 119–126.
           [36] J. Cho, H. Garcia-Molina, and L. Page, “Efﬁcient crawling through url ordering,” in Proceedings of the 7th World Wide Web Conference,
             Brisbane, Australia, Apr. 1998.
           [37] A. C. Tsoi, M. Hagenbuchner, and F. Scarselli, “Computing customized page ranks,” ACM Transactions on Internet Technology, vol. 6,
             no. 4, pp. 381–414, Nov. 2006.
           [38] H. Chang, D. Cohn, and McCallum A. K., “Learning to create customized authority lists,” in Proceedings of the 17th International
             Conference on Machine Learning. 2000, pp. 127–134, Morgan Kaufmann.
           [39] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “Computation capabilities of graph neural networks,” IEEE
             Transactions on Neural Networks, 2007, Submitted.
           [40] Mohamed A. Khamsi, An Introduction to Metric Spaces and Fixed Point Theory, John Wiley & Sons Inc, 2001.
           [41] M. Bianchini, M. Maggini, L. Sarti, and F. Scarselli, “Recursive neural networks for processing graphs with labelled edges: Theory and
             applications,” Neural Networks - Special Issue on Neural Networks and Kernel Methods for Structured Domains, 2005, to appear.
           [42] M. J. D. Powell, “An efﬁcient method for ﬁnding the minimum of a function of several variables without calculating derivatives,”
             Comput. J., vol. 7, pp. 155–162, 1964.
           [43] W. T. Miller III, R. Sutton, and P. Ed. Werbos, Neural Network for Control, MIT Press, Camrbidge, Mass., 1990.
           [44] A. C. Tsoi, “Adaptive processing of sequences and data structures, international summer school on neural networks, ”e.r. caianiello”, vietri
             sul mare, salerno, italy, september 6-13, 1997, tutorial lectures,” in Summer School on Neural Networks, C. Lee Giles and Marco Gori,
             Eds. 1998, vol. 1387 of Lecture Notes in Computer Science, pp. 27–62, Springer.
           [45] L.B. Almeida, “A learning rule for asynchronous perceptrons with feedback in a combinatorial environment,” in IEEE International
             Conference on Neural Networks, M. Caudill and C. Butler, Eds., San Diego, 1987, 1987, vol. 2, pp. 609–618, IEEE, New York.
           [46] F.J. Pineda, “Generalization of back–propagation to recurrent neural networks,” Physical Review Letters, vol. 59, pp. 2229–2232, 1987.
           [47] W. Rudin, Real and Complex Analysis, Third Edition, McGraw Hill, New York, 1987.
           [48] A. M. Bianucci, A. Micheli, A. Sperduti, and A. Starita, “Analysis of the internal representations developed by neural networks for structures
             applied to quantitative structure-activity relationship studies of benzodiazepines,” Journal of Chemical Information and Computer Sciences,
             vol. 41, no. 1, pp. 202–218, 2001.
           [49] M. Hagenbuchner, A. C. Tsoi, and A. Sperduti, “A supervised self-organising map for structured data,” in WSOM 2001 - Advances in
             Self-Organising Maps, N.Allinson, H.Yin, L.Allinson, and J.Slack, Eds. June 2001, pp. 21–28, Springer.
           [50] E. Seneta, Non–negative matrices and Markov chains, Springer Verlag, 1981, Chapter 4, pages 112–158.
           [51] D. E. Rumelhart, J.L. McClelland, and the PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of
             Cognition, vol. 1, MIT Press, Cambridge, 1986.
           [52] F. Scarselli, S.L. Yong, M. Gori, M. Hagenbuchner, A. C. Tsoi, and M. Maggini, “Graph neural networks for ranking web pages,” in
             Proc. of the 2005 IEEE/WIC/ACM Conference on Web Intellligence, 2005.
           [53] M. Gori, M. Hagenbuchner, F. Scarselli, and A. C. Tsoi, “Graphical-based learning environment for pattern recognition,” in Structural,
             Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshops, SSPR 2004 and SPR 2004. Lecture Notes in Computer
             Science, 2004, vol. 3138, pp. 42–56.
           May 24, 2007                                                    DRAFT
                      IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. Y, MONTH, YEAR                                                                      32
                      [54] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in graph domains,” in Proceedings of the International Joint
                           Conference on Neural Networks, 2005.
                      [55] V. Di Massa, G. Monfardini, L. Sarti, F. Scarselli, M. Maggini, and M. Gori, “A comparison between recursive neural networks and graph
                           neural networks,” in International Joint Conference on Neural Networks, July 2006.
                      [56] G. Monfardini, V. Di Massa, F. Scarselli, and M. Gori, “Graph neural networks for object localization,” in 17-th European Conference
                           on Artiﬁcial Intelligence, August 2006.
                      [57] The GNN toolbox, ,” Available at http://airgroup.dii.unisi.it/projects/GraphNeuralNetwork/download.htm.
                      [58] H. Bunke, “Graph matching: Theoretical foundations, algorithms, and applications,” in Proceedings of Vision Interface 2000, Montreal,
                           2000, pp. 82–88.
                      [59] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Graph matching applications in pattern recognition and image processing,” in International
                           Conference on Image Processing, September 2003, vol. 2, pp. 21–24.
                      [60] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Thirty years of graph matching in pattern recognition,” International Journal of Pattern
                           Recognition and Artiﬁcial Intelligence, vol. 18, no. 3, pp. 265–268, 2004.
                      [61] A. Srinivasan, S. Muggleton, R.D. King, and M.J.E. Sternberg, “Mutagenesis: Ilp experiments in a non-determinate biological domain,” in
                                                                                                                           ¨
                           Proceedings of the 4th International Workshop on Inductive Logic Programming, 1994, Gesellschaft fur Mathematik und Datenverarbeitung
                           MBH, pp. 217–232.
                      [62] A. K. Debnath, R.L. Lopex de Compandre, G. Debnath, A.J. Schusterman, and C. Hansch, “Structure-activity relationship of mutagenic
                           aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity,” Journal of Medicinal
                           Chemistry, vol. 34, no. 2, pp. 786–797, 1991.
                      [63] W. Uwents, G. Monfardini, H. Blockeel, F. Scarselli, and M Gori, “Two connectionist models for graph processing: an experimental
                           comparison on relational data,” in European Conference on Machine Learning, 2006.
                      [64] S. Kramer and L. De Raedt, “Feature Construction with Version Spaces for Biochemical Applications,” Proceedings of the Eighteenth
                           International Conference on Machine Learning table of contents, pp. 258–265, 2001.
                      [65] J.R. Quinlan and R.M. Cameron-Jones, “FOIL: A midterm report,” Proceedings of the European Conference on Machine Learning, pp.
                           3–20, 1993.
                      [66] J.R. Quinlan, “Boosting ﬁrst-order learning,” LNCS, vol. 1160, pp. 143, 1996.
                      [67] J. Ramon, Clustering and instance based learning in ﬁrst order logic, Ph.D. thesis, K.U. Leuven, Belgium, 2002.
                      [68] M. Kirsten,   Multirelational Distance-Based Clustering,   Ph.D. thesis, School of Computer Science, Otto-von-Guericke University,
                           Magdeburg, Germany, 2002.
                      [69] M.A. Krogel, S. Rawles, F. Zelezny, P. Flach, N. Lavrac, and S. Wrobel, “Comparative evaluation of approaches to propositionalization,”
                           Proc. 13th Int. Conf. on Inductive Logic Programming, pp. 197–214, 2003.
                      [70] S. Muggleton, “Machine learning for systems biology,” in 15th International Conference on Inductive Logic Programming (ILP 2005)
                           Bonn, Germany, August 10 - 13, 2005, 2005.
                                  ´
                      [71] A. Woznica, A. Kalousis, and M. Hilario, “Matching based kernels for labeled graphs,” in Proceedings of the International Workshop
                                                                                                                         ¨
                           on Mining and Learning with Graphs (MLG 2006) in conjunction with ECML/PKDD 2006, T. Gartner, G.C. Garriga, and T. Meinl, Eds.,
                           2006, pp. 97–108.
                      [72] L. De Raedt and H. Blockeel, “Using logical decision trees for clustering,” in Proceedings of the 7th International Workshop on Inductive
                           Logic Programming ILP 1997. 1997, vol. 1297 of Lecture Notes in Artiﬁcial Intelligence, pp. 133–141, Springer-Verlag.
                      [73] M. Diligenti, M. Gori, and M. Maggini, “Web page scoring systems for horizontal and vertical search,” in Proceedings of the 11th World
                           Wide Web Conference, 2002.
                      [74] T. H. Haveliwala, “Topic sensitive pagerank,” in Proceedings of the 11th World Wide Web Conference (WWW11), 2002, Available on the
                           Internet at http://dbpubs.stanford.edu:8090/pub/2002-6.
                      [75] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th World Wide Web Conference, 20–24May 2003.
                      [76] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “Computing personalized pagerankss,” in Proceedings of the 12th WWW Conference,
                           2003.
                      May 24, 2007                                                                                                                       DRAFT
