                               Bigtable: A Distributed Storage System for Structured Data
                           Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach
                                     MikeBurrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber
                                              {fay,jeff,sanjay,wilsonh,kerr,m3b,tushar,ﬁkes,gruber}@google.com
                                                                     Google, Inc.
                                         Abstract                             achieved scalability and high performance, but Bigtable
                     Bigtable is a distributed storage system for managing    providesadifferentinterfacethansuchsystems. Bigtable
                  structured data that is designed to scale to a very large   does not support a full relational data model; instead, it
                  size: petabytes of data across thousands of commodity       provides clients with a simple data model that supports
                  servers. Many projects at Google store data in Bigtable,    dynamic control over data layout and format, and al-
                  including web indexing, Google Earth, and Google Fi-        lows clients to reason about the locality properties of the
                  nance. These applications place very different demands      data represented in the underlying storage. Data is in-
                  on Bigtable, both in terms of data size (from URLs to       dexedusingrowandcolumnnamesthatcanbearbitrary
                  webpagestosatellite imagery)andlatencyrequirements          strings. Bigtable also treats data as uninterpreted strings,
                  (frombackendbulkprocessingtoreal-timedataserving).          although clients often serialize various forms of struc-
                  Despite these varied demands, Bigtable has successfully     tured and semi-structured data into these strings. Clients
                  provided a ﬂexible, high-performance solution for all of    can control the locality of their data through careful
                  theseGoogleproducts. Inthispaperwedescribethesim-           choices in their schemas. Finally, Bigtable schema pa-
                  ple data model providedbyBigtable, which gives clients      rameters let clients dynamically control whether to serve
                  dynamiccontroloverdatalayoutandformat,andwede-              data out of memoryor from disk.
                  scribe the design and implementation of Bigtable.              Section 2 describes the data model in more detail, and
                                                                              Section 3 provides an overview of the client API. Sec-
                  1 Introduction                                              tion 4 brieﬂy describes the underlyingGoogleinfrastruc-
                                                                              ture on which Bigtable depends. Section 5 describes the
                  Over the last two and a half years we have designed,        fundamentals of the Bigtable implementation, and Sec-
                  implemented, and deployed a distributed storage system      tion 6 describes some of the reﬁnements that we made
                  for managing structured data at Google called Bigtable.     to improve Bigtable’s performance. Section 7 provides
                  Bigtable is designed to reliably scale to petabytes of      measurements of Bigtable’s performance. We describe
                  data and thousands of machines. Bigtable has achieved       several examples of how Bigtable is used at Google
                  several goals: wide applicability, scalability, high per-   in Section 8, and discuss some lessons we learned in
                  formance, and high availability.   Bigtable is used by      designing and supporting Bigtable in Section 9.      Fi-
                  more than sixty Google products and projects, includ-       nally, Section 10 describes related work, and Section 11
                  ing Google Analytics, Google Finance, Orkut, Person-        presents our conclusions.
                  alized Search, Writely, and Google Earth. These prod-
                  ucts use Bigtable for a variety of demanding workloads,     2 DataModel
                  which range from throughput-oriented batch-processing
                  jobs to latency-sensitive serving of data to end users.
                  TheBigtableclustersusedbytheseproductsspanawide             A Bigtable is a sparse, distributed, persistent multi-
                  range of conﬁgurations, from a handful to thousands of      dimensional sorted map. The map is indexed by a row
                  servers, andstore upto severalhundredterabytesofdata.       key, columnkey,anda timestamp; eachvaluein the map
                     Inmanyways,Bigtableresemblesadatabase: itshares          is an uninterpreted array of bytes.
                  many implementation strategies with databases. Paral-
                  lel databases [14] and main-memorydatabases [13] have       (row:string, column:string, time:int64) → string
                   ToappearinOSDI2006                                                                                               1
                                                       "contents:"        "anchor:cnnsi.com"     "anchor:my.look.ca"
                                                         "<html>..."     t3                 t                        t
                              "com.cnn.www"             "<html>..."    t5       "CNN"       9       "CNN.com"         8
                                                      "<html>..."     t6
                  Figure 1: A slice of an example table that stores Web pages. The row name is a reversed URL. The contents column family con-
                  tains the page contents, and the anchor column family contains the text of any anchors that reference the page. CNN’s home page
                  is referenced by both the Sports Illustrated and the MY-look home pages, so the row contains columns named anchor:cnnsi.com
                  and anchor:my.look.ca. Each anchor cell has one version; the contents column has three versions, at timestamps t3, t5, and t6.
                     Wesettledonthisdatamodelafterexaminingavariety            ColumnFamilies
                  of potential uses of a Bigtable-like system. As one con-     Column keys are grouped into sets called column fami-
                  crete example that drove some of our design decisions,       lies, which form the basic unit of access control. All data
                  suppose we want to keep a copy of a large collection of      stored in a column family is usually of the same type (we
                  webpagesandrelatedinformationthat couldbe used by            compress data in the same column family together). A
                  many different projects; let us call this particular table   columnfamilymustbecreatedbeforedatacanbestored
                  the Webtable. In Webtable, we would use URLs as row          under any column key in that family; after a family has
                  keys,variousaspectsofwebpagesascolumnnames,and               been created, any column key within the family can be
                  store the contents of the web pages in the contents: col-    used. It is our intent that the number of distinct column
                  umn under the timestamps when they were fetched, as          families in a table be small (in the hundreds at most), and
                  illustrated in Figure 1.                                     that families rarely change during operation. In contrast,
                                                                               a table may have an unboundednumberof columns.
                                                                                 A column key is named using the following syntax:
                  Rows                                                         family:qualiﬁer. Column family names must be print-
                                                                               able, but qualiﬁers may be arbitrary strings. An exam-
                                                                               ple column family for the Webtable is language, which
                  Therowkeysinatablearearbitrarystrings(currentlyup            stores the languagein whicha webpagewas written. We
                  to 64KB in size, although 10-100 bytes is a typical size     use only one column key in the language family, and it
                  for most of our users). Every read or write of data under    stores each web page’s language ID. Another useful col-
                  a single row key is atomic (regardless of the number of      umnfamilyforthis table is anchor; each column key in
                  different columns being read or written in the row), a       this family represents a single anchor, as shown in Fig-
                  design decision that makes it easier for clients to reason   ure 1. The qualiﬁer is the name of the referring site; the
                  aboutthesystem’sbehaviorinthepresenceofconcurrent            cell contents is the link text.
                  updates to the same row.                                       Access control and both disk and memory account-
                     Bigtable maintains data in lexicographic order by row     ing are performed at the column-family level. In our
                  key. Therowrangeforatableisdynamicallypartitioned.           Webtable example, these controls allow us to manage
                  Eachrowrangeiscalledatablet,whichistheunitofdis-             severaldifferenttypesofapplications: somethataddnew
                  tribution and load balancing. As a result, reads of short    basedata,somethatreadthebasedataandcreatederived
                  row ranges are efﬁcient and typically require communi-       columnfamilies, and some that are only allowed to view
                  cation with only a small number of machines. Clients         existing data (and possibly not even to view all of the
                  can exploit this property by selecting their row keys so     existing families for privacy reasons).
                  that they get good locality for their data accesses. For
                  example, in Webtable, pages in the same domain are           Timestamps
                  grouped together into contiguous rows by reversing the
                  hostname components of the URLs. For example, we             Each cell in a Bigtable can contain multiple versions of
                  store data for maps.google.com/index.html under the          the same data; these versions are indexed by timestamp.
                  key com.google.maps/index.html. Storing pages from           Bigtable timestamps are 64-bit integers. They can be as-
                  the same domain near each other makes some host and          signed by Bigtable, in which case they represent “real
                  domainanalyses moreefﬁcient.                                 time”inmicroseconds,orbeexplicitlyassignedbyclient
                   ToappearinOSDI2006                                                                                                2
                   // Open the table                                             Scanner scanner(T);
                   Table *T = OpenOrDie("/bigtable/web/webtable");               ScanStream *stream;
                                                                                 stream = scanner.FetchColumnFamily("anchor");
                   // Write a new anchor and delete an old anchor                stream->SetReturnAllVersions();
                   RowMutation r1(T, "com.cnn.www");                             scanner.Lookup("com.cnn.www");
                   r1.Set("anchor:www.c-span.org", "CNN");                       for (; !stream->Done(); stream->Next()) {
                   r1.Delete("anchor:www.abc.com");                                printf("%s %s %lld %s\n",
                   Operation op;                                                            scanner.RowName(),
                   Apply(&op, &r1);                                                         stream->ColumnName(),
                                                                                            stream->MicroTimestamp(),
                                                                                            stream->Value());
                                 Figure 2: Writing to Bigtable.                  }
                   applications. Applications that need to avoid collisions                   Figure 3: Reading from Bigtable.
                   must generate unique timestamps themselves. Different
                   versions of a cell are stored in decreasing timestamp or-       Bigtable supports several other features that allow the
                   der, so that the most recent versions can be read ﬁrst.       user to manipulate data in more complex ways. First,
                     Tomakethemanagementofversioneddataless oner-                Bigtable supports single-row transactions, which can be
                   ous, we support two per-column-family settings that tell      used to perform atomic read-modify-write sequences on
                   Bigtable to garbage-collect cell versions automatically.      datastoredunderasinglerowkey. Bigtabledoesnotcur-
                   Theclient can specify either that only the last n versions    rently support general transactions across row keys, al-
                   of a cell be kept, or that only new-enough versions be        thoughit providesaninterface for batchingwrites across
                   kept (e.g., only keep values that were written in the last    row keys at the clients. Second, Bigtable allows cells
                   seven days).                                                  to be used as integer counters. Finally, Bigtable sup-
                     In our Webtable example, we set the timestamps of           ports the execution of client-supplied scripts in the ad-
                   the crawled pages stored in the contents: column to           dress spaces of the servers. The scripts are written in a
                   the times at which these page versions were actually          languagedevelopedat Googleforprocessingdata called
                   crawled. The garbage-collection mechanism described           Sawzall [28]. At the moment, our Sawzall-based API
                   abovelets us keep only the most recent three versions of      does not allow client scripts to write back into Bigtable,
                   every page.                                                   but it does allow various forms of data transformation,
                                                                                 ﬁltering based on arbitrary expressions, and summariza-
                   3 API                                                         tion via a variety of operators.
                                                                                   Bigtable can be used with MapReduce [12], a frame-
                                                                                 work for running large-scale parallel computations de-
                   The Bigtable API provides functions for creating and          veloped at Google. We have written a set of wrappers
                   deleting tables and column families.     It also provides     that allow a Bigtable to be used both as an input source
                   functions for changing cluster, table, and column family      andas an output target for MapReduce jobs.
                   metadata, such as access control rights.
                     Client applications can write or delete values in           4 Building Blocks
                   Bigtable, look up values from individual rows, or iter-
                   ate over a subset of the data in a table. Figure 2 shows      Bigtable is built on several other pieces of Google in-
                   C++codethatusesaRowMutationabstractiontoper-                  frastructure. Bigtable uses the distributed Google File
                   form a series of updates. (Irrelevant details were elided     System(GFS)[17]tostoreloganddataﬁles. ABigtable
                   to keep the example short.) The call to Apply performs        cluster typically operates in a shared pool of machines
                   an atomic mutation to the Webtable: it adds one anchor        that run a wide variety of other distributed applications,
                   to www.cnn.comanddeletes adifferentanchor.                    and Bigtable processes often share the same machines
                     Figure 3 shows C++ code that uses a Scanner ab-             with processes from other applications.     Bigtable de-
                   straction to iterate over all anchors in a particular row.    pends on a cluster management system for scheduling
                   Clients can iterate over multiple column families, and        jobs, managing resources on shared machines, dealing
                   there are several mechanisms for limiting the rows,           with machine failures, and monitoringmachine status.
                   columns, and timestamps produced by a scan. For ex-             The Google SSTable ﬁle format is used internally to
                   ample, we could restrict the scan above to only produce       store Bigtable data. An SSTable provides a persistent,
                   anchors whose columns match the regular expression            orderedimmutablemapfromkeystovalues,whereboth
                   anchor:*.cnn.com, or to only produce anchors whose            keysandvaluesarearbitrarybytestrings. Operationsare
                   timestamps fall within ten days of the current time.          providedto look up the value associated with a speciﬁed
                    ToappearinOSDI2006                                                                                                   3
                     key, and to iterate over all key/value pairs in a speciﬁed         dynamically added (or removed) from a cluster to acco-
                     key range. Internally, each SSTable contains a sequence            modatechangesin workloads.
                     of blocks (typically each block is 64KB in size, but this             Themasterisresponsibleforassigningtabletstotablet
                     is conﬁgurable). A block index (stored at the end of the           servers, detecting the addition and expiration of tablet
                     SSTable) is used to locate blocks; the index is loaded             servers, balancing tablet-server load, and garbage col-
                     into memory when the SSTable is opened. A lookup                   lection of ﬁles in GFS. In addition, it handles schema
                     can be performed with a single disk seek: we ﬁrst ﬁnd              changessuchas table and column family creations.
                     the appropriate block by performing a binary search in                Each tablet server manages a set of tablets (typically
                     the in-memory index, and then reading the appropriate              wehavesomewherebetweententoathousandtabletsper
                     block from disk. Optionally, an SSTable can be com-                tablet server). The tablet server handles read and write
                     pletely mappedintomemory,whichallowsustoperform                    requests to the tablets that it has loaded, and also splits
                     lookups and scans without touching disk.                           tablets that have grown too large.
                       Bigtable relies on a highly-available and persistent                As with many single-master distributed storage sys-
                     distributed lock service called Chubby [8]. A Chubby               tems[17,21],clientdatadoesnotmovethroughthemas-
                     service consists of ﬁve active replicas, one of which is           ter: clients communicate directly with tablet servers for
                     elected to be the master and actively serve requests. The          reads and writes. Because Bigtable clients do not rely on
                     service is live whena majorityof the replicas are running          the master for tablet location information, most clients
                     and can communicate with each other. Chubby uses the               nevercommunicatewiththemaster. Asaresult,themas-
                     Paxos algorithm [9, 23] to keep its replicas consistent in         ter is lightly loaded in practice.
                     the face of failure. Chubby provides a namespace that                 ABigtable cluster stores a number of tables. Each ta-
                     consists of directories and small ﬁles. Each directory or          ble consists of a set of tablets, and each tablet contains
                     ﬁle can be used as a lock, and reads and writes to a ﬁle           all data associated with a row range. Initially, each table
                     are atomic. The Chubby client library provides consis-             consists of just one tablet. As a table grows, it is auto-
                     tent caching of Chubby ﬁles. Each Chubby client main-              matically split into multiple tablets, each approximately
                     tains a session with a Chubby service. A client’s session          100-200MBinsizebydefault.
                     expiresif it is unable to renew its session lease within the
                     lease expiration time. When a client’s session expires, it         5.1    Tablet Location
                     loses any locks and open handles. Chubby clients can
                     also register callbacks on Chubby ﬁles and directories             Weuseathree-levelhierarchyanalogoustothatofaB+-
                     for notiﬁcation of changes or session expiration.                  tree [10] to store tablet location information (Figure 4).
                       Bigtable uses Chubby for a variety of tasks: to ensure                                                             UserTable1
                     that there is at most one active master at any time; to                                                 Other             ...
                     store the bootstrap location of Bigtable data (see Sec-                                            METADATA
                    tion 5.1); to discover tablet servers and ﬁnalize tablet                                                 tablets           ...
                                                                                                                              ...              .
                     server deaths (see Section 5.2); to store Bigtable schema                                                                 .
                                                                                                        Root tablet           ...              .
                    information (the column family information for each ta-             Chubby file    (1st METADATA tablet)  ...
                    ble); andto store access controllists. If Chubbybecomes                                  ...                          UserTableN
                                                                                                                              .
                                                                                                                              .                ...
                    unavailable for an extended period of time, Bigtable be-                                                  .
                                                                                                                                               .
                                                                                                                                               .
                     comes unavailable.      We recently measured this effect                                                 ...              .
                     in 14 Bigtable clusters spanning 11 Chubby instances.                                                                     ...
                     The average percentage of Bigtable server hours during
                     whichsomedatastoredinBigtablewasnotavailabledue                                 Figure 4: Tablet location hierarchy.
                     to Chubby unavailability (caused by either Chubby out-
                     ages or network issues) was 0.0047%. The percentage                   The ﬁrst level is a ﬁle stored in Chubby that contains
                     for the single cluster that was most affected by Chubby            the location of the root tablet. The root tablet contains
                     unavailability was 0.0326%.                                        the location of all tablets in a special METADATA table.
                                                                                        Each METADATAtablet contains the location of a set of
                     5 Implementation                                                   user tablets. The root tablet is just the ﬁrst tablet in the
                                                                                        METADATA table, but is treated specially—it is never
                                                                                        split—to ensure that the tablet location hierarchy has no
                     The Bigtable implementation has three major compo-                 morethanthreelevels.
                     nents: a library that is linked into every client, one mas-           The METADATA table stores the location of a tablet
                     ter server, and many tablet servers. Tablet servers can be         under a row key that is an encoding of the tablet’s table
                     ToappearinOSDI2006                                                                                                              4
                     identiﬁer and its end row. Each METADATA row stores                    The master is responsible for detecting when a tablet
                     approximately 1KB of data in memory. With a modest                  server is no longer serving its tablets, and for reassign-
                     limit of 128 MB METADATA tablets, our three-level lo-               ing those tablets as soon as possible. To detect when a
                     cation scheme is sufﬁcient to address 234 tablets (or 261           tablet server is no longer serving its tablets, the master
                     bytes in 128 MB tablets).                                           periodically asks each tablet server for the status of its
                        The client library caches tablet locations. If the client        lock. If a tablet server reports that it has lost its lock,
                     does not know the location of a tablet, or if it discov-            or if the master was unable to reach a server during its
                     ers that cached location information is incorrect, then             last several attempts, the master attempts to acquire an
                     it recursively moves up the tablet location hierarchy.              exclusivelock on the server’s ﬁle. If the master is able to
                     If the client’s cache is empty, the location algorithm              acquirethelock,thenChubbyisliveandthetabletserver
                     requires three network round-trips, including one read              is either dead or having trouble reaching Chubby, so the
                     from Chubby. If the client’s cache is stale, the location           masterensuresthatthetabletservercanneverserveagain
                     algorithm could take up to six round-trips, because stale           by deleting its server ﬁle. Once a server’s ﬁle has been
                     cacheentriesareonlydiscovereduponmisses(assuming                    deleted, the master canmoveallthetabletsthatwerepre-
                     that METADATA tablets do not move very frequently).                 viously assigned to that server into the set of unassigned
                     Although tablet locations are stored in memory, so no               tablets. To ensure that a Bigtable cluster is not vulnera-
                     GFS accesses are required, we further reduce this cost              bletonetworkingissuesbetweenthemasterandChubby,
                     in the commoncase by having the client library prefetch             the masterkills itself if its Chubby session expires. How-
                     tablet locations: it reads the metadata for more than one           ever, as described above, master failures do not change
                     tablet whenever it reads the METADATA table.                        the assignment of tablets to tablet servers.
                        We also store secondary information in the                          When a master is started by the cluster management
                     METADATA table, including a log of all events per-                  system, it needs to discover the current tablet assign-
                     taining to each tablet (such as when a server begins                ments before it can change them. The master executes
                     serving it).  This information is helpful for debugging             the following steps at startup.       (1) The master grabs
                     andperformanceanalysis.                                             a unique master lock in Chubby, which prevents con-
                                                                                         current master instantiations. (2) The master scans the
                     5.2    Tablet Assignment                                            servers directory in Chubby to ﬁnd the live servers.
                                                                                         (3) The master communicates with every live tablet
                     Eachtablet is assigned to one tablet server at a time. The          server to discover what tablets are already assigned to
                     master keeps track of the set of live tablet servers, and           each server. (4) The master scans the METADATA table
                     the current assignment of tablets to tablet servers, in-            to learn the set of tablets. Whenever this scan encounters
                     cluding which tablets are unassigned. When a tablet is              a tablet that is not already assigned, the master adds the
                     unassigned, and a tablet server with sufﬁcient room for             tablet to the set of unassigned tablets, which makes the
                     the tablet is available, the master assigns the tablet by           tablet eligible for tablet assignment.
                     sending a tablet load request to the tablet server.                    One complication is that the scan of the METADATA
                        Bigtable uses Chubby to keep track of tablet servers.            table cannot happen until the METADATA tablets have
                     When a tablet server starts, it creates, and acquires an            been assigned. Therefore, before starting this scan (step
                     exclusive lock on, a uniquely-named ﬁle in a speciﬁc                4), the master adds the root tablet to the set of unassigned
                     Chubby directory. The master monitors this directory                tablets if an assignment for the root tablet was not dis-
                     (the servers directory) to discover tablet servers. A tablet        coveredduringstep3. Thisadditionensuresthattheroot
                     server stops serving its tablets if it loses its exclusive          tablet will be assigned. Because the root tablet contains
                     lock: e.g., due to a network partition that caused the              the names of all METADATA tablets, the master knows
                     server to lose its Chubby session. (Chubby provides an              about all of them after it has scanned the root tablet.
                     efﬁcient mechanism that allows a tablet server to check                The set of existing tablets only changes when a ta-
                     whether it still holds its lock without incurring network           ble is created or deleted, two existing tablets are merged
                     trafﬁc.) A tablet server will attempt to reacquire an ex-           to form one larger tablet, or an existing tablet is split
                     clusive lock on its ﬁle as long as the ﬁle still exists. If the     into two smaller tablets.      The master is able to keep
                     ﬁle no longer exists, then the tablet server will never be          track of these changes because it initiates all but the last.
                     able to serve again, so it kills itself. Whenever a tablet          Tablet splits are treated specially since they are initi-
                     server terminates (e.g., because the cluster management             ated by a tablet server. The tablet server commits the
                     system is removing the tablet server’s machine from the             split by recording information for the new tablet in the
                     cluster), it attempts to release its lock so that the master        METADATAtable. Whenthesplithascommitted,itnoti-
                     will reassign its tablets more quickly.                             ﬁesthemaster. Incasethe split notiﬁcationis lost (either
                      ToappearinOSDI2006                                                                                                               5
                   because the tablet server or the master died), the master     5.4    Compactions
                   detects the new tablet when it asks a tablet server to load   Aswriteoperationsexecute,the size ofthe memtablein-
                   the tablet that has now split. The tablet server will notify  creases. Whenthememtablesizereachesathreshold,the
                   the master of the split, because the tablet entry it ﬁnds in  memtable is frozen, a new memtable is created, and the
                   the METADATA table will specify only a portion of the         frozen memtable is converted to an SSTable and written
                   tablet that the master asked it to load.                      to GFS. This minor compaction process has two goals:
                   5.3    Tablet Serving                                         it shrinks the memory usage of the tablet server, and it
                                                                                 reduces the amount of data that has to be read from the
                   The persistent state of a tablet is stored in GFS, as illus-  commit log during recovery if this server dies. Incom-
                   trated in Figure 5. Updates are committed to a commit         ing read and write operations can continue while com-
                   log that stores redo records. Of these updates, the re-       pactions occur.
                   cently committed ones are stored in memory in a sorted          EveryminorcompactioncreatesanewSSTable. Ifthis
                   buffercalledamemtable;theolderupdatesarestoredina             behavior continued unchecked, read operations might
                   sequenceofSSTables. Torecoveratablet,atabletserver            need to merge updates from an arbitrary number of
                                                                                 SSTables. Instead, we bound the number of such ﬁles
                                  memtable                                       by periodically executing a merging compaction in the
                                                         Read Op                 background. A merging compaction reads the contents
                                                                                 of a few SSTables and the memtable, and writes out a
                     Memory                                                      newSSTable. The input SSTables and memtable can be
                                                                                 discarded as soon as the compaction has ﬁnished.
                     GFS                                                           A merging compaction that rewrites all SSTables
                          tablet log                                             into exactly one SSTable is called a major compaction.
                                                                                 SSTables produced by non-major compactions can con-
                         Write Op                                                tain special deletion entries that suppress deleted data in
                                                       SSTable Files             older SSTables that are still live. A major compaction,
                                                                                 on the other hand, produces an SSTable that contains
                                 Figure 5: Tablet Representation                 no deletion information or deleted data. Bigtable cy-
                                                                                 cles through all of its tablets and regularly applies major
                   readsits metadatafromtheMETADATAtable. Thismeta-              compactions to them. These major compactions allow
                   data contains the list of SSTables that comprise a tablet     Bigtable to reclaim resources used by deleted data, and
                   and a set of a redo points, which are pointers into any       also allow it to ensure that deleted data disappears from
                   commit logs that may contain data for the tablet. The         the system in a timely fashion, which is important for
                   serverreadstheindicesoftheSSTablesintomemoryand               services that store sensitive data.
                   reconstructs the memtable by applying all of the updates
                   that have committed since the redo points.                    6 Reﬁnements
                     When a write operation arrives at a tablet server, the
                   server checks that it is well-formed, and that the sender     The implementation described in the previous section
                   is authorized to perform the mutation. Authorization is       required a number of reﬁnements to achieve the high
                   performedbyreadingthelist of permittedwriters froma           performance, availability, and reliability required by our
                   Chubbyﬁle(whichis almost always a hit in the Chubby           users. This section describesportionsoftheimplementa-
                   client cache). A valid mutation is written to the commit      tioninmoredetailinordertohighlightthesereﬁnements.
                   log. Group commit is used to improve the throughput of
                   lots of small mutations [13, 16]. After the write has been
                   committed, its contents are inserted into the memtable.       Locality groups
                     When a read operation arrives at a tablet server, it is
                   similarly checkedforwell-formednessandproperautho-            Clients can groupmultiple column families together into
                   rization. A valid read operation is executed on a merged      a locality group. A separate SSTable is generated for
                   view of the sequence of SSTables and the memtable.            each locality group in each tablet. Segregating column
                   Since the SSTables and the memtable are lexicograph-          families that are not typically accessed together into sep-
                   ically sorted data structures, the merged view can be         arate locality groups enables more efﬁcient reads. For
                   formedefﬁciently.                                             example, page metadata in Webtable (such as language
                     Incoming read and write operations can continue             and checksums) can be in one locality group, and the
                   while tablets are split and merged.                           contents of the page can be in a different group: an ap-
                    ToappearinOSDI2006                                                                                                   6
                    plication that wants to read the metadata does not need          Cachingforreadperformance
                    to read through all of the page contents.                        Toimprovereadperformance,tabletserversusetwolev-
                      In addition, some useful tuning parameters can be              els of caching. The Scan Cache is a higher-level cache
                    speciﬁedonaper-localitygroupbasis. Forexample,alo-               that caches the key-value pairs returned by the SSTable
                    cality group can be declared to be in-memory. SSTables           interface to the tablet server code. The Block Cache is a
                    for in-memory locality groups are loaded lazily into the         lower-level cache that caches SSTables blocks that were
                    memoryofthe tablet server. Once loaded, column fam-              read fromGFS.TheScanCacheismostusefulforappli-
                    ilies that belong to such locality groups can be read            cations that tend to read the same data repeatedly. The
                    without accessing the disk. This feature is useful for           Block Cache is useful for applications that tend to read
                    small pieces of data that are accessed frequently: we            data that is close to the data they recently read (e.g., se-
                    use it internally for the location column family in the          quential reads, or random reads of different columns in
                    METADATAtable.                                                   the same locality group within a hot row).
                                                                                     Bloomﬁlters
                    Compression                                                      Asdescribedin Section 5.3, a read operation has to read
                                                                                     from all SSTables that make up the state of a tablet.
                    Clients can control whether or not the SSTables for a            If these SSTables are not in memory, we may end up
                    locality group are compressed, and if so, which com-             doing many disk accesses. We reduce the number of
                    pression format is used.     The user-speciﬁed compres-          accesses by allowing clients to specify that Bloom ﬁl-
                    sion format is applied to each SSTable block (whose size         ters [7] should be created for SSTables in a particu-
                    is controllable via a locality group speciﬁc tuning pa-          lar locality group.    A Bloom ﬁlter allows us to ask
                    rameter). Although we lose some space by compress-               whether an SSTable might contain any data for a spec-
                    ing each block separately, we beneﬁt in that small por-          iﬁed row/column pair. For certain applications, a small
                    tions of an SSTable can be read without decompress-              amount of tablet server memory used for storing Bloom
                    ing the entire ﬁle. Many clients use a two-pass custom           ﬁlters drastically reduces the number of disk seeks re-
                    compression scheme. The ﬁrst pass uses Bentley and               quired for read operations.     Our use of Bloom ﬁlters
                    McIlroy’s scheme [6], which compresses long common               also implies that most lookups for non-existent rows or
                    strings across a large window. The second pass uses a            columnsdonotneedtotouchdisk.
                    fast compression algorithm that looks for repetitions in
                    a small 16 KB window of the data. Both compression               Commit-logimplementation
                    passes are very fast—they encode at 100–200MB/s, and
                    decodeat 400–1000MB/s onmodernmachines.                          If we kept the commit log for each tablet in a separate
                      Eventhoughweemphasizedspeedinsteadofspacere-                   log ﬁle, a very large number of ﬁles would be written
                    duction when choosingourcompressionalgorithms,this               concurrently in GFS. Depending on the underlying ﬁle
                    two-pass compression scheme does surprisingly well.              system implementationoneach GFS server,these writes
                    For example, in Webtable, we use this compression                could cause a large number of disk seeks to write to the
                    scheme to store Web page contents. In one experiment,            different physical log ﬁles. In addition, having separate
                    westored a large number of documents in a compressed             log ﬁles per tablet also reduces the effectiveness of the
                    locality group. For the purposes of the experiment, we           group commit optimization, since groups would tend to
                    limited ourselves to one version of each document in-            be smaller. To ﬁx these issues, we append mutations
                    stead of storing all versions available to us. The scheme        to a single commit log per tablet server, co-mingling
                    achieved a 10-to-1 reduction in space.       This is much        mutations for different tablets in the same physical log
                    better than typical Gzip reductions of 3-to-1 or 4-to-1          ﬁle [18, 20].
                    on HTMLpages because of the way Webtable rows are                   Using one log provides signiﬁcant performance ben-
                    laid out: all pages from a single host are stored close          eﬁts during normal operation, but it complicates recov-
                    to each other.   This allows the Bentley-McIlroy algo-           ery. When a tablet server dies, the tablets that it served
                    rithm to identify large amounts of shared boilerplate in         will be moved to a large number of other tablet servers:
                    pages from the same host. Many applications, not just            each server typically loads a small number of the orig-
                    Webtable, choose their row names so that similar data            inal server’s tablets.  To recover the state for a tablet,
                    ends up clustered, and therefore achieve very good com-          the new tablet server needs to reapply the mutations for
                    pression ratios. Compression ratios get even better when         that tablet from the commit log written by the original
                    westoremultipleversionsofthesame valuein Bigtable.               tablet server. However, the mutations for these tablets
                     ToappearinOSDI2006                                                                                                         7
                   were co-mingled in the same physical log ﬁle. One ap-         of the SSTables that we generate are immutable. For ex-
                   proach would be for each new tablet server to read this       ample, we do not need any synchronization of accesses
                   full commit log ﬁle and apply just the entries needed for     to the ﬁle system when reading from SSTables. As a re-
                   the tablets it needs to recover. However, under such a        sult, concurrency control over rows can be implemented
                   scheme, if 100 machines were each assigned a single           very efﬁciently. The only mutable data structure that is
                   tablet from a failed tablet server, then the log ﬁle would    accessedbybothreadsandwritesisthememtable. Tore-
                   be read 100 times (once by each server).                      duce contention during reads of the memtable, we make
                     We avoid duplicating log reads by ﬁrst sort-                each memtable row copy-on-write and allow reads and
                   ing the commit log entries in order of the keys               writes to proceed in parallel.
                   htable,row name,log sequence numberi.             In the        Since SSTables are immutable, the problem of perma-
                   sorted output, all mutations for a particular tablet are      nently removing deleted data is transformed to garbage
                   contiguousandcanthereforebereadefﬁcientlywithone              collecting obsolete SSTables. Each tablet’s SSTables are
                   disk seek followed by a sequential read. To parallelize       registered in the METADATA table. The master removes
                   the sorting, we partition the log ﬁle into 64 MB seg-         obsolete SSTables as a mark-and-sweep garbage collec-
                   ments, and sort each segment in parallel on different         tion [25] over the set of SSTables, where the METADATA
                   tablet servers. This sorting process is coordinated by the    table contains the set of roots.
                   master and is initiated when a tablet server indicates that     Finally, the immutability of SSTables enables us to
                   it needs to recover mutations from some commit log ﬁle.       split tablets quickly. Instead of generating a new set of
                     WritingcommitlogstoGFSsometimescausesperfor-                SSTables for each child tablet, we let the child tablets
                   mancehiccupsforavarietyofreasons(e.g.,aGFSserver              share the SSTables of the parent tablet.
                   machine involved in the write crashes, or the network
                   paths traversed to reach the particular set of three GFS      7 PerformanceEvaluation
                   servers is suffering network congestion, or is heavily
                   loaded). To protect mutations from GFS latency spikes,        We set up a Bigtable cluster with N tablet servers to
                   each tablet server actually has two log writing threads,      measure the performance and scalability of Bigtable as
                   each writing to its own log ﬁle; only one of these two        Nisvaried. The tablet servers were conﬁgured to use 1
                   threads is actively in use at a time. If writes to the ac-    GBofmemoryandto write to a GFS cell consisting of
                   tive log ﬁle are performing poorly, the log ﬁle writing is    1786 machines with two 400 GB IDE hard drives each.
                   switched to the other thread, and mutations that are in       N client machines generated the Bigtable load used for
                   the commitlogqueuearewrittenbythenewlyactivelog               thesetests. (We usedthesamenumberofclientsastablet
                   writing thread. Log entries contain sequence numbers          servers to ensure that clients were never a bottleneck.)
                   to allow the recovery process to elide duplicated entries     Each machine had two dual-core Opteron 2 GHz chips,
                   resulting from this log switching process.                    enough physical memory to hold the working set of all
                   Speeding up tablet recovery                                   running processes, and a single gigabit Ethernet link.
                                                                                 The machines were arranged in a two-level tree-shaped
                   If the master moves a tablet from one tablet server to        switched network with approximately 100-200 Gbps of
                   another, the source tablet server ﬁrst does a minor com-      aggregatebandwidthavailable at the root. All of the ma-
                   paction on that tablet. This compaction reduces recov-        chineswereinthesamehostingfacilityandthereforethe
                   ery time by reducingthe amountofuncompactedstate in           round-trip time between any pair of machines was less
                   the tablet server’s commit log. After ﬁnishing this com-      than a millisecond.
                   paction, the tablet server stops serving the tablet. Before     The tablet servers and master, test clients, and GFS
                   it actually unloads the tablet, the tablet server does an-    servers all ran on the same set of machines. Every ma-
                   other (usually very fast) minor compaction to eliminate       chine ran a GFS server. Some of the machines also ran
                   any remaining uncompacted state in the tablet server’s        either a tablet server, or a client process, or processes
                   log that arrived while the ﬁrst minor compaction was          fromotherjobsthatwereusingthepoolatthesametime
                   being performed. After this second minor compaction           as these experiments.
                   is complete, the tablet can be loaded on another tablet         RisthedistinctnumberofBigtablerowkeysinvolved
                   server without requiring any recovery of log entries.         in the test. R was chosen so that each benchmark read or
                                                                                 wrote approximately1 GB of data per tablet server.
                   Exploiting immutability                                         The sequential write benchmark used row keys with
                                                                                 names 0 to R − 1. This space of row keys was parti-
                   Besides the SSTable caches, various other parts of the        tioned into 10N equal-sized ranges. These ranges were
                   Bigtable system have been simpliﬁed by the fact that all      assigned to the N clients by a central scheduler that as-
                    ToappearinOSDI2006                                                                                                   8
                                                                                      4M        scans
                                                   # of Tablet Servers                          random reads (mem)
                       Experiment                 1      50     250     500           3M        random writes
                                                                                                sequential reads
                       random reads           1212      593     479     241                     sequential writes
                       random reads (mem)    10811     8511    8000    6250           2M        random reads
                       random writes          8850     3745    3425    2000           1M
                       sequential reads       4425     2463    2625    2469
                       sequential writes      8547     3623    2451    1905
                       scans                 15385    10526    9524    7843           Values read/written per second100200300400   500
                                                                                                     Number of tablet servers
                   Figure 6: Number of 1000-byte values read/written per second. The table shows the rate per tablet server; the graph shows the
                   aggregate rate.
                   signed the next available range to a client as soon as the     Single tablet-server performance
                   client ﬁnished processing the previous range assigned to       Let us ﬁrst consider performance with just one tablet
                   it. This dynamic assignment helped mitigate the effects        server. Randomreadsareslowerthanallotheroperations
                   ofperformancevariationscausedbyotherprocessesrun-              byanorderofmagnitudeormore. Eachrandomreadin-
                   ningontheclientmachines. Wewroteasinglestringun-               volves the transfer of a 64 KB SSTable block over the
                   der each row key. Each string was generated randomly           networkfromGFStoatabletserver,outofwhichonlya
                   and was therefore uncompressible. In addition, strings         single1000-bytevalueisused. Thetabletserverexecutes
                   under different row key were distinct, so no cross-row         approximately 1200 reads per second, which translates
                   compressionwaspossible. Therandomwritebenchmark                into approximately75MB/sofdatareadfromGFS.This
                   was similar except that the row key was hashed modulo          bandwidth is enough to saturate the tablet server CPUs
                   Rimmediately before writing so that the write load was         because of overheads in our networking stack, SSTable
                   spread roughlyuniformlyacross the entire row space for         parsing, and Bigtable code, and is also almost enough
                   the entire duration of the benchmark.                          to saturate the network links used in our system. Most
                      Thesequentialread benchmarkgeneratedrowkeysin               Bigtable applications with this type of an access pattern
                   exactly the same way as the sequential write benchmark,        reduce the block size to a smaller value, typically 8KB.
                   butinsteadofwritingundertherowkey,itreadthestring                Random reads from memory are much faster since
                   storedundertherowkey(whichwaswrittenbyanearlier                each 1000-byte read is satisﬁed from the tablet server’s
                   invocationofthesequentialwritebenchmark). Similarly,           local memorywithoutfetchingalarge64KBblockfrom
                   the random read benchmark shadowed the operation of            GFS.
                   the random write benchmark.                                      Randomandsequentialwritesperformbetterthanran-
                      The scan benchmark is similar to the sequential read        domreadssince each tablet server appends all incoming
                   benchmark, but uses support provided by the Bigtable           writes to a single commit log and uses group commit to
                   API for scanning over all values in a row range. Us-           stream these writes efﬁciently to GFS. There is no sig-
                   ing a scan reduces the number of RPCs executed by the          niﬁcant difference between the performance of random
                   benchmark since a single RPC fetches a large sequence          writes and sequential writes; in both cases, all writes to
                   of values from a tablet server.                                the tablet server are recorded in the same commit log.
                      Therandomreads(mem) benchmarkis similar to the                Sequential reads perform better than random reads
                   randomreadbenchmark,but the locality group that con-           since every 64 KB SSTable block that is fetched from
                   tains the benchmark data is marked as in-memory, and           GFS is stored into our block cache, where it is used to
                   therefore the reads are satisﬁed from the tablet server’s      serve the next 64 read requests.
                   memory instead of requiring a GFS read. For just this            Scans are even faster since the tablet server can return
                   benchmark, we reduced the amount of data per tablet            a large number of values in response to a single client
                   server from 1 GB to 100 MB so that it would ﬁt com-            RPC, and therefore RPC overhead is amortized over a
                   fortably in the memory available to the tablet server.         large number of values.
                      Figure 6 shows two views on the performance of our          Scaling
                   benchmarkswhen reading and writing 1000-byte values
                   to Bigtable. The table shows the number of operations          Aggregate throughput increases dramatically, by over a
                   per second per tablet server; the graph shows the aggre-       factor of a hundred, as we increase the number of tablet
                   gate numberof operations per second.                           servers in the system from 1 to 500. For example, the
                    ToappearinOSDI2006                                                                                                     9
                                   # of tablet servers   # of clusters                   percentage of data served from memory, and complexity
                                        0   ..     19             259                    of the table schema. In the rest of this section, we brieﬂy
                                       20   ..     49              47                    describe how three product teams use Bigtable.
                                       50   ..     99              20
                                     100    ..    499              50                    8.1     GoogleAnalytics
                                   >500                            12
                                                                                         Google Analytics (analytics.google.com) is a service
                     Table 1: Distribution of number of tablet servers in Bigtable       that helps webmasters analyze trafﬁc patterns at their
                     clusters.                                                           web sites. It provides aggregate statistics, such as the
                                                                                         number of unique visitors per day and the page views
                     performanceofrandomreadsfrommemoryincreasesby                       per URLperday,aswellassite-trackingreports,suchas
                     almost a factor of 300 as the number of tablet server in-           the percentage of users that made a purchase, given that
                     creases by a factor of 500. This behavior occurs because            they earlier viewed a speciﬁc page.
                     the bottleneck on performance for this benchmark is the                To enable the service, webmasters embed a small
                     individual tablet server CPU.                                       JavaScript program in their web pages. This program
                        However, performancedoes not increase linearly. For              is invoked whenever a page is visited. It records various
                     mostbenchmarks,thereisasigniﬁcantdropinper-server                   information about the request in Google Analytics, such
                     throughputwhen going from 1 to 50 tablet servers. This              as a user identiﬁer and information about the page be-
                     drop is caused by imbalance in load in multiple server              ing fetched. Google Analytics summarizes this data and
                     conﬁgurations, often due to other processes contending              makesit available to webmasters.
                     for CPU and network. Our load balancing algorithm at-                  Webrieﬂy describe two of the tables used by Google
                     tempts to deal with this imbalance, but cannot do a per-            Analytics. The raw click table (˜200 TB) maintains a
                     fect job for two main reasons: rebalancing is throttled to          row for each end-user session. The row name is a tuple
                     reduce the number of tablet movements (a tablet is un-              containing the website’s name and the time at which the
                     available for a short time, typically less than one second,         session was created. This schema ensures that sessions
                     whenit is moved),and the load generated by our bench-               that visit the same web site are contiguous, and that they
                     marksshifts around as the benchmarkprogresses.                      aresortedchronologically. Thistablecompressesto14%
                        The random read benchmark shows the worst scaling                of its original size.
                     (an increase in aggregate throughput by only a factor of               The summary table (˜20 TB) contains various prede-
                     100 for a 500-fold increase in number of servers). This             ﬁned summaries for each website. This table is gener-
                     behavioroccursbecause(asexplainedabove)wetransfer                   ated from the raw click table by periodically scheduled
                     one large 64KB block over the network for every 1000-               MapReduce jobs. Each MapReduce job extracts recent
                     byte read. This transfer saturates various shared 1 Gi-             session data from the raw click table. The overall sys-
                     gabit links in our network and as a result, the per-server          tem’s throughput is limited by the throughput of GFS.
                     throughputdropssigniﬁcantlyasweincreasethenumber                    This table compresses to 29% of its original size.
                     of machines.                                                        8.2     GoogleEarth
                     8 RealApplications                                                  Google operates a collection of services that provide
                                                                                         users with access to high-resolution satellite imagery of
                     AsofAugust2006,thereare388non-testBigtableclus-                     the world’s surface, both through the web-based Google
                     ters running in various Google machine clusters, with a             Maps interface (maps.google.com) and through the
                     combined total of about 24,500 tablet servers. Table 1              Google Earth (earth.google.com) custom client soft-
                     shows a rough distribution of tablet servers per cluster.           ware. These products allow users to navigate across the
                     Many of these clusters are used for development pur-                world’s surface: they can pan, view, and annotate satel-
                     poses and therefore are idle for signiﬁcant periods. One            lite imagery at many different levels of resolution. This
                     group of 14 busy clusters with 8069 total tablet servers            system uses one table to preprocess data, and a different
                     saw an aggregate volume of more than 1.2 million re-                set of tables for serving client data.
                     quests per second, with incoming RPC trafﬁc of about                   Thepreprocessingpipeline uses one table to store raw
                     741MB/sandoutgoingRPCtrafﬁcofabout16GB/s.                           imagery. During preprocessing, the imagery is cleaned
                        Table 2 provides some data about a few of the tables             and consolidated into ﬁnal serving data. This table con-
                     currently in use. Some tables store data that is served             tains approximately 70 terabytes of data and therefore is
                     to users, whereas others store data for batch processing;           served fromdisk. The images are efﬁciently compressed
                     the tables range widely in total size, average cell size,           already, so Bigtable compression is disabled.
                      ToappearinOSDI2006                                                                                                             10
                              Project         Table size  Compression      # Cells    # Column    # Locality    %in       Latency-
                               name             (TB)          ratio       (billions)  Families      Groups     memory     sensitive?
                               Crawl              800          11%           1000        16            8           0%        No
                               Crawl               50          33%            200          2           2           0%        No
                          Google Analytics         20          29%             10          1           1           0%        Yes
                          Google Analytics        200          14%             80          1           1           0%        Yes
                            Google Base              2         31%             10        29            3          15%        Yes
                            Google Earth             0.5       64%              8          7           2          33%        Yes
                            Google Earth           70           –               9          8           3           0%        No
                               Orkut                 9          –               0.9        8           5           1%        Yes
                         Personalized Search         4         47%              6        93           11           5%        Yes
                   Table 2: Characteristics of a few tables in production use. Table size (measured before compression) and # Cells indicate approxi-
                   mate sizes. Compression ratio is not given for tables that have compression disabled.
                     Each row in the imagery table corresponds to a sin-           ThePersonalized Search data is replicated across sev-
                   gle geographic segment. Rows are named to ensure that        eral Bigtable clusters to increase availability and to re-
                   adjacent geographicsegmentsare storednear eachother.         duce latency due to distance from clients. The Personal-
                   The table contains a column family to keep track of the      ized Search team originally built a client-side replication
                   sources of data for each segment. This column family         mechanismontopofBigtablethatensuredeventualcon-
                   has a large number of columns: essentially one for each      sistency of all replicas. The current system now uses a
                   raw data image. Since each segment is only built from a      replication subsystem that is built into the servers.
                   fewimages, this columnfamily is very sparse.                    Thedesign of the Personalized Search storage system
                     The preprocessing pipeline relies heavily on MapRe-        allows other groups to add new per-user information in
                   duceoverBigtabletotransformdata. Theoverallsystem            their own columns, and the system is now used by many
                   processes over 1 MB/sec of data per tablet server during     other Google properties that need to store per-user con-
                   someoftheseMapReducejobs.                                    ﬁguration options and settings. Sharing a table amongst
                     Theservingsystemusesonetabletoindexdatastored              many groups resulted in an unusually large number of
                   in GFS. This table is relatively small (˜500 GB), but it     column families. To help support sharing, we added a
                   must serve tens of thousands of queries per second per       simple quota mechanism to Bigtable to limit the stor-
                   datacenter with low latency. As a result, this table is      age consumption by any particular client in shared ta-
                   hosted across hundreds of tablet servers and contains in-    bles; this mechanism provides some isolation between
                   memorycolumnfamilies.                                        the various product groups using this system for per-user
                                                                                information storage.
                   8.3    Personalized Search                                   9 Lessons
                   Personalized Search (www.google.com/psearch) is an
                   opt-in service that records user queries and clicks across   In the process of designing, implementing, maintaining,
                   a variety of Google properties such as web search, im-       and supporting Bigtable, we gained useful experience
                   ages, and news. Users can browse their search histories      andlearnedseveral interesting lessons.
                   to revisit their old queries and clicks, and they can ask       One lesson we learned is that large distributed sys-
                   for personalized search results based on their historical    tems are vulnerable to many types of failures, not just
                   Googleusagepatterns.                                         the standard network partitions and fail-stop failures as-
                     Personalized Search stores each user’s data in             sumed in many distributed protocols. For example, we
                   Bigtable. Each user has a unique userid and is assigned      have seen problems due to all of the following causes:
                   a row named by that userid. All user actions are stored      memoryandnetworkcorruption,largeclockskew,hung
                   in a table. A separate column family is reserved for each    machines, extended and asymmetric network partitions,
                   type of action (for example, there is a column family that   bugs in other systems that we are using (Chubby for ex-
                   stores all web queries). Each data element uses as its       ample), overﬂow of GFS quotas, and planned and un-
                   Bigtable timestamp the time at which the corresponding       plannedhardwaremaintenance. Aswehavegainedmore
                   user action occurred. Personalized Search generates user     experiencewiththeseproblems,wehaveaddressedthem
                   proﬁles using a MapReduce over Bigtable. These user          by changing various protocols. For example, we added
                   proﬁles are used to personalize live search results.         checksummingtoourRPCmechanism. Wealsohandled
                    ToappearinOSDI2006                                                                                                 11
                   some problems by removing assumptions made by one              the behavior of Chubby features that were seldom exer-
                   part of the system about another part. For example, we         cised by other applications. We discovered that we were
                   stoppedassumingagivenChubbyoperationcouldreturn                spending an inordinate amount of time debugging ob-
                   only one of a ﬁxed set of errors.                              scure corner cases, not only in Bigtable code, but also in
                      Another lesson we learned is that it is important to        Chubbycode. Eventually,we scrappedthis protocol and
                   delay adding new features until it is clear how the new        moved to a newer simpler protocol that depends solely
                   features will be used. For example, we initially planned       onwidely-usedChubbyfeatures.
                   to support general-purpose transactions in our API. Be-
                   cause we did not have an immediate use for them, how-          10 RelatedWork
                   ever, we did not implement them. Now that we have
                   many real applications running on Bigtable, we have            The Boxwood project [24] has components that overlap
                   beenabletoexaminetheiractualneeds,andhavediscov-               in some ways with Chubby, GFS, and Bigtable, since it
                   eredthatmostapplicationsrequireonlysingle-rowtrans-            provides for distributed agreement, locking, distributed
                   actions. Where people have requested distributed trans-        chunk storage, and distributed B-tree storage. In each
                   actions, the most important use is for maintaining sec-        case where there is overlap, it appears that the Box-
                   ondary indices, and we plan to add a specialized mech-         wood’s componentis targeted at a somewhat lower level
                   anism to satisfy this need.    The new mechanism will          than the corresponding Google service. The Boxwood
                   be less general than distributed transactions, but will be     project’s goal is to provide infrastructure for building
                   moreefﬁcient (especially for updates that span hundreds        higher-level services such as ﬁle systems or databases,
                   of rows or more) and will also interact better with our        while the goal of Bigtable is to directly support client
                   schemeforoptimistic cross-data-center replication.             applications that wish to store data.
                      A practical lesson that we learned from supporting            Manyrecentprojectshavetackledtheproblemofpro-
                   Bigtable is the importance of proper system-level mon-         viding distributed storage or higher-level services over
                   itoring (i.e., monitoring both Bigtable itself, as well as     wide area networks, often at “Internet scale.” This in-
                   the client processes usingBigtable). Forexample,weex-          cludes work on distributed hash tables that began with
                   tendedourRPCsystemsothatforasampleoftheRPCs,                   projects such as CAN [29], Chord [32], Tapestry [37],
                   it keeps a detailed trace of the important actions done on     andPastry [30]. These systems address concerns that do
                   behalf of that RPC. This feature has allowed us to de-         notariseforBigtable, suchas highlyvariablebandwidth,
                   tect and ﬁx many problems such as lock contention on           untrusted participants, or frequent reconﬁguration; de-
                   tablet data structures, slow writes to GFS while com-          centralized control and Byzantine fault tolerance are not
                   mitting Bigtable mutations, and stuck accesses to the          Bigtable goals.
                   METADATAtable when METADATA tablets are unavail-                 In terms of the distributed data storage model that one
                   able. Another example of useful monitoring is that ev-         might provide to application developers, we believe the
                   ery Bigtable cluster is registered in Chubby. This allows      key-value pair model provided by distributed B-trees or
                   us to track down all clusters, discover how big they are,      distributed hash tables is too limiting. Key-value pairs
                   see whichversionsofoursoftwaretheyarerunning,how               are a useful building block, but they should not be the
                   muchtrafﬁc they are receiving, and whether or not there        only building block one provides to developers. The
                   are any problems such as unexpectedlylarge latencies.          model we chose is richer than simple key-value pairs,
                      The most important lesson we learned is the value           and supports sparse semi-structured data. Nonetheless,
                   of simple designs. Given both the size of our system           it is still simple enough that it lends itself to a very efﬁ-
                   (about 100,000 lines of non-test code), as well as the         cient ﬂat-ﬁle representation, and it is transparent enough
                   fact that code evolves over time in unexpected ways, we        (via locality groups) to allow our users to tune important
                   have found that code and design clarity are of immense         behaviors of the system.
                   help in code maintenance and debugging. One exam-                Several database vendors have developed parallel
                   ple of this is our tablet-server membership protocol. Our      databases that can store large volumes of data. Oracle’s
                   ﬁrst protocol was simple: the master periodically issued       Real ApplicationCluster database [27] uses shared disks
                   leases to tablet servers, and tablet servers killed them-      to store data (Bigtable uses GFS) and a distributed lock
                   selves if their lease expired. Unfortunately, this proto-      manager (Bigtable uses Chubby). IBM’s DB2 Parallel
                   col reduced availability signiﬁcantly in the presence of       Edition[4] is basedonashared-nothing[33]architecture
                   network problems, and was also sensitive to master re-         similar to Bigtable. Each DB2 server is responsible for
                   covery time. We redesigned the protocol several times          a subset of the rows in a table which it stores in a local
                   until we had a protocol that performed well. However,          relational database. Both products provide a complete
                   the resulting protocol was too complex and depended on         relational model with transactions.
                    ToappearinOSDI2006                                                                                                   12
                      Bigtable locality groups realize similar compression          Given the unusual interface to Bigtable, an interest-
                   and disk read performance beneﬁts observed for other           ing question is how difﬁcult it has been for our users to
                   systems that organize data on disk using column-based          adapt to using it. New users are sometimes uncertain of
                   rather than row-based storage, including C-Store [1, 34]       howtobestusetheBigtableinterface,particularlyifthey
                   and commercial products such as Sybase IQ [15, 36],            are accustomedtousingrelationaldatabasesthatsupport
                   SenSage [31], KDB+ [22], and the ColumnBM storage              general-purpose transactions. Nevertheless, the fact that
                   layer in MonetDB/X100 [38]. Another system that does           manyGoogleproductssuccessfullyuseBigtabledemon-
                   vertical and horizontal data partioning into ﬂat ﬁles and      strates that our design works well in practice.
                   achieves good data compression ratios is AT&T’s Day-             We are in the process of implementing several addi-
                   tona database [19]. Locality groups do not support CPU-        tional Bigtable features, such as support for secondary
                   cache-level optimizations, such as those described by          indices and infrastructure for building cross-data-center
                   Ailamaki [2].                                                  replicated Bigtables with multiple master replicas. We
                      The manner in which Bigtable uses memtables and             have also begun deploying Bigtable as a service to prod-
                   SSTables to store updates to tablets is analogous to the       uctgroups,sothatindividualgroupsdonotneedtomain-
                   way that the Log-Structured Merge Tree [26] stores up-         tain their own clusters. As our service clusters scale,
                   dates to index data.    In both systems, sorted data is        we will need to deal with more resource-sharing issues
                   buffered in memory before being written to disk, and           within Bigtable itself [3, 5].
                   reads must merge data from memory and disk.                      Finally, we have found that there are signiﬁcant ad-
                      C-Store and Bigtable share many characteristics: both       vantages to building our own storage solution at Google.
                   systems use a shared-nothing architecture and have two         We have gotten a substantial amount of ﬂexibility from
                   different data structures, one for recent writes, and one      designing our own data model for Bigtable. In addi-
                   for storing long-lived data, with a mechanism for mov-         tion, our control over Bigtable’s implementation, and
                   ing data from one form to the other. The systems dif-          the other Google infrastructure upon which Bigtable de-
                   fer signiﬁcantly in their API: C-Store behaves like a          pends, means that we can remove bottlenecks and inefﬁ-
                   relational database, whereas Bigtable provides a lower         ciencies as they arise.
                   level read and write interface and is designed to support      Acknowledgements
                   manythousandsofsuchoperationspersecondperserver.
                   C-Store is also a “read-optimized relational DBMS”,
                   whereas Bigtable provides good performance on both             We thank the anonymous reviewers, David Nagle, and
                   read-intensive and write-intensive applications.               our shepherd Brad Calder, for their feedback on this pa-
                      Bigtable’s load balancer has to solve some of the same      per. The Bigtable system has beneﬁted greatly from the
                   kinds of load and memory balancing problems faced by           feedback of our many users within Google. In addition,
                   shared-nothingdatabases(e.g.,[11, 35]). Our problemis          we thank the following people for their contributions to
                   somewhatsimpler: (1)wedonotconsiderthepossibility              Bigtable: Dan Aguayo, Sameer Ajmani, Zhifeng Chen,
                   of multiple copies of the same data, possibly in alternate     Bill Coughran,MikeEpstein,HealfdeneGoguen,Robert
                   forms due to views or indices; (2) we let the user tell us     Griesemer, Jeremy Hylton, Josh Hyman, Alex Khesin,
                   what data belongs in memory and what data should stay          Joanna Kulik, Alberto Lerner, Sherry Listgarten, Mike
                   ondisk,ratherthantryingtodeterminethisdynamically;             Maloney,EduardoPinheiro,KathyPolizzi,FrankYellin,
                   (3) we have no complex queries to execute or optimize.         andArthurZwiegincew.
                                                                                  References
                   11 Conclusions                                                  [1] ABADI, D. J., MADDEN, S. R., AND FERREIRA,
                                                                                       M.C. Integrating compression and execution in column-
                   We have described Bigtable, a distributed system for                oriented database systems. Proc. of SIGMOD (2006).
                   storing structured data at Google. Bigtable clusters have       [2] AILAMAKI,A.,DEWITT,D.J.,HILL,M.D.,ANDSK-
                   been in production use since April 2005, and we spent               OUNAKIS, M. Weaving relations for cache performance.
                   roughly seven person-years on design and implementa-                In The VLDBJournal (2001), pp. 169–180.
                   tion beforethat date. As of August 2006,morethansixty           [3] BANGA, G., DRUSCHEL, P., AND MOGUL, J. C. Re-
                   projects are using Bigtable. Our users like the perfor-             source containers: A new facility for resource manage-
                   manceandhighavailabilityprovidedbytheBigtableim-                    ment in server systems. In Proc. of the 3rd OSDI (Feb.
                   plementation,andthattheycanscalethecapacityoftheir                  1999), pp. 45–58.
                   clusters by simply adding more machines to the system           [4] BARU, C. K., FECTEAU, G., GOYAL, A., HSIAO,
                   as their resource demands change over time.                         H., JHINGRAN, A., PADMANABHAN, S., COPELAND,
                    ToappearinOSDI2006                                                                                                   13
                           G. P., AND WILSON, W. G. DB2 parallel edition. IBM               [22] KX.COM. kx.com/products/database.php. Product page.
                           Systems Journal 34, 2 (1995), 292–322.                           [23] LAMPORT, L. Thepart-time parliament. ACM TOCS 16,
                       [5] BAVIER, A., BOWMAN, M., CHUN, B., CULLER, D.,                         2 (1998), 133–169.
                           KARLIN,S.,PETERSON,L.,ROSCOE,T.,SPALINK,T.,                      [24] MACCORMICK, J., MURPHY, N., NAJORK, M.,
                           AND WAWRZONIAK, M. Operating system support for                       THEKKATH,C. A., AND ZHOU, L. Boxwood: Abstrac-
                           planetary-scale network services. In Proc. of the 1st NSDI            tions as the foundation for storage infrastructure. In Proc.
                           (Mar. 2004), pp. 253–266.                                             of the 6th OSDI (Dec. 2004), pp. 105–120.
                       [6] BENTLEY, J. L., AND MCILROY, M. D. Data compres-                 [25] MCCARTHY, J. Recursive functions of symbolic expres-
                           sion using long common strings. In Data Compression                   sionsandtheircomputation bymachine. CACM3,4(Apr.
                           Conference (1999), pp. 287–295.                                       1960), 184–195.
                       [7] BLOOM,B.H. Space/timetrade-offsinhashcodingwith                  [26] O’NEIL, P., CHENG, E., GAWLICK, D., AND O’NEIL,
                           allowable errors. CACM 13, 7 (1970), 422–426.                         E. The log-structured merge-tree (LSM-tree). Acta Inf.
                       [8] BURROWS, M. The Chubby lock service for loosely-                      33, 4 (1996), 351–385.
                           coupled distributed systems. In Proc. of the 7th OSDI            [27] ORACLE.COM. www.oracle.com/technology/products/-
                           (Nov. 2006).                                                          database/clustering/index.html. Product page.
                       [9] CHANDRA, T., GRIESEMER, R., AND REDSTONE, J.                     [28] PIKE, R., DORWARD, S., GRIESEMER, R., AND QUIN-
                           Paxos made live — An engineering perspective. In Proc.                LAN, S.     Interpreting the data: Parallel analysis with
                           of PODC(2007).                                                        Sawzall. Scientiﬁc Programming Journal 13, 4 (2005),
                     [10] COMER,D. UbiquitousB-tree. Computing Surveys 11, 2                     227–298.
                           (June 1979), 121–137.                                            [29] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP,
                     [11] COPELAND, G. P., ALEXANDER, W., BOUGHTER,                              R., AND SHENKER, S. A scalable content-addressable
                           E. E., AND KELLER,T. W. Dataplacement inBubba. In                     network. In Proc. of SIGCOMM (Aug. 2001), pp. 161–
                           Proc. of SIGMOD (1988), pp. 99–108.                                   172.
                     [12] DEAN, J., AND GHEMAWAT, S. MapReduce: Simpliﬁed                   [30] ROWSTRON, A., AND DRUSCHEL, P. Pastry: Scal-
                           data processing on large clusters. In Proc. of the 6th OSDI           able, distributed object location and routing for large-
                           (Dec. 2004), pp. 137–150.                                             scale peer-to-peer systems. In Proc. of Middleware 2001
                                                                                                 (Nov. 2001), pp. 329–350.
                     [13] DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L.,                     [31] SENSAGE.COM.           sensage.com/products-sensage.htm.
                           STONEBRAKER, M., AND WOOD, D. Implementation                          Product page.
                           techniques for main memory database systems. In Proc.            [32] STOICA, I., MORRIS, R., KARGER, D., KAASHOEK,
                           of SIGMOD(June1984), pp. 1–8.                                         M. F., AND BALAKRISHNAN, H. Chord: A scalable
                     [14] DEWITT, D. J., AND GRAY, J. Parallel database sys-                     peer-to-peer lookup service for Internet applications. In
                           tems: The future of high performance database systems.                Proc. of SIGCOMM (Aug. 2001), pp. 149–160.
                           CACM35,6(June1992),85–98.                                        [33] STONEBRAKER, M.            The case for shared nothing.
                     [15] FRENCH, C. D. One size ﬁts all database architectures                  Database Engineering Bulletin 9, 1 (Mar. 1986), 4–9.
                           do not work for DSS. In Proc. of SIGMOD (May 1995),              [34] STONEBRAKER,M.,ABADI,D.J.,BATKIN,A.,CHEN,
                           pp. 449–450.                                                          X., CHERNIACK, M., FERREIRA, M., LAU, E., LIN,
                     [16] GAWLICK, D., AND KINKADE, D. Varieties of concur-                      A., MADDEN, S., O’NEIL, E., O’NEIL, P., RASIN,
                           rency control in IMS/VSfast path. Database Engineering                A., TRAN, N., AND ZDONIK, S. C-Store: A column-
                           Bulletin 8, 2 (1985), 3–10.                                           oriented DBMS. In Proc.of VLDB(Aug.2005), pp. 553–
                     [17] GHEMAWAT,S., GOBIOFF, H., AND LEUNG, S.-T. The                         564.
                           Googleﬁlesystem. InProc.ofthe19thACMSOSP(Dec.                    [35] STONEBRAKER, M., AOKI, P. M., DEVINE, R.,
                           2003), pp. 29–43.                                                     LITWIN, W., AND OLSON, M. A. Mariposa: A new ar-
                     [18] GRAY, J. Notes on database operating systems. In Oper-                 chitecture for distributed data. In Proc. of the Tenth ICDE
                           ating Systems — An Advanced Course, vol. 60 of Lecture                (1994), IEEE Computer Society, pp. 54–65.
                           Notes in Computer Science. Springer-Verlag, 1978.                [36] SYBASE.COM.          www.sybase.com/products/database-
                     [19] GREER, R. Daytona and the fourth-generation language                   servers/sybaseiq. Product page.
                           Cymbal. In Proc. of SIGMOD (1999), pp. 525–526.                  [37] ZHAO, B. Y., KUBIATOWICZ, J., AND JOSEPH, A. D.
                     [20] HAGMANN, R. Reimplementing the Cedar ﬁle system                        Tapestry: An infrastructure for fault-tolerant wide-area
                           using logging and group commit. In Proc. of the 11th                  location and routing. Tech. Rep. UCB/CSD-01-1141, CS
                           SOSP(Dec.1987), pp. 155–162.                                          Division, UC Berkeley, Apr. 2001.
                     [21] HARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra                   [38] ZUKOWSKI,M.,BONCZ,P.A.,NES,N.,ANDHEMAN,
                           striped network ﬁle system. In Proc. of the 14th SOSP                 S. MonetDB/X100—ADBMSintheCPUcache. IEEE
                           (Asheville, NC, 1993), pp. 29–43.                                     Data Eng. Bull. 28, 2 (2005), 17–22.
                      ToappearinOSDI2006                                                                                                                  14
